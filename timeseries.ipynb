{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thegallier/timeseries/blob/main/timeseries.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ozPGMJjwveMB",
        "outputId": "74eca55d-48e4-451f-c597-9c5123349676"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trying hyperparameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.001, 'num_epochs': 5}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Loss: 11.0924\n",
            "Epoch [2/5], Loss: 11.0257\n",
            "Epoch [3/5], Loss: 11.0144\n",
            "Epoch [4/5], Loss: 11.0029\n",
            "Epoch [5/5], Loss: 10.9988\n",
            "Accuracy on test set: 33.72%\n",
            "Cohen's Kappa: 0.0068\n",
            "Trying hyperparameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.0001, 'num_epochs': 5}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Loss: 11.0859\n",
            "Epoch [2/5], Loss: 11.0087\n",
            "Epoch [3/5], Loss: 11.0018\n",
            "Epoch [4/5], Loss: 10.9990\n",
            "Epoch [5/5], Loss: 10.9879\n",
            "Accuracy on test set: 33.90%\n",
            "Cohen's Kappa: 0.0061\n",
            "Trying hyperparameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 0.001, 'num_epochs': 5}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Loss: 11.0833\n",
            "Epoch [2/5], Loss: 11.0058\n",
            "Epoch [3/5], Loss: 11.0039\n",
            "Epoch [4/5], Loss: 10.9923\n",
            "Epoch [5/5], Loss: 10.9951\n",
            "Accuracy on test set: 34.12%\n",
            "Cohen's Kappa: 0.0106\n",
            "Trying hyperparameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 0.0001, 'num_epochs': 5}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Loss: 11.0824\n",
            "Epoch [2/5], Loss: 11.0182\n",
            "Epoch [3/5], Loss: 11.0041\n",
            "Epoch [4/5], Loss: 10.9953\n",
            "Epoch [5/5], Loss: 10.9975\n",
            "Accuracy on test set: 33.69%\n",
            "Cohen's Kappa: 0.0055\n",
            "Trying hyperparameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.001, 'num_epochs': 5}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Loss: 11.0701\n",
            "Epoch [2/5], Loss: 11.0172\n",
            "Epoch [3/5], Loss: 11.0140\n",
            "Epoch [4/5], Loss: 10.9992\n",
            "Epoch [5/5], Loss: 10.9938\n",
            "Accuracy on test set: 33.79%\n",
            "Cohen's Kappa: 0.0051\n",
            "Trying hyperparameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.0001, 'num_epochs': 5}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Loss: 11.0957\n",
            "Epoch [2/5], Loss: 11.0185\n",
            "Epoch [3/5], Loss: 11.0050\n",
            "Epoch [4/5], Loss: 10.9930\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-4034ed44d46f>\u001b[0m in \u001b[0;36m<cell line: 487>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;31m# Start hyperparameter tuning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m \u001b[0mhyperparameter_tuning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyperparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[0;31m# Example 1: Combine TransformerModel and LSTMModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-4034ed44d46f>\u001b[0m in \u001b[0;36mhyperparameter_tuning\u001b[0;34m(hyperparams, train_loader, test_loader, device)\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m         \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m         \u001b[0mkappa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-4034ed44d46f>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, criterion, optimizer, num_epochs, device, writer)\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_securities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    519\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m             )\n\u001b[0;32m--> 521\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    770\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from itertools import product\n",
        "import time\n",
        "\n",
        "# Parameters\n",
        "num_timesteps = 5000  # Reduced for practical purposes\n",
        "num_securities = 10  # Number of different securities\n",
        "num_features_per_security = 4  # Features per security\n",
        "num_classes = 3  # Number of target classes\n",
        "num_features = num_securities * num_features_per_security  # Total number of features\n",
        "\n",
        "# Generate timestamps\n",
        "timestamps = np.arange(num_timesteps)\n",
        "\n",
        "# Generate random data for the primary dataset (X_data) and labels (y_data)\n",
        "X_data = np.random.rand(num_timesteps, num_features).astype(np.float32)  # Random features\n",
        "y_data = np.random.randint(0, num_classes, size=(num_timesteps, num_securities)).astype(np.int64)  # Random labels\n",
        "\n",
        "# Generate the second dataset with 3 categorical strings and 2 continuous floats per timestamp\n",
        "str_columns = ['str1', 'str2', 'str3']\n",
        "float_columns = ['float1', 'float2']\n",
        "second_dataset = {\n",
        "    'timestamp': timestamps,\n",
        "    'str1': np.random.choice(['A', 'B', 'C'], num_timesteps),\n",
        "    'str2': np.random.choice(['D', 'E', 'F'], num_timesteps),\n",
        "    'str3': np.random.choice(['G', 'H', 'I'], num_timesteps),\n",
        "    'float1': np.random.rand(num_timesteps),\n",
        "    'float2': np.random.rand(num_timesteps),\n",
        "}\n",
        "\n",
        "# Encode categorical string columns using LabelEncoder\n",
        "label_encoders = {}\n",
        "for col in str_columns:\n",
        "    le = LabelEncoder()\n",
        "    second_dataset[col] = le.fit_transform(second_dataset[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# Combine all features from the second dataset into a single array\n",
        "second_X_data = np.column_stack([second_dataset[col] for col in str_columns + float_columns]).astype(np.float32)\n",
        "\n",
        "# Apply Min-Max scaling to both datasets\n",
        "scaler_X = MinMaxScaler()\n",
        "X_data = scaler_X.fit_transform(X_data)\n",
        "\n",
        "scaler_second_X = MinMaxScaler()\n",
        "second_X_data = scaler_second_X.fit_transform(second_X_data)\n",
        "\n",
        "def add_positional_encoding(X, timestamps, option='shared'):\n",
        "    \"\"\"\n",
        "    Add positional encoding to the feature matrix.\n",
        "\n",
        "    Parameters:\n",
        "    - X (ndarray): Feature matrix.\n",
        "    - timestamps (ndarray): Array of timestamps.\n",
        "    - option (str): 'shared' or 'per_security'.\n",
        "\n",
        "    Returns:\n",
        "    - X_pe (ndarray): X with positional encoding added.\n",
        "    \"\"\"\n",
        "    if option == 'shared':\n",
        "        # Shared positional encoding across all features\n",
        "        pe = np.sin(timestamps[:, None] / 10000 ** (np.arange(X.shape[1]) / X.shape[1]))\n",
        "        X_pe = X + pe.astype(np.float32)\n",
        "    elif option == 'per_security':\n",
        "        # Separate positional encoding for each security\n",
        "        pe_list = []\n",
        "        for i in range(num_securities):\n",
        "            pe = np.sin(timestamps[:, None] / 10000 ** (np.arange(num_features_per_security) / num_features_per_security))\n",
        "            pe_list.append(pe)\n",
        "        pe_concat = np.hstack(pe_list)\n",
        "        X_pe = X + pe_concat.astype(np.float32)\n",
        "    else:\n",
        "        X_pe = X  # No positional encoding\n",
        "    return X_pe\n",
        "\n",
        "# Apply positional encoding\n",
        "positional_encoding_option = 'shared'  # 'shared' or 'per_security'\n",
        "X_data = add_positional_encoding(X_data, timestamps, positional_encoding_option)\n",
        "\n",
        "def create_windows(X1, X2, y, window_size, horizon):\n",
        "    \"\"\"\n",
        "    Create sliding windows for time series data.\n",
        "\n",
        "    Parameters:\n",
        "    - X1 (ndarray): Primary dataset features.\n",
        "    - X2 (ndarray): Secondary dataset features.\n",
        "    - y (ndarray): Target variable.\n",
        "    - window_size (int): Size of the window.\n",
        "    - horizon (int): Prediction horizon.\n",
        "\n",
        "    Returns:\n",
        "    - Tuple[np.ndarray, np.ndarray, np.ndarray]: Tuples of windows for X1, X2, and y.\n",
        "    \"\"\"\n",
        "    X1_windows = []\n",
        "    X2_windows = []\n",
        "    y_windows = []\n",
        "    for i in range(len(X1) - window_size - horizon + 1):\n",
        "        X1_windows.append(X1[i:i+window_size])\n",
        "        X2_windows.append(X2[i:i+window_size])\n",
        "        y_windows.append(y[i+window_size+horizon-1])\n",
        "    return np.array(X1_windows), np.array(X2_windows), np.array(y_windows)\n",
        "\n",
        "# Define window sizes\n",
        "window_size = 20\n",
        "horizon = 1\n",
        "\n",
        "# Create windows\n",
        "X1_windows, X2_windows, y_windows = create_windows(X_data, second_X_data, y_data, window_size, horizon)\n",
        "\n",
        "# Split into train and test sets\n",
        "train_ratio = 0.8\n",
        "train_size = int(len(X1_windows) * train_ratio)\n",
        "\n",
        "X1_train = X1_windows[:train_size]\n",
        "X2_train = X2_windows[:train_size]\n",
        "y_train = y_windows[:train_size]\n",
        "\n",
        "X1_test = X1_windows[train_size:]\n",
        "X2_test = X2_windows[train_size:]\n",
        "y_test = y_windows[train_size:]\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X1_train_tensor = torch.tensor(X1_train)\n",
        "X2_train_tensor = torch.tensor(X2_train)\n",
        "y_train_tensor = torch.tensor(y_train)\n",
        "\n",
        "X1_test_tensor = torch.tensor(X1_test)\n",
        "X2_test_tensor = torch.tensor(X2_test)\n",
        "y_test_tensor = torch.tensor(y_test)\n",
        "\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom Dataset for time series data with two feature sets.\n",
        "\n",
        "    Parameters:\n",
        "    - X1 (torch.Tensor): Primary dataset features.\n",
        "    - X2 (torch.Tensor): Secondary dataset features.\n",
        "    - y (torch.Tensor): Target variable.\n",
        "    \"\"\"\n",
        "    def __init__(self, X1, X2, y):\n",
        "        self.X1 = X1.float()\n",
        "        self.X2 = X2.float()\n",
        "        self.y = y.long()\n",
        "    def __len__(self):\n",
        "        return len(self.X1)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X1[idx], self.X2[idx], self.y[idx]\n",
        "\n",
        "# DataLoaders\n",
        "batch_size = 64\n",
        "\n",
        "train_dataset = TimeSeriesDataset(X1_train_tensor, X2_train_tensor, y_train_tensor)\n",
        "test_dataset = TimeSeriesDataset(X1_test_tensor, X2_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "class MatrixRegressionModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Matrix Regression Model: Y = A * X * B\n",
        "\n",
        "    Advantages:\n",
        "    - Simple and interpretable.\n",
        "    - Fast training.\n",
        "\n",
        "    Disadvantages:\n",
        "    - Limited in capturing complex patterns.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_securities, num_features):\n",
        "        super(MatrixRegressionModel, self).__init__()\n",
        "        self.A = nn.Parameter(torch.randn(1, num_securities))\n",
        "        self.B = nn.Parameter(torch.randn(num_features, num_classes))\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)  # Flatten over time\n",
        "        out = self.A @ x @ self.B  # Shape: (batch_size, num_classes)\n",
        "        out = out.view(-1, num_securities, num_classes)\n",
        "        return out\n",
        "\n",
        "class LogisticRegressionModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Logistic Regression Model.\n",
        "\n",
        "    Advantages:\n",
        "    - Simple and interpretable.\n",
        "    - Good baseline model.\n",
        "\n",
        "    Disadvantages:\n",
        "    - Assumes linear relationship.\n",
        "    - May underfit complex data.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, num_securities, num_classes):\n",
        "        super(LogisticRegressionModel, self).__init__()\n",
        "        self.num_securities = num_securities\n",
        "        self.num_classes = num_classes\n",
        "        self.linear = nn.Linear(input_size, num_securities * num_classes)\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        out = self.linear(x)\n",
        "        out = out.view(-1, self.num_securities, self.num_classes)\n",
        "        return out\n",
        "\n",
        "class CNNModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Convolutional Neural Network Model.\n",
        "\n",
        "    Advantages:\n",
        "    - Captures local temporal patterns.\n",
        "    - Efficient computation.\n",
        "\n",
        "    Disadvantages:\n",
        "    - Limited in capturing long-term dependencies.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_features, num_securities, num_classes):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.num_securities = num_securities\n",
        "        self.num_classes = num_classes\n",
        "        self.conv1 = nn.Conv1d(in_channels=num_features, out_channels=64, kernel_size=3)\n",
        "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc = nn.Linear(128, num_securities * num_classes)\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)  # (batch_size, num_features, window_size)\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = torch.mean(x, dim=2)  # Global average pooling\n",
        "        x = self.fc(x)\n",
        "        out = x.view(-1, self.num_securities, self.num_classes)\n",
        "        return out\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Long Short-Term Memory Model.\n",
        "\n",
        "    Advantages:\n",
        "    - Captures long-term dependencies.\n",
        "    - Suitable for sequential data.\n",
        "\n",
        "    Disadvantages:\n",
        "    - Computationally intensive.\n",
        "    - Prone to overfitting.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_securities, num_classes):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.num_securities = num_securities\n",
        "        self.num_classes = num_classes\n",
        "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_securities * num_classes)\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = out[:, -1, :]  # Last time step\n",
        "        out = self.fc(out)\n",
        "        out = out.view(-1, self.num_securities, self.num_classes)\n",
        "        return out\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer Model.\n",
        "\n",
        "    Advantages:\n",
        "    - Captures global dependencies.\n",
        "    - Parallel computation.\n",
        "\n",
        "    Disadvantages:\n",
        "    - Requires large datasets.\n",
        "    - Computationally intensive.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_features, num_securities, num_classes):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.num_securities = num_securities\n",
        "        self.num_classes = num_classes\n",
        "        self.embedding = nn.Linear(num_features, 128)\n",
        "        self.pos_encoder = PositionalEncoding(128)\n",
        "        encoder_layers = nn.TransformerEncoderLayer(d_model=128, nhead=8)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=2)\n",
        "        self.decoder = nn.Linear(128, num_securities * num_classes)\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), x.size(1), -1)  # (batch_size, window_size, num_features)\n",
        "        x = self.embedding(x)\n",
        "        x = self.pos_encoder(x)\n",
        "        x = x.permute(1, 0, 2)  # (sequence_length, batch_size, embedding_dim)\n",
        "        output = self.transformer_encoder(x)\n",
        "        output = output[-1, :, :]  # Last output\n",
        "        output = self.decoder(output)\n",
        "        output = output.view(-1, self.num_securities, self.num_classes)\n",
        "        return output\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Positional Encoding for Transformer.\n",
        "\n",
        "    Adds positional information to the embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        if d_model % 2 == 1:\n",
        "            pe[:, 1::2] = torch.sin(position * div_term)\n",
        "        else:\n",
        "            pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(1)\n",
        "        self.register_buffer('pe', pe)\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return x\n",
        "\n",
        "class SimpleRNNModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple Recurrent Neural Network Model.\n",
        "\n",
        "    Advantages:\n",
        "    - Captures sequential dependencies.\n",
        "    - Simpler than LSTM.\n",
        "\n",
        "    Disadvantages:\n",
        "    - Struggles with long-term dependencies.\n",
        "    - May suffer from vanishing gradients.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size, num_securities, num_classes):\n",
        "        super(SimpleRNNModel, self).__init__()\n",
        "        self.num_securities = num_securities\n",
        "        self.num_classes = num_classes\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_securities * num_classes)\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(1, x.size(0), self.rnn.hidden_size).to(x.device)\n",
        "        out, _ = self.rnn(x, h0)\n",
        "        out = out[:, -1, :]  # Last time step\n",
        "        out = self.fc(out)\n",
        "        out = out.view(-1, self.num_securities, self.num_classes)\n",
        "        return out\n",
        "\n",
        "class CombinedModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Combined Model: Merges outputs from two models.\n",
        "\n",
        "    Advantages:\n",
        "    - Leverages multiple data sources.\n",
        "    - Potentially better performance.\n",
        "\n",
        "    Disadvantages:\n",
        "    - More complex.\n",
        "    - Computationally intensive.\n",
        "    \"\"\"\n",
        "    def __init__(self, model1, model2, num_classes, num_securities):\n",
        "        super(CombinedModel, self).__init__()\n",
        "        self.model1 = model1\n",
        "        self.model2 = model2\n",
        "        self.fc = nn.Linear(num_classes * 2, num_classes)\n",
        "        self.num_securities = num_securities\n",
        "        self.num_classes = num_classes\n",
        "    def forward(self, x1, x2):\n",
        "        outputs1 = self.model1(x1)\n",
        "        outputs2 = self.model2(x2)\n",
        "        outputs = torch.cat((outputs1, outputs2), dim=2)\n",
        "        batch_size = outputs.size(0)\n",
        "        outputs = outputs.view(-1, outputs.size(2))\n",
        "        final_output = self.fc(outputs)\n",
        "        final_output = final_output.view(batch_size, self.num_securities, -1)\n",
        "        return final_output\n",
        "\n",
        "def train_model(model, train_loader, criterion, optimizer, num_epochs, device, writer):\n",
        "    \"\"\"\n",
        "    Training loop for the model.\n",
        "\n",
        "    Parameters:\n",
        "    - model (nn.Module): The model to train.\n",
        "    - train_loader (DataLoader): DataLoader for training data.\n",
        "    - criterion (nn.Module): Loss function.\n",
        "    - optimizer (torch.optim.Optimizer): Optimizer.\n",
        "    - num_epochs (int): Number of epochs.\n",
        "    - device (torch.device): Computation device.\n",
        "    - writer (SummaryWriter): TensorBoard SummaryWriter.\n",
        "    \"\"\"\n",
        "    model = model.to(device)\n",
        "    global_step = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for X1_batch, X2_batch, y_batch in train_loader:\n",
        "            X1_batch = X1_batch.to(device)\n",
        "            X2_batch = X2_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X1_batch, X2_batch)\n",
        "            loss = 0\n",
        "            for i in range(num_securities):\n",
        "                loss += criterion(outputs[:, i, :], y_batch[:, i])\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            writer.add_scalar('Training Loss', loss.item(), global_step)\n",
        "            global_step += 1\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
        "        # Save checkpoint\n",
        "        torch.save(model.state_dict(), f'checkpoint_epoch_{epoch+1}.pth')\n",
        "\n",
        "def evaluate_model(model, test_loader, device):\n",
        "    \"\"\"\n",
        "    Evaluate the model on the test set.\n",
        "\n",
        "    Parameters:\n",
        "    - model (nn.Module): The model to evaluate.\n",
        "    - test_loader (DataLoader): DataLoader for test data.\n",
        "    - device (torch.device): Computation device.\n",
        "\n",
        "    Returns:\n",
        "    - float: Cohen's kappa score.\n",
        "    \"\"\"\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    with torch.no_grad():\n",
        "        for X1_batch, X2_batch, y_batch in test_loader:\n",
        "            X1_batch = X1_batch.to(device)\n",
        "            X2_batch = X2_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "            outputs = model(X1_batch, X2_batch)\n",
        "            _, predicted = torch.max(outputs.data, 2)\n",
        "            total += y_batch.numel()\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "            all_preds.extend(predicted.cpu().numpy().flatten())\n",
        "            all_targets.extend(y_batch.cpu().numpy().flatten())\n",
        "    accuracy = 100 * correct / total\n",
        "    kappa = cohen_kappa_score(all_targets, all_preds)\n",
        "    print(f'Accuracy on test set: {accuracy:.2f}%')\n",
        "    print(f\"Cohen's Kappa: {kappa:.4f}\")\n",
        "    return kappa\n",
        "\n",
        "def hyperparameter_tuning(hyperparams, train_loader, test_loader, device):\n",
        "    \"\"\"\n",
        "    Perform hyperparameter tuning.\n",
        "\n",
        "    Parameters:\n",
        "    - hyperparams (dict): Dictionary of hyperparameters to try.\n",
        "    - train_loader (DataLoader): DataLoader for training data.\n",
        "    - test_loader (DataLoader): DataLoader for test data.\n",
        "    - device (torch.device): Computation device.\n",
        "    \"\"\"\n",
        "    best_kappa = -1\n",
        "    best_params = None\n",
        "    for params in product(*hyperparams.values()):\n",
        "        param_dict = dict(zip(hyperparams.keys(), params))\n",
        "        print(f\"Trying hyperparameters: {param_dict}\")\n",
        "        model1 = TransformerModel(num_features=num_features, num_securities=num_securities, num_classes=num_classes)\n",
        "        model2 = LSTMModel(input_size=second_X_data.shape[1], hidden_size=param_dict['hidden_size'],\n",
        "                           num_layers=param_dict['num_layers'], num_securities=num_securities, num_classes=num_classes)\n",
        "        combined_model = CombinedModel(model1, model2, num_classes, num_securities)\n",
        "        optimizer = torch.optim.Adam(combined_model.parameters(), lr=param_dict['learning_rate'])\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        writer = SummaryWriter()\n",
        "        train_model(combined_model, train_loader, criterion, optimizer, param_dict['num_epochs'], device, writer)\n",
        "        kappa = evaluate_model(combined_model, test_loader, device)\n",
        "        writer.close()\n",
        "        if kappa > best_kappa:\n",
        "            best_kappa = kappa\n",
        "            best_params = param_dict\n",
        "    print(f\"Best Cohen's Kappa: {best_kappa:.4f} with parameters: {best_params}\")\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyperparameters for tuning\n",
        "hyperparams = {\n",
        "    'hidden_size': [64, 128],\n",
        "    'num_layers': [1, 2],\n",
        "    'learning_rate': [0.001, 0.0001],\n",
        "    'num_epochs': [5]\n",
        "}\n",
        "\n",
        "# Start hyperparameter tuning\n",
        "hyperparameter_tuning(hyperparams, train_loader, test_loader, device)\n",
        "\n",
        "# Example 1: Combine TransformerModel and LSTMModel\n",
        "print(\"\\nTraining Combined Model 1: TransformerModel + LSTMModel\")\n",
        "model1 = TransformerModel(num_features=num_features, num_securities=num_securities, num_classes=num_classes)\n",
        "model2 = LSTMModel(input_size=second_X_data.shape[1], hidden_size=128, num_layers=2, num_securities=num_securities, num_classes=num_classes)\n",
        "combined_model = CombinedModel(model1, model2, num_classes, num_securities)\n",
        "optimizer = torch.optim.Adam(combined_model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "writer = SummaryWriter()\n",
        "train_model(combined_model, train_loader, criterion, optimizer, num_epochs=5, device=device, writer=writer)\n",
        "evaluate_model(combined_model, test_loader, device)\n",
        "writer.close()\n",
        "\n",
        "# Example 2: Combine CNNModel and SimpleRNNModel\n",
        "print(\"\\nTraining Combined Model 2: CNNModel + SimpleRNNModel\")\n",
        "model1 = CNNModel(num_features=num_features, num_securities=num_securities, num_classes=num_classes)\n",
        "model2 = SimpleRNNModel(input_size=second_X_data.shape[1], hidden_size=64, num_securities=num_securities, num_classes=num_classes)\n",
        "combined_model = CombinedModel(model1, model2, num_classes, num_securities)\n",
        "optimizer = torch.optim.Adam(combined_model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "writer = SummaryWriter()\n",
        "train_model(combined_model, train_loader, criterion, optimizer, num_epochs=5, device=device, writer=writer)\n",
        "evaluate_model(combined_model, test_loader, device)\n",
        "writer.close()\n",
        "\n",
        "# Example 3: Combine LogisticRegressionModel and LSTMModel\n",
        "print(\"\\nTraining Combined Model 3: LogisticRegressionModel + LSTMModel\")\n",
        "model1 = LogisticRegressionModel(input_size=num_features*window_size, num_securities=num_securities, num_classes=num_classes)\n",
        "model2 = LSTMModel(input_size=second_X_data.shape[1], hidden_size=128, num_layers=1, num_securities=num_securities, num_classes=num_classes)\n",
        "combined_model = CombinedModel(model1, model2, num_classes, num_securities)\n",
        "optimizer = torch.optim.Adam(combined_model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "writer = SummaryWriter()\n",
        "train_model(combined_model, train_loader, criterion, optimizer, num_epochs=5, device=device, writer=writer)\n",
        "evaluate_model(combined_model, test_loader, device)\n",
        "writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BO-t4USs2aJt",
        "outputId": "97e42d1c-edcd-4401-b534-6832093c0199"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Combined Model 1: GNNModel + TransformerModel\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Loss: 11.0505\n",
            "Epoch [2/5], Loss: 11.0070\n",
            "Epoch [3/5], Loss: 11.0030\n",
            "Epoch [4/5], Loss: 10.9930\n",
            "Epoch [5/5], Loss: 10.9843\n",
            "Accuracy on test set: 33.00%\n",
            "Cohen's Kappa: -0.0043\n",
            "\n",
            "Training Combined Model 2: MambaModel + LSTMModel\n",
            "Epoch [1/5], Loss: 11.0014\n",
            "Epoch [2/5], Loss: 10.9867\n",
            "Epoch [3/5], Loss: 10.9818\n",
            "Epoch [4/5], Loss: 10.9776\n",
            "Epoch [5/5], Loss: 10.9677\n",
            "Accuracy on test set: 33.00%\n",
            "Cohen's Kappa: -0.0045\n",
            "\n",
            "Training Combined Model 3: LiquidNetModel + CNNModel\n",
            "Epoch [1/5], Loss: 10.9973\n",
            "Epoch [2/5], Loss: 10.9749\n",
            "Epoch [3/5], Loss: 10.9603\n",
            "Epoch [4/5], Loss: 10.9382\n",
            "Epoch [5/5], Loss: 10.9085\n",
            "Accuracy on test set: 33.32%\n",
            "Cohen's Kappa: -0.0008\n",
            "\n",
            "Training Combined Model 4: HiddenMarkovModel + LogisticRegressionModel\n",
            "Epoch [1/5], Loss: 11.0670\n",
            "Epoch [2/5], Loss: 10.9946\n",
            "Epoch [3/5], Loss: 10.9829\n",
            "Epoch [4/5], Loss: 10.9717\n",
            "Epoch [5/5], Loss: 10.9646\n",
            "Accuracy on test set: 33.44%\n",
            "Cohen's Kappa: 0.0014\n",
            "\n",
            "Training Combined Model 5: GNNModel + MambaModel\n",
            "Epoch [1/5], Loss: 11.0025\n",
            "Epoch [2/5], Loss: 10.9864\n",
            "Epoch [3/5], Loss: 10.9820\n",
            "Epoch [4/5], Loss: 10.9792\n",
            "Epoch [5/5], Loss: 10.9707\n",
            "Accuracy on test set: 33.76%\n",
            "Cohen's Kappa: 0.0050\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from itertools import product\n",
        "import time\n",
        "\n",
        "# Parameters\n",
        "num_timesteps = 5000  # Total number of time steps\n",
        "num_securities = 10  # Number of different securities\n",
        "num_features_per_security = 4  # Features per security\n",
        "num_classes = 3  # Number of target classes\n",
        "num_features = num_securities * num_features_per_security  # Total number of features\n",
        "\n",
        "# Generate timestamps\n",
        "timestamps = np.arange(num_timesteps)\n",
        "\n",
        "# Generate random data for the primary dataset (X_data) and labels (y_data)\n",
        "X_data = np.random.rand(num_timesteps, num_features).astype(np.float32)  # Random features\n",
        "y_data = np.random.randint(0, num_classes, size=(num_timesteps, num_securities)).astype(np.int64)  # Random labels\n",
        "\n",
        "# Generate the second dataset with 3 categorical strings and 2 continuous floats per timestamp\n",
        "str_columns = ['str1', 'str2', 'str3']\n",
        "float_columns = ['float1', 'float2']\n",
        "second_dataset = {\n",
        "    'timestamp': timestamps,\n",
        "    'str1': np.random.choice(['A', 'B', 'C'], num_timesteps),\n",
        "    'str2': np.random.choice(['D', 'E', 'F'], num_timesteps),\n",
        "    'str3': np.random.choice(['G', 'H', 'I'], num_timesteps),\n",
        "    'float1': np.random.rand(num_timesteps),\n",
        "    'float2': np.random.rand(num_timesteps),\n",
        "}\n",
        "\n",
        "# Encode categorical string columns using LabelEncoder\n",
        "label_encoders = {}\n",
        "for col in str_columns:\n",
        "    le = LabelEncoder()\n",
        "    second_dataset[col] = le.fit_transform(second_dataset[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# Combine all features from the second dataset into a single array\n",
        "second_X_data = np.column_stack([second_dataset[col] for col in str_columns + float_columns]).astype(np.float32)\n",
        "\n",
        "# Apply Min-Max scaling to both datasets\n",
        "scaler_X = MinMaxScaler()\n",
        "X_data = scaler_X.fit_transform(X_data)\n",
        "\n",
        "scaler_second_X = MinMaxScaler()\n",
        "second_X_data = scaler_second_X.fit_transform(second_X_data)\n",
        "\n",
        "def add_positional_encoding(X, timestamps, option='shared'):\n",
        "    \"\"\"\n",
        "    Add positional encoding to the feature matrix.\n",
        "\n",
        "    Parameters:\n",
        "    - X (ndarray): Feature matrix.\n",
        "    - timestamps (ndarray): Array of timestamps.\n",
        "    - option (str): 'shared' or 'per_security'.\n",
        "\n",
        "    Returns:\n",
        "    - X_pe (ndarray): X with positional encoding added.\n",
        "    \"\"\"\n",
        "    if option == 'shared':\n",
        "        # Shared positional encoding across all features\n",
        "        pe = np.sin(timestamps[:, None] / 10000 ** (np.arange(X.shape[1]) / X.shape[1]))\n",
        "        X_pe = X + pe.astype(np.float32)\n",
        "    elif option == 'per_security':\n",
        "        # Separate positional encoding for each security\n",
        "        pe_list = []\n",
        "        for i in range(num_securities):\n",
        "            pe = np.sin(timestamps[:, None] / 10000 ** (np.arange(num_features_per_security) / num_features_per_security))\n",
        "            pe_list.append(pe)\n",
        "        pe_concat = np.hstack(pe_list)\n",
        "        X_pe = X + pe_concat.astype(np.float32)\n",
        "    else:\n",
        "        X_pe = X  # No positional encoding\n",
        "    return X_pe\n",
        "\n",
        "# Apply positional encoding\n",
        "positional_encoding_option = 'shared'  # 'shared' or 'per_security'\n",
        "X_data = add_positional_encoding(X_data, timestamps, positional_encoding_option)\n",
        "\n",
        "def create_windows(X1, X2, y, window_size, horizon):\n",
        "    \"\"\"\n",
        "    Create sliding windows for time series data.\n",
        "\n",
        "    Parameters:\n",
        "    - X1 (ndarray): Primary dataset features.\n",
        "    - X2 (ndarray): Secondary dataset features.\n",
        "    - y (ndarray): Target variable.\n",
        "    - window_size (int): Size of the window.\n",
        "    - horizon (int): Prediction horizon.\n",
        "\n",
        "    Returns:\n",
        "    - Tuple[np.ndarray, np.ndarray, np.ndarray]: Tuples of windows for X1, X2, and y.\n",
        "    \"\"\"\n",
        "    X1_windows = []\n",
        "    X2_windows = []\n",
        "    y_windows = []\n",
        "    for i in range(len(X1) - window_size - horizon + 1):\n",
        "        X1_windows.append(X1[i:i+window_size])\n",
        "        X2_windows.append(X2[i:i+window_size])\n",
        "        y_windows.append(y[i+window_size+horizon-1])\n",
        "    return np.array(X1_windows), np.array(X2_windows), np.array(y_windows)\n",
        "\n",
        "# Define window sizes\n",
        "window_size = 20\n",
        "horizon = 1\n",
        "\n",
        "# Create windows\n",
        "X1_windows, X2_windows, y_windows = create_windows(X_data, second_X_data, y_data, window_size, horizon)\n",
        "\n",
        "# Split into train and test sets\n",
        "train_ratio = 0.8\n",
        "train_size = int(len(X1_windows) * train_ratio)\n",
        "\n",
        "X1_train = X1_windows[:train_size]\n",
        "X2_train = X2_windows[:train_size]\n",
        "y_train = y_windows[:train_size]\n",
        "\n",
        "X1_test = X1_windows[train_size:]\n",
        "X2_test = X2_windows[train_size:]\n",
        "y_test = y_windows[train_size:]\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X1_train_tensor = torch.tensor(X1_train)\n",
        "X2_train_tensor = torch.tensor(X2_train)\n",
        "y_train_tensor = torch.tensor(y_train)\n",
        "\n",
        "X1_test_tensor = torch.tensor(X1_test)\n",
        "X2_test_tensor = torch.tensor(X2_test)\n",
        "y_test_tensor = torch.tensor(y_test)\n",
        "\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom Dataset for time series data with two feature sets.\n",
        "\n",
        "    Parameters:\n",
        "    - X1 (torch.Tensor): Primary dataset features.\n",
        "    - X2 (torch.Tensor): Secondary dataset features.\n",
        "    - y (torch.Tensor): Target variable.\n",
        "    \"\"\"\n",
        "    def __init__(self, X1, X2, y):\n",
        "        self.X1 = X1.float()\n",
        "        self.X2 = X2.float()\n",
        "        self.y = y.long()\n",
        "    def __len__(self):\n",
        "        return len(self.X1)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X1[idx], self.X2[idx], self.y[idx]\n",
        "\n",
        "# DataLoaders\n",
        "batch_size = 64\n",
        "\n",
        "train_dataset = TimeSeriesDataset(X1_train_tensor, X2_train_tensor, y_train_tensor)\n",
        "test_dataset = TimeSeriesDataset(X1_test_tensor, X2_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "class MatrixRegressionModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Matrix Regression Model: Y = A * X * B\n",
        "\n",
        "    Advantages:\n",
        "    - Simple and interpretable.\n",
        "    - Fast training.\n",
        "\n",
        "    Disadvantages:\n",
        "    - Limited in capturing complex patterns.\n",
        "\n",
        "    Parameters:\n",
        "    - num_securities (int): Number of securities.\n",
        "    - num_features (int): Total number of features.\n",
        "    - num_classes (int): Number of target classes.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_securities, num_features, num_classes):\n",
        "        super(MatrixRegressionModel, self).__init__()\n",
        "        self.A = nn.Parameter(torch.randn(1, num_securities))\n",
        "        self.B = nn.Parameter(torch.randn(num_features, num_classes))\n",
        "        self.num_securities = num_securities\n",
        "        self.num_classes = num_classes\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)  # Flatten over time\n",
        "        out = self.A @ x @ self.B  # Shape: (batch_size, num_classes)\n",
        "        out = out.view(-1, self.num_securities, self.num_classes)\n",
        "        return out\n",
        "\n",
        "class LogisticRegressionModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Logistic Regression Model.\n",
        "\n",
        "    Advantages:\n",
        "    - Simple and interpretable.\n",
        "    - Good baseline model.\n",
        "\n",
        "    Disadvantages:\n",
        "    - Assumes linear relationship.\n",
        "    - May underfit complex data.\n",
        "\n",
        "    Parameters:\n",
        "    - input_size (int): Size of input features.\n",
        "    - num_securities (int): Number of securities.\n",
        "    - num_classes (int): Number of target classes.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, num_securities, num_classes):\n",
        "        super(LogisticRegressionModel, self).__init__()\n",
        "        self.num_securities = num_securities\n",
        "        self.num_classes = num_classes\n",
        "        self.linear = nn.Linear(input_size, num_securities * num_classes)\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        out = self.linear(x)\n",
        "        out = out.view(-1, self.num_securities, self.num_classes)\n",
        "        return out\n",
        "\n",
        "class CNNModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Convolutional Neural Network Model.\n",
        "\n",
        "    Advantages:\n",
        "    - Captures local temporal patterns.\n",
        "    - Efficient computation.\n",
        "\n",
        "    Disadvantages:\n",
        "    - Limited in capturing long-term dependencies.\n",
        "\n",
        "    Parameters:\n",
        "    - num_features (int): Total number of features.\n",
        "    - num_securities (int): Number of securities.\n",
        "    - num_classes (int): Number of target classes.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_features, num_securities, num_classes):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.num_securities = num_securities\n",
        "        self.num_classes = num_classes\n",
        "        self.conv1 = nn.Conv1d(in_channels=num_features, out_channels=64, kernel_size=3)\n",
        "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc = nn.Linear(128, num_securities * num_classes)\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)  # (batch_size, num_features, window_size)\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = torch.mean(x, dim=2)  # Global average pooling\n",
        "        x = self.fc(x)\n",
        "        out = x.view(-1, self.num_securities, self.num_classes)\n",
        "        return out\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Long Short-Term Memory Model.\n",
        "\n",
        "    Advantages:\n",
        "    - Captures long-term dependencies.\n",
        "    - Suitable for sequential data.\n",
        "\n",
        "    Disadvantages:\n",
        "    - Computationally intensive.\n",
        "    - Prone to overfitting.\n",
        "\n",
        "    Parameters:\n",
        "    - input_size (int): Size of input features.\n",
        "    - hidden_size (int): Size of hidden state.\n",
        "    - num_layers (int): Number of LSTM layers.\n",
        "    - num_securities (int): Number of securities.\n",
        "    - num_classes (int): Number of target classes.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_securities, num_classes):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.num_securities = num_securities\n",
        "        self.num_classes = num_classes\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_securities * num_classes)\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = out[:, -1, :]  # Last time step\n",
        "        out = self.fc(out)\n",
        "        out = out.view(-1, self.num_securities, self.num_classes)\n",
        "        return out\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer Model.\n",
        "\n",
        "    Advantages:\n",
        "    - Captures global dependencies.\n",
        "    - Parallel computation.\n",
        "\n",
        "    Disadvantages:\n",
        "    - Requires large datasets.\n",
        "    - Computationally intensive.\n",
        "\n",
        "    Parameters:\n",
        "    - num_features (int): Total number of features.\n",
        "    - num_securities (int): Number of securities.\n",
        "    - num_classes (int): Number of target classes.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_features, num_securities, num_classes):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.num_securities = num_securities\n",
        "        self.num_classes = num_classes\n",
        "        self.embedding = nn.Linear(num_features, 128)\n",
        "        self.pos_encoder = PositionalEncoding(128)\n",
        "        encoder_layers = nn.TransformerEncoderLayer(d_model=128, nhead=8)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=2)\n",
        "        self.decoder = nn.Linear(128, num_securities * num_classes)\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), x.size(1), -1)  # (batch_size, window_size, num_features)\n",
        "        x = self.embedding(x)\n",
        "        x = self.pos_encoder(x)\n",
        "        x = x.permute(1, 0, 2)  # (sequence_length, batch_size, embedding_dim)\n",
        "        output = self.transformer_encoder(x)\n",
        "        output = output[-1, :, :]  # Last output\n",
        "        output = self.decoder(output)\n",
        "        output = output.view(-1, self.num_securities, self.num_classes)\n",
        "        return output\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Positional Encoding for Transformer.\n",
        "\n",
        "    Adds positional information to the embeddings.\n",
        "\n",
        "    Parameters:\n",
        "    - d_model (int): Embedding dimension.\n",
        "    - max_len (int): Maximum sequence length.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        if d_model % 2 == 1:\n",
        "            pe[:, 1::2] = torch.sin(position * div_term)\n",
        "        else:\n",
        "            pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(1)\n",
        "        self.register_buffer('pe', pe)\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return x\n",
        "\n",
        "class SimpleRNNModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple Recurrent Neural Network Model.\n",
        "\n",
        "    Advantages:\n",
        "    - Captures sequential dependencies.\n",
        "    - Simpler than LSTM.\n",
        "\n",
        "    Disadvantages:\n",
        "    - Struggles with long-term dependencies.\n",
        "    - May suffer from vanishing gradients.\n",
        "\n",
        "    Parameters:\n",
        "    - input_size (int): Size of input features.\n",
        "    - hidden_size (int): Size of hidden state.\n",
        "    - num_securities (int): Number of securities.\n",
        "    - num_classes (int): Number of target classes.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size, num_securities, num_classes):\n",
        "        super(SimpleRNNModel, self).__init__()\n",
        "        self.num_securities = num_securities\n",
        "        self.num_classes = num_classes\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_securities * num_classes)\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n",
        "        out, _ = self.rnn(x, h0)\n",
        "        out = out[:, -1, :]  # Last time step\n",
        "        out = self.fc(out)\n",
        "        out = out.view(-1, self.num_securities, self.num_classes)\n",
        "        return out\n",
        "\n",
        "# Missing Models with Docstrings and Comments\n",
        "\n",
        "class GNNModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Graph Neural Network Model.\n",
        "\n",
        "    Advantages:\n",
        "    - Captures relationships between securities.\n",
        "    - Utilizes attention mechanisms.\n",
        "\n",
        "    Disadvantages:\n",
        "    - Computationally intensive.\n",
        "    - Requires graph structure.\n",
        "\n",
        "    Parameters:\n",
        "    - num_securities (int): Number of securities.\n",
        "    - num_features_per_security (int): Features per security.\n",
        "    - num_classes (int): Number of target classes.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_securities, num_features_per_security, num_classes):\n",
        "        super(GNNModel, self).__init__()\n",
        "        self.num_securities = num_securities\n",
        "        self.num_classes = num_classes\n",
        "        # Create a linear layer for each security\n",
        "        self.gcn_layers = nn.ModuleList([nn.Linear(num_features_per_security, 64) for _ in range(num_securities)])\n",
        "        # Multi-head attention mechanism\n",
        "        self.attention = nn.MultiheadAttention(embed_dim=64, num_heads=4, batch_first=True)\n",
        "        # Fully connected layer for classification\n",
        "        self.fc = nn.Linear(64, num_classes)\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, window_size, num_features)\n",
        "        batch_size = x.size(0)\n",
        "        x = x[:, -1, :]  # Use the last time step\n",
        "        x = x.view(batch_size, self.num_securities, -1)  # (batch_size, num_securities, num_features_per_security)\n",
        "        node_embeddings = []\n",
        "        for i in range(self.num_securities):\n",
        "            h = self.gcn_layers[i](x[:, i, :])  # (batch_size, 64)\n",
        "            node_embeddings.append(h)\n",
        "        h = torch.stack(node_embeddings, dim=1)  # (batch_size, num_securities, 64)\n",
        "        # Apply attention mechanism\n",
        "        attn_output, _ = self.attention(h, h, h)\n",
        "        out = self.fc(attn_output)  # (batch_size, num_securities, num_classes)\n",
        "        return out\n",
        "\n",
        "class MambaModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Mamba Model: Combines CNN and LSTM architectures.\n",
        "\n",
        "    Advantages:\n",
        "    - Captures both local and long-term dependencies.\n",
        "    - Utilizes CNN for feature extraction and LSTM for temporal patterns.\n",
        "\n",
        "    Disadvantages:\n",
        "    - More complex and computationally intensive.\n",
        "\n",
        "    Parameters:\n",
        "    - num_features (int): Total number of features.\n",
        "    - num_securities (int): Number of securities.\n",
        "    - num_classes (int): Number of target classes.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_features, num_securities, num_classes):\n",
        "        super(MambaModel, self).__init__()\n",
        "        self.num_securities = num_securities\n",
        "        self.num_classes = num_classes\n",
        "        # Convolutional layer\n",
        "        self.cnn = nn.Conv1d(in_channels=num_features, out_channels=64, kernel_size=3)\n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(input_size=64, hidden_size=128, num_layers=2, batch_first=True)\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(128, num_securities * num_classes)\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, window_size, num_features)\n",
        "        x = x.permute(0, 2, 1)  # (batch_size, num_features, window_size)\n",
        "        x = F.relu(self.cnn(x))  # Apply CNN\n",
        "        x = x.permute(0, 2, 1)  # (batch_size, new_window_size, channels)\n",
        "        h0 = torch.zeros(2, x.size(0), 128).to(x.device)  # Initial hidden state\n",
        "        c0 = torch.zeros(2, x.size(0), 128).to(x.device)  # Initial cell state\n",
        "        out, _ = self.lstm(x, (h0, c0))  # Apply LSTM\n",
        "        out = out[:, -1, :]  # Get the last output\n",
        "        out = self.fc(out)\n",
        "        out = out.view(-1, self.num_securities, self.num_classes)\n",
        "        return out\n",
        "\n",
        "class LiquidNetModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Liquid Neural Network Model.\n",
        "\n",
        "    Advantages:\n",
        "    - Dynamic adaptation to inputs.\n",
        "    - Good for time-series data.\n",
        "\n",
        "    Disadvantages:\n",
        "    - Relatively new, less tested.\n",
        "    - May be complex to tune.\n",
        "\n",
        "    Parameters:\n",
        "    - input_size (int): Size of input features.\n",
        "    - hidden_size (int): Size of hidden state.\n",
        "    - num_securities (int): Number of securities.\n",
        "    - num_classes (int): Number of target classes.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size, num_securities, num_classes):\n",
        "        super(LiquidNetModel, self).__init__()\n",
        "        self.num_securities = num_securities\n",
        "        self.num_classes = num_classes\n",
        "        # RNN Cell with ReLU activation\n",
        "        self.rnn_cell = nn.RNNCell(input_size, hidden_size, nonlinearity='relu')\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(hidden_size, num_securities * num_classes)\n",
        "    def forward(self, x):\n",
        "        h_t = torch.zeros(x.size(0), self.rnn_cell.hidden_size).to(x.device)  # Initial hidden state\n",
        "        for t in range(x.size(1)):\n",
        "            h_t = self.rnn_cell(x[:, t, :], h_t)  # Update hidden state\n",
        "        out = self.fc(h_t)\n",
        "        out = out.view(-1, self.num_securities, self.num_classes)\n",
        "        return out\n",
        "\n",
        "class HiddenMarkovModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Hidden Markov Model.\n",
        "\n",
        "    Advantages:\n",
        "    - Probabilistic approach.\n",
        "    - Good for sequential data.\n",
        "\n",
        "    Disadvantages:\n",
        "    - Simplified version here.\n",
        "    - May not capture complex patterns.\n",
        "\n",
        "    Parameters:\n",
        "    - num_states (int): Number of hidden states.\n",
        "    - num_securities (int): Number of securities.\n",
        "    - num_classes (int): Number of target classes.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_states, num_securities, num_classes):\n",
        "        super(HiddenMarkovModel, self).__init__()\n",
        "        self.num_states = num_states\n",
        "        self.num_securities = num_securities\n",
        "        self.num_classes = num_classes\n",
        "        # Start probabilities\n",
        "        self.start_prob = nn.Parameter(torch.randn(num_states))\n",
        "        # Transition probabilities\n",
        "        self.transition_prob = nn.Parameter(torch.randn(num_states, num_states))\n",
        "        # Emission probabilities\n",
        "        self.emission_prob = nn.Parameter(torch.randn(num_states, num_classes))\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        # Simplified; in practice, you'd implement the forward algorithm\n",
        "        out = torch.softmax(self.emission_prob, dim=1)\n",
        "        # Corrected repeat dimensions to match tensor dimensions\n",
        "        out = out.unsqueeze(0).unsqueeze(0).repeat(batch_size, self.num_securities, 1, 1)\n",
        "        # Since the model expects output shape (batch_size, num_securities, num_classes)\n",
        "        # We need to aggregate over the hidden states (num_states)\n",
        "        out = torch.mean(out, dim=2)  # Average over hidden states\n",
        "        return out\n",
        "\n",
        "class CombinedModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Combined Model: Merges outputs from two models.\n",
        "\n",
        "    Advantages:\n",
        "    - Leverages multiple data sources.\n",
        "    - Potentially better performance.\n",
        "\n",
        "    Disadvantages:\n",
        "    - More complex.\n",
        "    - Computationally intensive.\n",
        "\n",
        "    Parameters:\n",
        "    - model1 (nn.Module): First model.\n",
        "    - model2 (nn.Module): Second model.\n",
        "    - num_classes (int): Number of target classes.\n",
        "    - num_securities (int): Number of securities.\n",
        "    \"\"\"\n",
        "    def __init__(self, model1, model2, num_classes, num_securities):\n",
        "        super(CombinedModel, self).__init__()\n",
        "        self.model1 = model1\n",
        "        self.model2 = model2\n",
        "        self.fc = nn.Linear(num_classes * 2, num_classes)\n",
        "        self.num_securities = num_securities\n",
        "        self.num_classes = num_classes\n",
        "    def forward(self, x1, x2):\n",
        "        outputs1 = self.model1(x1)\n",
        "        outputs2 = self.model2(x2)\n",
        "        outputs = torch.cat((outputs1, outputs2), dim=2)\n",
        "        batch_size = outputs.size(0)\n",
        "        outputs = outputs.view(-1, outputs.size(2))\n",
        "        final_output = self.fc(outputs)\n",
        "        final_output = final_output.view(batch_size, self.num_securities, -1)\n",
        "        return final_output\n",
        "\n",
        "def train_model(model, train_loader, criterion, optimizer, num_epochs, device, writer):\n",
        "    \"\"\"\n",
        "    Training loop for the model.\n",
        "\n",
        "    Parameters:\n",
        "    - model (nn.Module): The model to train.\n",
        "    - train_loader (DataLoader): DataLoader for training data.\n",
        "    - criterion (nn.Module): Loss function.\n",
        "    - optimizer (torch.optim.Optimizer): Optimizer.\n",
        "    - num_epochs (int): Number of epochs.\n",
        "    - device (torch.device): Computation device.\n",
        "    - writer (SummaryWriter): TensorBoard SummaryWriter.\n",
        "    \"\"\"\n",
        "    model = model.to(device)\n",
        "    global_step = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for X1_batch, X2_batch, y_batch in train_loader:\n",
        "            X1_batch = X1_batch.to(device)\n",
        "            X2_batch = X2_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X1_batch, X2_batch)\n",
        "            loss = 0\n",
        "            for i in range(num_securities):\n",
        "                loss += criterion(outputs[:, i, :], y_batch[:, i])\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            writer.add_scalar('Training Loss', loss.item(), global_step)\n",
        "            global_step += 1\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
        "        # Save checkpoint\n",
        "        torch.save(model.state_dict(), f'checkpoint_epoch_{epoch+1}.pth')\n",
        "\n",
        "def evaluate_model(model, test_loader, device):\n",
        "    \"\"\"\n",
        "    Evaluate the model on the test set.\n",
        "\n",
        "    Parameters:\n",
        "    - model (nn.Module): The model to evaluate.\n",
        "    - test_loader (DataLoader): DataLoader for test data.\n",
        "    - device (torch.device): Computation device.\n",
        "\n",
        "    Returns:\n",
        "    - float: Cohen's kappa score.\n",
        "    \"\"\"\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    with torch.no_grad():\n",
        "        for X1_batch, X2_batch, y_batch in test_loader:\n",
        "            X1_batch = X1_batch.to(device)\n",
        "            X2_batch = X2_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "            outputs = model(X1_batch, X2_batch)\n",
        "            _, predicted = torch.max(outputs.data, 2)\n",
        "            total += y_batch.numel()\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "            all_preds.extend(predicted.cpu().numpy().flatten())\n",
        "            all_targets.extend(y_batch.cpu().numpy().flatten())\n",
        "    accuracy = 100 * correct / total\n",
        "    kappa = cohen_kappa_score(all_targets, all_preds)\n",
        "    print(f'Accuracy on test set: {accuracy:.2f}%')\n",
        "    print(f\"Cohen's Kappa: {kappa:.4f}\")\n",
        "    return kappa\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Examples of combination models using the new models\n",
        "\n",
        "# Example 1: Combine GNNModel and TransformerModel\n",
        "print(\"\\nTraining Combined Model 1: GNNModel + TransformerModel\")\n",
        "model1 = GNNModel(num_securities=num_securities, num_features_per_security=num_features_per_security, num_classes=num_classes)\n",
        "model2 = TransformerModel(num_features=second_X_data.shape[1], num_securities=num_securities, num_classes=num_classes)\n",
        "combined_model = CombinedModel(model1, model2, num_classes, num_securities)\n",
        "optimizer = torch.optim.Adam(combined_model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "writer = SummaryWriter()\n",
        "train_model(combined_model, train_loader, criterion, optimizer, num_epochs=5, device=device, writer=writer)\n",
        "evaluate_model(combined_model, test_loader, device)\n",
        "writer.close()\n",
        "\n",
        "# Example 2: Combine MambaModel and LSTMModel\n",
        "print(\"\\nTraining Combined Model 2: MambaModel + LSTMModel\")\n",
        "model1 = MambaModel(num_features=num_features, num_securities=num_securities, num_classes=num_classes)\n",
        "model2 = LSTMModel(input_size=second_X_data.shape[1], hidden_size=64, num_layers=1, num_securities=num_securities, num_classes=num_classes)\n",
        "combined_model = CombinedModel(model1, model2, num_classes, num_securities)\n",
        "optimizer = torch.optim.Adam(combined_model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "writer = SummaryWriter()\n",
        "train_model(combined_model, train_loader, criterion, optimizer, num_epochs=5, device=device, writer=writer)\n",
        "evaluate_model(combined_model, test_loader, device)\n",
        "writer.close()\n",
        "\n",
        "# Example 3: Combine LiquidNetModel and CNNModel\n",
        "print(\"\\nTraining Combined Model 3: LiquidNetModel + CNNModel\")\n",
        "model1 = LiquidNetModel(input_size=num_features, hidden_size=128, num_securities=num_securities, num_classes=num_classes)\n",
        "model2 = CNNModel(num_features=second_X_data.shape[1], num_securities=num_securities, num_classes=num_classes)\n",
        "combined_model = CombinedModel(model1, model2, num_classes, num_securities)\n",
        "optimizer = torch.optim.Adam(combined_model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "writer = SummaryWriter()\n",
        "train_model(combined_model, train_loader, criterion, optimizer, num_epochs=5, device=device, writer=writer)\n",
        "evaluate_model(combined_model, test_loader, device)\n",
        "writer.close()\n",
        "\n",
        "# Example 4: Combine HiddenMarkovModel and LogisticRegressionModel\n",
        "print(\"\\nTraining Combined Model 4: HiddenMarkovModel + LogisticRegressionModel\")\n",
        "model1 = HiddenMarkovModel(num_states=5, num_securities=num_securities, num_classes=num_classes)\n",
        "model2 = LogisticRegressionModel(input_size=second_X_data.shape[1]*window_size, num_securities=num_securities, num_classes=num_classes)\n",
        "combined_model = CombinedModel(model1, model2, num_classes, num_securities)\n",
        "optimizer = torch.optim.Adam(combined_model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "writer = SummaryWriter()\n",
        "train_model(combined_model, train_loader, criterion, optimizer, num_epochs=5, device=device, writer=writer)\n",
        "evaluate_model(combined_model, test_loader, device)\n",
        "writer.close()\n",
        "\n",
        "# Example 5: Combine GNNModel and MambaModel\n",
        "print(\"\\nTraining Combined Model 5: GNNModel + MambaModel\")\n",
        "model1 = GNNModel(num_securities=num_securities, num_features_per_security=num_features_per_security, num_classes=num_classes)\n",
        "model2 = MambaModel(num_features=second_X_data.shape[1], num_securities=num_securities, num_classes=num_classes)\n",
        "combined_model = CombinedModel(model1, model2, num_classes, num_securities)\n",
        "optimizer = torch.optim.Adam(combined_model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "writer = SummaryWriter()\n",
        "train_model(combined_model, train_loader, criterion, optimizer, num_epochs=5, device=device, writer=writer)\n",
        "evaluate_model(combined_model, test_loader, device)\n",
        "writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_ZCKOPo0MRM",
        "outputId": "2c2878d8-a5ca-4030-feca-75735a27c6df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Combined Model with Transformer and LSTM...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Loss: 11.0751\n",
            "Epoch [2/5], Loss: 11.0117\n",
            "Epoch [3/5], Loss: 11.0131\n",
            "Epoch [4/5], Loss: 11.0011\n",
            "Epoch [5/5], Loss: 10.9982\n",
            "Accuracy on test set: 32.97%\n"
          ]
        }
      ],
      "source": [
        "# ... [Previous code remains the same up to model definitions]\n",
        "\n",
        "# Instantiate models for primary and second datasets\n",
        "print(\"Training Combined Model with Transformer and LSTM...\")\n",
        "\n",
        "# Primary model\n",
        "primary_model = TransformerModel(num_features=num_features, num_securities=num_securities, num_classes=num_classes)\n",
        "# Secondary model\n",
        "secondary_model = LSTMModel(input_size=second_input_size, hidden_size=128, num_layers=2, num_securities=num_securities, num_classes=num_classes)\n",
        "\n",
        "# Define final linear layer\n",
        "class FinalModel(nn.Module):\n",
        "    def __init__(self, num_classes, num_securities):\n",
        "        super(FinalModel, self).__init__()\n",
        "        self.fc = nn.Linear(num_classes * 2, num_classes)\n",
        "        self.num_securities = num_securities\n",
        "    def forward(self, outputs1, outputs2):\n",
        "        # Concatenate over class dimension\n",
        "        outputs = torch.cat((outputs1, outputs2), dim=2)\n",
        "        # Pass through linear layer\n",
        "        batch_size = outputs.size(0)\n",
        "        outputs = outputs.view(-1, outputs.size(2))\n",
        "        final_output = self.fc(outputs)\n",
        "        final_output = final_output.view(batch_size, self.num_securities, -1)\n",
        "        return final_output\n",
        "\n",
        "final_model = FinalModel(num_classes=num_classes, num_securities=num_securities)\n",
        "\n",
        "# Combine the parameters of all models\n",
        "params = list(primary_model.parameters()) + list(secondary_model.parameters()) + list(final_model.parameters())\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params, lr=learning_rate)\n",
        "\n",
        "# Updated training function\n",
        "def train_model(model1, model2, final_model, train_loader, criterion, optimizer, num_epochs, device):\n",
        "    model1 = model1.to(device)\n",
        "    model2 = model2.to(device)\n",
        "    final_model = final_model.to(device)\n",
        "    for epoch in range(num_epochs):\n",
        "        model1.train()\n",
        "        model2.train()\n",
        "        final_model.train()\n",
        "        total_loss = 0\n",
        "        for X1_batch, X2_batch, y_batch in train_loader:\n",
        "            X1_batch = X1_batch.to(device)\n",
        "            X2_batch = X2_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs1 = model1(X1_batch)\n",
        "            outputs2 = model2(X2_batch)\n",
        "            final_output = final_model(outputs1, outputs2)\n",
        "            loss = 0\n",
        "            for i in range(num_securities):\n",
        "                loss += criterion(final_output[:, i, :], y_batch[:, i])\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
        "\n",
        "# Updated evaluation function\n",
        "def evaluate_model(model1, model2, final_model, test_loader, device):\n",
        "    model1 = model1.to(device)\n",
        "    model2 = model2.to(device)\n",
        "    final_model = final_model.to(device)\n",
        "    model1.eval()\n",
        "    model2.eval()\n",
        "    final_model.eval()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for X1_batch, X2_batch, y_batch in test_loader:\n",
        "            X1_batch = X1_batch.to(device)\n",
        "            X2_batch = X2_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "            outputs1 = model1(X1_batch)\n",
        "            outputs2 = model2(X2_batch)\n",
        "            final_output = final_model(outputs1, outputs2)\n",
        "            _, predicted = torch.max(final_output.data, 2)\n",
        "            total += y_batch.numel()\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy on test set: {accuracy:.2f}%')\n",
        "\n",
        "# Start training\n",
        "train_model(primary_model, secondary_model, final_model, train_loader, criterion, optimizer, num_epochs, device)\n",
        "evaluate_model(primary_model, secondary_model, final_model, test_loader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6sf3SVposU0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "# Time2Vec implementation\n",
        "class Time2Vec(nn.Module):\n",
        "    def __init__(self, seq_len, d_model):\n",
        "        super(Time2Vec, self).__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.d_model = d_model\n",
        "        self.w0 = nn.Parameter(torch.randn(1))\n",
        "        self.b0 = nn.Parameter(torch.randn(1))\n",
        "        self.w = nn.Parameter(torch.randn(d_model - 1))\n",
        "        self.b = nn.Parameter(torch.randn(d_model - 1))\n",
        "\n",
        "    def forward(self, batch_size):\n",
        "        # Create time indices\n",
        "        t = torch.arange(self.seq_len).unsqueeze(-1).float().to(self.w0.device)  # Shape: (seq_len, 1)\n",
        "        # Linear component\n",
        "        v = self.w0 * t + self.b0  # Shape: (seq_len, 1)\n",
        "        # Periodic component\n",
        "        vp = torch.sin(self.w * t + self.b)  # Broadcasting over (seq_len, d_model - 1)\n",
        "        # Concatenate components\n",
        "        time_emb = torch.cat([v, vp], dim=-1)  # Shape: (seq_len, d_model)\n",
        "        # Expand to match batch size\n",
        "        time_emb = time_emb.unsqueeze(0).expand(batch_size, -1, -1)  # Shape: (batch_size, seq_len, d_model)\n",
        "        return time_emb\n",
        "\n",
        "# Corrected Multi-Head Attention Mechanism\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        self.q_linear = nn.Linear(d_model, d_model)\n",
        "        self.k_linear = nn.Linear(d_model, d_model)\n",
        "        self.v_linear = nn.Linear(d_model, d_model)\n",
        "        self.out_linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        batch_size = q.size(0)\n",
        "\n",
        "        # Perform linear projections and split into heads\n",
        "        q = self.q_linear(q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2)\n",
        "        k = self.k_linear(k).view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2)\n",
        "        v = self.v_linear(v).view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2)\n",
        "\n",
        "        # Calculate attention scores\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / np.sqrt(self.d_k)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        attn_weights = torch.softmax(scores, dim=-1)\n",
        "        context = torch.matmul(attn_weights, v)\n",
        "\n",
        "        # Concatenate heads\n",
        "        context = context.transpose(1,2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n",
        "        output = self.out_linear(context)\n",
        "\n",
        "        return output\n",
        "\n",
        "# Corrected Gated Residual Network\n",
        "class GatedResidualNetwork(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(GatedResidualNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.elu = nn.ELU()\n",
        "        self.fc2 = nn.Linear(hidden_size, 2 * input_size)  # Output size doubled for GLU\n",
        "        self.glu = nn.GLU(dim=-1)  # Splits last dimension into two halves\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        x = self.fc1(x)\n",
        "        x = self.elu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.glu(x)\n",
        "        return residual + x\n",
        "\n",
        "# Corrected Transformer Encoder Block\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, hidden_size):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
        "        self.grn = GatedResidualNetwork(d_model, hidden_size)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        attn_output = self.attention(x, x, x, mask)\n",
        "        x = self.norm1(x + attn_output)\n",
        "        grn_output = self.grn(x)\n",
        "        x = self.norm2(x + grn_output)\n",
        "        return x\n",
        "\n",
        "# Corrected Transformer Decoder Block\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, hidden_size):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.self_attention = MultiHeadAttention(d_model, num_heads)\n",
        "        self.cross_attention = MultiHeadAttention(d_model, num_heads)\n",
        "        self.grn = GatedResidualNetwork(d_model, hidden_size)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
        "        # Self-attention with masking\n",
        "        attn_output = self.self_attention(x, x, x, tgt_mask)\n",
        "        x = self.norm1(x + attn_output)\n",
        "\n",
        "        # Cross-attention with encoder outputs\n",
        "        attn_output = self.cross_attention(x, enc_output, enc_output, src_mask)\n",
        "        x = self.norm2(x + attn_output)\n",
        "\n",
        "        # Apply GRN and residual connection\n",
        "        grn_output = self.grn(x)\n",
        "        x = self.norm3(x + grn_output)\n",
        "        return x\n",
        "\n",
        "# Corrected Portfolio Transformer Model\n",
        "class PortfolioTransformer(nn.Module):\n",
        "    def __init__(self, num_assets, d_model, num_heads, hidden_size, num_layers, seq_len):\n",
        "        super(PortfolioTransformer, self).__init__()\n",
        "        self.num_assets = num_assets\n",
        "        self.d_model = d_model\n",
        "        self.seq_len = seq_len\n",
        "        self.input_proj = nn.Linear(num_assets, d_model)\n",
        "        self.time2vec = Time2Vec(seq_len, d_model)\n",
        "        self.encoder_layers = nn.ModuleList(\n",
        "            [TransformerEncoder(d_model, num_heads, hidden_size) for _ in range(num_layers)]\n",
        "        )\n",
        "        self.decoder_layers = nn.ModuleList(\n",
        "            [TransformerDecoder(d_model, num_heads, hidden_size) for _ in range(num_layers)]\n",
        "        )\n",
        "        self.fc_out = nn.Linear(d_model, num_assets)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_len, num_assets)\n",
        "        batch_size = x.size(0)\n",
        "        # Project input features to d_model\n",
        "        x_proj = self.input_proj(x)  # Shape: (batch_size, seq_len, d_model)\n",
        "        # Get time embeddings\n",
        "        time_emb = self.time2vec(batch_size)  # Shape: (batch_size, seq_len, d_model)\n",
        "        # Combine input embeddings and time embeddings\n",
        "        x = x_proj + time_emb  # Element-wise addition\n",
        "        # Encoder\n",
        "        for encoder in self.encoder_layers:\n",
        "            x = encoder(x)\n",
        "        # Decoder\n",
        "        dec_input = x\n",
        "        for decoder in self.decoder_layers:\n",
        "            x = decoder(dec_input, x)\n",
        "        s_i_t = self.fc_out(x)  # Output shape: (batch_size, seq_len, num_assets)\n",
        "        # Compute weights using the compound function\n",
        "        weights = torch.sign(s_i_t) * torch.softmax(torch.abs(s_i_t), dim=-1)\n",
        "        return weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrPdsSy0o3YJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Generate random returns data: 100 securities over 1000 time steps\n",
        "num_securities = 100\n",
        "num_time_steps = 1000\n",
        "np.random.seed(42)  # For reproducibility\n",
        "\n",
        "# Simulate random returns\n",
        "returns_data = np.random.randn(num_time_steps, num_securities) * 0.01  # Small random returns\n",
        "prices = 100 + np.cumsum(returns_data, axis=0)  # Simulate price paths\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "prices = torch.tensor(prices, dtype=torch.float32)\n",
        "returns = torch.tensor(returns_data, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTj0mZgqo5xO",
        "outputId": "819e1e54-1d45-4619-f621-f2dd92818dec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Loss: 0.0227\n",
            "Epoch 2/10, Loss: -0.1482\n",
            "Epoch 3/10, Loss: -0.1936\n",
            "Epoch 4/10, Loss: -0.2280\n",
            "Epoch 5/10, Loss: -0.2235\n",
            "Epoch 6/10, Loss: -0.2329\n",
            "Epoch 7/10, Loss: -0.2364\n",
            "Epoch 8/10, Loss: -0.2379\n",
            "Epoch 9/10, Loss: -0.2571\n",
            "Epoch 10/10, Loss: -0.2559\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Prepare the dataset\n",
        "sequence_length = 20  # Use last 20 days for prediction\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for i in range(len(prices) - sequence_length):\n",
        "    X.append(returns[i:i+sequence_length])\n",
        "    y.append(returns[i+sequence_length])\n",
        "\n",
        "X = torch.stack(X)  # Shape: (num_samples, seq_len, num_assets)\n",
        "y = torch.stack(y)  # Shape: (num_samples, num_assets)\n",
        "\n",
        "dataset = TensorDataset(X, y)\n",
        "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Initialize the model\n",
        "num_assets = num_securities\n",
        "model = PortfolioTransformer(\n",
        "    num_assets=num_assets, d_model=64, num_heads=8, hidden_size=128, num_layers=4, seq_len=sequence_length\n",
        ")\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "def train(model, optimizer, data_loader, num_epochs):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for batch_X, batch_y in data_loader:\n",
        "            optimizer.zero_grad()\n",
        "            portfolio_weights = model(batch_X)  # Output shape: (batch_size, seq_len, num_assets)\n",
        "            # Use the last time step's weights\n",
        "            portfolio_weights = portfolio_weights[:, -1, :]  # Shape: (batch_size, num_assets)\n",
        "            # Compute portfolio returns\n",
        "            portfolio_returns = torch.sum(portfolio_weights * batch_y, dim=1)\n",
        "            loss = sharpe_loss(portfolio_returns)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(data_loader):.4f}\")\n",
        "\n",
        "# Loss function (Negative Sharpe Ratio)\n",
        "def sharpe_loss(portfolio_returns, risk_free_rate=0.0):\n",
        "    # Assuming daily returns, annualize by multiplying by sqrt(252)\n",
        "    mean_return = torch.mean(portfolio_returns)\n",
        "    std_return = torch.std(portfolio_returns)\n",
        "    sharpe_ratio = (mean_return - risk_free_rate) / (std_return + 1e-6)\n",
        "    return -sharpe_ratio  # Negative for minimization\n",
        "\n",
        "# Train the model\n",
        "train(model, optimizer, data_loader, num_epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lzRyXdVtM2g"
      },
      "outputs": [],
      "source": [
        "## version 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udFpzIeosoq6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "# Advanced Time Encoding: Learnable Positional Encoding\n",
        "class LearnablePositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements learnable positional encoding for sequences.\n",
        "\n",
        "    Args:\n",
        "        seq_len (int): The maximum length of the input sequences.\n",
        "        d_model (int): The dimension of the model.\n",
        "    \"\"\"\n",
        "    def __init__(self, seq_len, d_model):\n",
        "        super(LearnablePositionalEncoding, self).__init__()\n",
        "        self.position_embeddings = nn.Embedding(seq_len, d_model)\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass for positional encoding.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Input tensor of shape (batch_size, seq_len, d_model).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Positionally encoded tensor.\n",
        "        \"\"\"\n",
        "        positions = torch.arange(0, self.seq_len, device=x.device).unsqueeze(0)\n",
        "        pos_embed = self.position_embeddings(positions)\n",
        "        x = x + pos_embed\n",
        "        return x\n",
        "\n",
        "# PortfolioTransformer using PyTorch's nn.Transformer\n",
        "class PortfolioTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Portfolio Transformer model for asset allocation.\n",
        "\n",
        "    Args:\n",
        "        num_assets (int): Number of assets in the portfolio.\n",
        "        d_model (int): Dimension of the model.\n",
        "        nhead (int): Number of attention heads.\n",
        "        num_layers (int): Number of transformer encoder layers.\n",
        "        seq_len (int): Length of the input sequences.\n",
        "        dropout (float): Dropout probability.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self, num_assets, d_model, nhead, num_layers, seq_len, dropout=0.1\n",
        "    ):\n",
        "        super(PortfolioTransformer, self).__init__()\n",
        "        self.num_assets = num_assets\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Input projection layer to map asset returns to model dimension\n",
        "        self.input_proj = nn.Linear(num_assets, d_model)\n",
        "\n",
        "        # Learnable positional encoding\n",
        "        self.pos_encoder = LearnablePositionalEncoding(seq_len, d_model)\n",
        "\n",
        "        # Transformer encoder layers\n",
        "        encoder_layers = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model, nhead=nhead, dropout=dropout, batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            encoder_layers, num_layers=num_layers\n",
        "        )\n",
        "\n",
        "        # Output layer to map back to asset space\n",
        "        self.fc_out = nn.Linear(d_model, num_assets)\n",
        "\n",
        "        # Initialize weights\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        \"\"\"\n",
        "        Initializes weights for better convergence.\n",
        "        \"\"\"\n",
        "        initrange = 0.1\n",
        "        self.input_proj.weight.data.uniform_(-initrange, initrange)\n",
        "        self.input_proj.bias.data.zero_()\n",
        "        self.fc_out.bias.data.zero_()\n",
        "        self.fc_out.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the model.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Input tensor of shape (batch_size, seq_len, num_assets).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Portfolio weights of shape (batch_size, seq_len, num_assets).\n",
        "        \"\"\"\n",
        "        # Project input to model dimension and scale\n",
        "        x = self.input_proj(x) * np.sqrt(self.d_model)\n",
        "        # Add positional encoding\n",
        "        x = self.pos_encoder(x)\n",
        "        # Pass through transformer encoder\n",
        "        x = self.transformer_encoder(x)\n",
        "        # Map back to asset space\n",
        "        s_i_t = self.fc_out(x)  # Shape: (batch_size, seq_len, num_assets)\n",
        "        # Apply activation and normalize weights\n",
        "        weights = torch.tanh(s_i_t)\n",
        "        weights = weights / torch.sum(torch.abs(weights), dim=-1, keepdim=True)\n",
        "        return weights\n",
        "\n",
        "def sharpe_loss(\n",
        "    portfolio_returns, portfolio_weights, prev_weights, transaction_cost=0.0002\n",
        "):\n",
        "    \"\"\"\n",
        "    Computes the negative Sharpe ratio as the loss function, including transaction costs.\n",
        "\n",
        "    Args:\n",
        "        portfolio_returns (Tensor): Portfolio returns of shape (batch_size,).\n",
        "        portfolio_weights (Tensor): Current portfolio weights of shape (batch_size, num_assets).\n",
        "        prev_weights (Tensor): Previous portfolio weights of shape (batch_size, num_assets).\n",
        "        transaction_cost (float): Transaction cost rate per unit weight change.\n",
        "\n",
        "    Returns:\n",
        "        Tensor: Loss value (negative Sharpe ratio).\n",
        "    \"\"\"\n",
        "    # Compute transaction costs based on weight changes\n",
        "    tc = transaction_cost * torch.sum(torch.abs(portfolio_weights - prev_weights), dim=1)\n",
        "    # Net returns after subtracting transaction costs\n",
        "    net_returns = portfolio_returns - tc\n",
        "    # Compute mean and standard deviation of returns\n",
        "    mean_return = torch.mean(net_returns)\n",
        "    std_return = torch.std(net_returns)\n",
        "    # Compute Sharpe ratio\n",
        "    sharpe_ratio = mean_return / (std_return + 1e-6)\n",
        "    # Return negative Sharpe ratio for minimization\n",
        "    return -sharpe_ratio\n",
        "\n",
        "def compute_expected_time_to_target(portfolio_returns, target_sharpe, window=20):\n",
        "    \"\"\"\n",
        "    Estimates the expected time to reach the target Sharpe ratio.\n",
        "\n",
        "    Args:\n",
        "        portfolio_returns (list): List of past portfolio returns.\n",
        "        target_sharpe (float): Target Sharpe ratio.\n",
        "        window (int): Window size for rolling calculation.\n",
        "\n",
        "    Returns:\n",
        "        Tensor: Estimated time to reach the target Sharpe ratio.\n",
        "    \"\"\"\n",
        "    # If insufficient data, return zero\n",
        "    if len(portfolio_returns) < window:\n",
        "        return torch.tensor(0.0)\n",
        "    # Compute rolling mean and std\n",
        "    recent_returns = torch.tensor(portfolio_returns[-window:])\n",
        "    rolling_returns = torch.mean(recent_returns)\n",
        "    rolling_std = torch.std(recent_returns)\n",
        "    rolling_sharpe = rolling_returns / (rolling_std + 1e-6)\n",
        "    # Estimate time to target\n",
        "    if rolling_sharpe >= target_sharpe:\n",
        "        time_to_target = torch.tensor(0.0)\n",
        "    else:\n",
        "        # Simplified estimation\n",
        "        time_to_target = (target_sharpe - rolling_sharpe) * window\n",
        "    return time_to_target\n",
        "\n",
        "def adjust_position_scaling(portfolio_weights, time_to_target, scaling_factor=0.1):\n",
        "    \"\"\"\n",
        "    Adjusts position scaling based on the estimated time to target Sharpe ratio.\n",
        "\n",
        "    Args:\n",
        "        portfolio_weights (Tensor): Current portfolio weights.\n",
        "        time_to_target (Tensor): Estimated time to reach target Sharpe ratio.\n",
        "        scaling_factor (float): Factor to control scaling sensitivity.\n",
        "\n",
        "    Returns:\n",
        "        Tensor: Adjusted portfolio weights.\n",
        "    \"\"\"\n",
        "    # Compute scaling coefficient\n",
        "    scaling = 1.0 - scaling_factor * torch.sigmoid(time_to_target)\n",
        "    # Adjust portfolio weights\n",
        "    adjusted_weights = portfolio_weights * scaling\n",
        "    return adjusted_weights\n",
        "\n",
        "def train(model, optimizer, data_loader, num_epochs, target_sharpe=1.0):\n",
        "    \"\"\"\n",
        "    Trains the PortfolioTransformer model.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The PortfolioTransformer model.\n",
        "        optimizer (torch.optim.Optimizer): The optimizer.\n",
        "        data_loader (DataLoader): DataLoader for training data.\n",
        "        num_epochs (int): Number of training epochs.\n",
        "        target_sharpe (float): Target Sharpe ratio for scaling positions.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    portfolio_returns_history = []\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for batch_X, batch_y in data_loader:\n",
        "            optimizer.zero_grad()\n",
        "            # Forward pass to get portfolio weights\n",
        "            portfolio_weights = model(batch_X)\n",
        "            # Use the last time step's weights\n",
        "            portfolio_weights = portfolio_weights[:, -1, :]\n",
        "            # Initialize prev_weights with zeros matching portfolio_weights\n",
        "            prev_weights = torch.zeros_like(portfolio_weights)\n",
        "            # Compute portfolio returns\n",
        "            portfolio_returns = torch.sum(portfolio_weights * batch_y, dim=1)\n",
        "            # Adjust positions based on expected time to target\n",
        "            time_to_target = compute_expected_time_to_target(\n",
        "                portfolio_returns_history, target_sharpe\n",
        "            )\n",
        "            portfolio_weights = adjust_position_scaling(\n",
        "                portfolio_weights, time_to_target\n",
        "            )\n",
        "            # Update returns history\n",
        "            portfolio_returns_history.extend(portfolio_returns.tolist())\n",
        "            # Compute loss with transaction costs\n",
        "            loss = sharpe_loss(portfolio_returns, portfolio_weights, prev_weights)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            # Update prev_weights for the next batch\n",
        "            prev_weights = portfolio_weights.detach()\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(data_loader):.4f}\")\n",
        "\n",
        "def backtest(model, data, train_size, test_size, sequence_length, target_sharpe=1.0):\n",
        "    \"\"\"\n",
        "    Performs backtesting by training on a subset of data and testing on future data.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The PortfolioTransformer model.\n",
        "        data (Tensor): Asset returns data.\n",
        "        train_size (int): Number of samples for training.\n",
        "        test_size (int): Number of samples for testing.\n",
        "        sequence_length (int): Length of input sequences.\n",
        "        target_sharpe (float): Target Sharpe ratio for scaling positions.\n",
        "    \"\"\"\n",
        "    num_samples = len(data) - sequence_length\n",
        "    train_indices = range(train_size)\n",
        "    test_indices = range(train_size, train_size + test_size)\n",
        "\n",
        "    # Prepare training data\n",
        "    X_train = []\n",
        "    y_train = []\n",
        "    for i in train_indices:\n",
        "        X_train.append(data[i:i+sequence_length])\n",
        "        y_train.append(data[i+sequence_length])\n",
        "    X_train = torch.stack(X_train)\n",
        "    y_train = torch.stack(y_train)\n",
        "    train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    # Prepare test data\n",
        "    X_test = []\n",
        "    y_test = []\n",
        "    for i in test_indices:\n",
        "        X_test.append(data[i:i+sequence_length])\n",
        "        y_test.append(data[i+sequence_length])\n",
        "    X_test = torch.stack(X_test)\n",
        "    y_test = torch.stack(y_test)\n",
        "\n",
        "    # Train the model\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "    train(model, optimizer, train_loader, num_epochs=10, target_sharpe=target_sharpe)\n",
        "\n",
        "    # Backtest on test data\n",
        "    model.eval()\n",
        "    prev_weights = torch.zeros(y_test.size(1))\n",
        "    portfolio_returns_history = []\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(X_test)):\n",
        "            batch_X = X_test[i:i+1]  # Shape: (1, seq_len, num_assets)\n",
        "            batch_y = y_test[i:i+1]  # Shape: (1, num_assets)\n",
        "            # Get portfolio weights\n",
        "            portfolio_weights = model(batch_X)\n",
        "            portfolio_weights = portfolio_weights[:, -1, :]\n",
        "            # Compute portfolio returns\n",
        "            portfolio_returns = torch.sum(portfolio_weights * batch_y, dim=1)\n",
        "            # Adjust positions based on expected time to target\n",
        "            time_to_target = compute_expected_time_to_target(\n",
        "                portfolio_returns_history, target_sharpe=target_sharpe\n",
        "            )\n",
        "            portfolio_weights = adjust_position_scaling(\n",
        "                portfolio_weights, time_to_target\n",
        "            )\n",
        "            # Update returns history\n",
        "            portfolio_returns_history.extend(portfolio_returns.tolist())\n",
        "            # Update prev_weights\n",
        "            prev_weights = portfolio_weights.detach()\n",
        "        # Compute cumulative returns and Sharpe ratio\n",
        "        portfolio_returns = np.array(portfolio_returns_history)\n",
        "        cumulative_returns = np.cumprod(1 + portfolio_returns) - 1\n",
        "        mean_return = np.mean(portfolio_returns)\n",
        "        std_return = np.std(portfolio_returns)\n",
        "        sharpe_ratio = mean_return / (std_return + 1e-6) * np.sqrt(252)\n",
        "        print(f\"Cumulative Return: {cumulative_returns[-1]:.4f}\")\n",
        "        print(f\"Annualized Sharpe Ratio: {sharpe_ratio:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qmk8tSvSswV7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Generate random returns data: 100 securities over 1000 time steps\n",
        "num_securities = 100\n",
        "num_time_steps = 1000\n",
        "\n",
        "# Simulate random returns (e.g., normally distributed with small mean and std)\n",
        "returns_data = np.random.normal(loc=0.001, scale=0.01, size=(num_time_steps, num_securities))\n",
        "returns = torch.tensor(returns_data, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRQEHmx-s2Lt",
        "outputId": "968a8d84-fb43-4a7a-cec6-66b7d2efa499"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Loss: -0.4861\n",
            "Epoch 2/10, Loss: -0.7983\n",
            "Epoch 3/10, Loss: -0.8931\n",
            "Epoch 4/10, Loss: -0.9067\n",
            "Epoch 5/10, Loss: -0.9006\n",
            "Epoch 6/10, Loss: -0.9224\n",
            "Epoch 7/10, Loss: -0.9115\n",
            "Epoch 8/10, Loss: -0.9196\n",
            "Epoch 9/10, Loss: -0.9191\n",
            "Epoch 10/10, Loss: -0.9201\n",
            "Cumulative Return: 0.2299\n",
            "Annualized Sharpe Ratio: 12.6064\n"
          ]
        }
      ],
      "source": [
        "# Define parameters\n",
        "sequence_length = 20\n",
        "train_size = 700\n",
        "test_size = 200\n",
        "num_assets = num_securities\n",
        "\n",
        "# Initialize the model\n",
        "model = PortfolioTransformer(\n",
        "    num_assets=num_assets,\n",
        "    d_model=64,\n",
        "    nhead=8,\n",
        "    num_layers=4,\n",
        "    seq_len=sequence_length,\n",
        "    dropout=0.1\n",
        ")\n",
        "\n",
        "# Run backtest\n",
        "backtest(model, returns, train_size, test_size, sequence_length, target_sharpe=1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7bsziDwk4_nQ",
        "outputId": "f6e16500-03a3-4970-d2b5-f8cd5b9382ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Logistic Regression Model\n",
            "Epoch [1/5], Loss: 0.009969\n",
            "Epoch [2/5], Loss: -0.013706\n",
            "Epoch [3/5], Loss: -0.030145\n",
            "Epoch [4/5], Loss: -0.055203\n",
            "Epoch [5/5], Loss: -0.082532\n",
            "Cumulative Return: -0.071690\n",
            "Annualized Sharpe Ratio: -0.277367\n",
            "\n",
            "Training LSTM Model\n",
            "Epoch [1/5], Loss: 0.015314\n",
            "Epoch [2/5], Loss: -0.019160\n",
            "Epoch [3/5], Loss: -0.023031\n",
            "Epoch [4/5], Loss: -0.035522\n",
            "Epoch [5/5], Loss: -0.023104\n",
            "Cumulative Return: -0.001735\n",
            "Annualized Sharpe Ratio: 0.024882\n",
            "\n",
            "Training CNN Model\n",
            "Epoch [1/5], Loss: 0.008797\n",
            "Epoch [2/5], Loss: -0.006740\n",
            "Epoch [3/5], Loss: -0.012274\n",
            "Epoch [4/5], Loss: -0.018750\n",
            "Epoch [5/5], Loss: -0.021238\n",
            "Cumulative Return: 0.168103\n",
            "Annualized Sharpe Ratio: 0.680245\n",
            "\n",
            "Training Transformer Model\n",
            "Epoch [1/5], Loss: 0.055456\n",
            "Epoch [2/5], Loss: 0.000523\n",
            "Epoch [3/5], Loss: -0.012663\n",
            "Epoch [4/5], Loss: -0.023768\n",
            "Epoch [5/5], Loss: -0.035341\n",
            "Cumulative Return: -0.019282\n",
            "Annualized Sharpe Ratio: -0.069464\n",
            "\n",
            "Training Portfolio Transformer Model\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (20) must match the size of tensor b (64) at non-singleton dimension 1",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-0a32d9d195ad>\u001b[0m in \u001b[0;36m<cell line: 342>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPortfolioTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_assets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_assets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnhead\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msharpe_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0mbacktest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-0a32d9d195ad>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, criterion, optimizer, num_epochs, device)\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0mportfolio_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0;31m# Compute portfolio returns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m             \u001b[0mportfolio_returns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mportfolio_weights\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m             \u001b[0;31m# Compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mportfolio_returns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mportfolio_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mportfolio_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (20) must match the size of tensor b (64) at non-singleton dimension 1"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import time\n",
        "\n",
        "# Parameters\n",
        "num_timesteps = 5000  # Total number of time steps\n",
        "num_assets = 10       # Number of assets in the portfolio\n",
        "seq_len = 20          # Length of input sequences\n",
        "d_model = 64          # Model dimension for Transformer\n",
        "batch_size = 64       # Batch size for training\n",
        "num_epochs = 5        # Number of training epochs\n",
        "\n",
        "# Generate synthetic returns data\n",
        "np.random.seed(42)\n",
        "returns_data = np.random.normal(0, 0.01, size=(num_timesteps, num_assets)).astype(np.float32)\n",
        "returns_data = torch.tensor(returns_data)\n",
        "\n",
        "# Dataset and DataLoader for training\n",
        "class ReturnsDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for asset returns.\n",
        "\n",
        "    Args:\n",
        "        data (Tensor): Asset returns data of shape (num_timesteps, num_assets).\n",
        "        seq_len (int): Length of input sequences.\n",
        "    \"\"\"\n",
        "    def __init__(self, data, seq_len):\n",
        "        self.data = data\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.seq_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        X = self.data[idx:idx + self.seq_len]\n",
        "        y = self.data[idx + self.seq_len]\n",
        "        return X, y\n",
        "\n",
        "# Split data into training and testing sets\n",
        "train_ratio = 0.8\n",
        "train_size = int(len(returns_data) * train_ratio)\n",
        "train_data = returns_data[:train_size]\n",
        "test_data = returns_data[train_size:]\n",
        "\n",
        "train_dataset = ReturnsDataset(train_data, seq_len)\n",
        "test_dataset = ReturnsDataset(test_data, seq_len)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Loss function: Negative Sharpe Ratio\n",
        "def sharpe_loss(portfolio_returns, portfolio_weights, prev_weights, transaction_cost=0.0002):\n",
        "    \"\"\"\n",
        "    Computes the negative Sharpe ratio as the loss function, including transaction costs.\n",
        "\n",
        "    Args:\n",
        "        portfolio_returns (Tensor): Portfolio returns of shape (batch_size,).\n",
        "        portfolio_weights (Tensor): Current portfolio weights of shape (batch_size, num_assets).\n",
        "        prev_weights (Tensor): Previous portfolio weights of shape (batch_size, num_assets).\n",
        "        transaction_cost (float): Transaction cost rate per unit weight change.\n",
        "\n",
        "    Returns:\n",
        "        Tensor: Loss value (negative Sharpe ratio).\n",
        "    \"\"\"\n",
        "    # Compute transaction costs based on weight changes\n",
        "    tc = transaction_cost * torch.sum(torch.abs(portfolio_weights - prev_weights), dim=1)\n",
        "    # Net returns after subtracting transaction costs\n",
        "    net_returns = portfolio_returns - tc\n",
        "    # Compute mean and standard deviation of returns\n",
        "    mean_return = torch.mean(net_returns)\n",
        "    std_return = torch.std(net_returns)\n",
        "    # Compute Sharpe ratio\n",
        "    sharpe_ratio = mean_return / (std_return + 1e-6)\n",
        "    # Return negative Sharpe ratio for minimization\n",
        "    return -sharpe_ratio\n",
        "\n",
        "# Base class for models\n",
        "class BaseModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Base class for asset allocation models.\n",
        "\n",
        "    Args:\n",
        "        num_assets (int): Number of assets in the portfolio.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_assets):\n",
        "        super(BaseModel, self).__init__()\n",
        "        self.num_assets = num_assets\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Input tensor of shape (batch_size, seq_len, num_assets).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Portfolio weights of shape (batch_size, num_assets).\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "# Logistic Regression Model\n",
        "class LogisticRegressionModel(BaseModel):\n",
        "    \"\"\"\n",
        "    Logistic Regression Model for asset allocation.\n",
        "\n",
        "    Args:\n",
        "        input_size (int): Size of input features (seq_len * num_assets).\n",
        "        num_assets (int): Number of assets.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, num_assets):\n",
        "        super(LogisticRegressionModel, self).__init__(num_assets)\n",
        "        self.linear = nn.Linear(input_size, num_assets)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        out = self.linear(x)\n",
        "        weights = torch.tanh(out)\n",
        "        weights = weights / torch.sum(torch.abs(weights), dim=1, keepdim=True)\n",
        "        return weights\n",
        "\n",
        "# LSTM Model\n",
        "class LSTMModel(BaseModel):\n",
        "    \"\"\"\n",
        "    LSTM Model for asset allocation.\n",
        "\n",
        "    Args:\n",
        "        num_assets (int): Number of assets.\n",
        "        hidden_size (int): Size of hidden state.\n",
        "        num_layers (int): Number of LSTM layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_assets, hidden_size=64, num_layers=2):\n",
        "        super(LSTMModel, self).__init__(num_assets)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size=num_assets, hidden_size=hidden_size,\n",
        "                            num_layers=num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_assets)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = out[:, -1, :]  # Last time step\n",
        "        out = self.fc(out)\n",
        "        weights = torch.tanh(out)\n",
        "        weights = weights / torch.sum(torch.abs(weights), dim=1, keepdim=True)\n",
        "        return weights\n",
        "\n",
        "# CNN Model\n",
        "class CNNModel(BaseModel):\n",
        "    \"\"\"\n",
        "    CNN Model for asset allocation.\n",
        "\n",
        "    Args:\n",
        "        num_assets (int): Number of assets.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_assets):\n",
        "        super(CNNModel, self).__init__(num_assets)\n",
        "        self.conv1 = nn.Conv1d(in_channels=num_assets, out_channels=32, kernel_size=3)\n",
        "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3)\n",
        "        self.fc = nn.Linear(64, num_assets)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)  # (batch_size, num_assets, seq_len)\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = torch.mean(x, dim=2)  # Global average pooling\n",
        "        x = self.fc(x)\n",
        "        weights = torch.tanh(x)\n",
        "        weights = weights / torch.sum(torch.abs(weights), dim=1, keepdim=True)\n",
        "        return weights\n",
        "\n",
        "# Transformer Model\n",
        "class TransformerModel(BaseModel):\n",
        "    \"\"\"\n",
        "    Transformer Model for asset allocation.\n",
        "\n",
        "    Args:\n",
        "        num_assets (int): Number of assets.\n",
        "        d_model (int): Dimension of the model.\n",
        "        nhead (int): Number of attention heads.\n",
        "        num_layers (int): Number of transformer encoder layers.\n",
        "        seq_len (int): Length of the input sequences.\n",
        "        dropout (float): Dropout probability.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_assets, d_model=64, nhead=4, num_layers=2, seq_len=20, dropout=0.1):\n",
        "        super(TransformerModel, self).__init__(num_assets)\n",
        "        self.d_model = d_model\n",
        "        self.input_proj = nn.Linear(num_assets, d_model)\n",
        "        self.pos_encoder = LearnablePositionalEncoding(seq_len, d_model)\n",
        "        encoder_layers = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dropout=dropout, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
        "        self.fc_out = nn.Linear(d_model, num_assets)\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.input_proj.weight.data.uniform_(-initrange, initrange)\n",
        "        self.input_proj.bias.data.zero_()\n",
        "        self.fc_out.bias.data.zero_()\n",
        "        self.fc_out.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_proj(x) * np.sqrt(self.d_model)\n",
        "        x = self.pos_encoder(x)\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = x[:, -1, :]  # Last time step\n",
        "        x = self.fc_out(x)\n",
        "        weights = torch.tanh(x)\n",
        "        weights = weights / torch.sum(torch.abs(weights), dim=1, keepdim=True)\n",
        "        return weights\n",
        "\n",
        "# Learnable Positional Encoding\n",
        "class LearnablePositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements learnable positional encoding for sequences.\n",
        "\n",
        "    Args:\n",
        "        seq_len (int): The maximum length of the input sequences.\n",
        "        d_model (int): The dimension of the model.\n",
        "    \"\"\"\n",
        "    def __init__(self, seq_len, d_model):\n",
        "        super(LearnablePositionalEncoding, self).__init__()\n",
        "        self.position_embeddings = nn.Embedding(seq_len, d_model)\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def forward(self, x):\n",
        "        positions = torch.arange(0, self.seq_len, device=x.device).unsqueeze(0)\n",
        "        pos_embed = self.position_embeddings(positions)\n",
        "        x = x + pos_embed\n",
        "        return x\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_loader, criterion, optimizer, num_epochs, device):\n",
        "    \"\"\"\n",
        "    Training loop for the model.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The model to train.\n",
        "        train_loader (DataLoader): DataLoader for training data.\n",
        "        criterion (function): Loss function.\n",
        "        optimizer (torch.optim.Optimizer): Optimizer.\n",
        "        num_epochs (int): Number of epochs.\n",
        "        device (torch.device): Computation device.\n",
        "    \"\"\"\n",
        "    model = model.to(device)\n",
        "    prev_weights = torch.zeros((batch_size, model.num_assets)).to(device)\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            # Forward pass\n",
        "            portfolio_weights = model(X_batch)\n",
        "            # Compute portfolio returns\n",
        "            portfolio_returns = torch.sum(portfolio_weights * y_batch, dim=1)\n",
        "            # Compute loss\n",
        "            loss = criterion(portfolio_returns, portfolio_weights, prev_weights[:portfolio_weights.size(0)])\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            # Update prev_weights\n",
        "            prev_weights[:portfolio_weights.size(0)] = portfolio_weights.detach()\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.6f}')\n",
        "\n",
        "# Backtesting function\n",
        "def backtest_model(model, test_loader, device):\n",
        "    \"\"\"\n",
        "    Backtesting the model on test data.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The trained model.\n",
        "        test_loader (DataLoader): DataLoader for test data.\n",
        "        device (torch.device): Computation device.\n",
        "    \"\"\"\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    portfolio_returns_history = []\n",
        "    prev_weights = torch.zeros((1, model.num_assets)).to(device)\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "            portfolio_weights = model(X_batch)\n",
        "            portfolio_returns = torch.sum(portfolio_weights * y_batch, dim=1)\n",
        "            portfolio_returns_history.extend(portfolio_returns.cpu().numpy())\n",
        "            # Update prev_weights\n",
        "            prev_weights = portfolio_weights.detach()\n",
        "    portfolio_returns = np.array(portfolio_returns_history)\n",
        "    cumulative_returns = np.cumprod(1 + portfolio_returns) - 1\n",
        "    mean_return = np.mean(portfolio_returns)\n",
        "    std_return = np.std(portfolio_returns)\n",
        "    sharpe_ratio = mean_return / (std_return + 1e-6) * np.sqrt(252)\n",
        "    print(f\"Cumulative Return: {cumulative_returns[-1]:.6f}\")\n",
        "    print(f\"Annualized Sharpe Ratio: {sharpe_ratio:.6f}\")\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Example usage with Logistic Regression Model\n",
        "print(\"\\nTraining Logistic Regression Model\")\n",
        "model = LogisticRegressionModel(input_size=seq_len * num_assets, num_assets=num_assets)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "train_model(model, train_loader, sharpe_loss, optimizer, num_epochs, device)\n",
        "backtest_model(model, test_loader, device)\n",
        "\n",
        "# Example usage with LSTM Model\n",
        "print(\"\\nTraining LSTM Model\")\n",
        "model = LSTMModel(num_assets=num_assets)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "train_model(model, train_loader, sharpe_loss, optimizer, num_epochs, device)\n",
        "backtest_model(model, test_loader, device)\n",
        "\n",
        "# Example usage with CNN Model\n",
        "print(\"\\nTraining CNN Model\")\n",
        "model = CNNModel(num_assets=num_assets)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "train_model(model, train_loader, sharpe_loss, optimizer, num_epochs, device)\n",
        "backtest_model(model, test_loader, device)\n",
        "\n",
        "# Example usage with Transformer Model\n",
        "print(\"\\nTraining Transformer Model\")\n",
        "model = TransformerModel(num_assets=num_assets, d_model=d_model, seq_len=seq_len)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "train_model(model, train_loader, sharpe_loss, optimizer, num_epochs, device)\n",
        "backtest_model(model, test_loader, device)\n",
        "\n",
        "# New Model provided by the user\n",
        "print(\"\\nTraining Portfolio Transformer Model\")\n",
        "model = PortfolioTransformer(num_assets=num_assets, d_model=d_model, nhead=4, num_layers=2, seq_len=seq_len)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "train_model(model, train_loader, sharpe_loss, optimizer, num_epochs, device)\n",
        "backtest_model(model, test_loader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsrS2ceu6OaZ",
        "outputId": "5945a4e9-1c55-4038-effd-a49611ec6812"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Logistic Regression Model\n",
            "Epoch [1/5], Loss: 0.032195\n",
            "Epoch [2/5], Loss: -0.005823\n",
            "Epoch [3/5], Loss: -0.027968\n",
            "Epoch [4/5], Loss: -0.059304\n",
            "Epoch [5/5], Loss: -0.124792\n",
            "Cumulative Return: 0.098651\n",
            "Annualized Sharpe Ratio: 0.424042\n",
            "\n",
            "Training LSTM Model\n",
            "Epoch [1/5], Loss: 0.000014\n",
            "Epoch [2/5], Loss: -0.012548\n",
            "Epoch [3/5], Loss: -0.020844\n",
            "Epoch [4/5], Loss: -0.017139\n",
            "Epoch [5/5], Loss: -0.019319\n",
            "Cumulative Return: -0.051313\n",
            "Annualized Sharpe Ratio: -0.166005\n",
            "\n",
            "Training CNN Model\n",
            "Epoch [1/5], Loss: 0.009937\n",
            "Epoch [2/5], Loss: 0.006323\n",
            "Epoch [3/5], Loss: 0.004441\n",
            "Epoch [4/5], Loss: -0.014695\n",
            "Epoch [5/5], Loss: -0.013243\n",
            "Cumulative Return: 0.122629\n",
            "Annualized Sharpe Ratio: 0.562786\n",
            "\n",
            "Training Transformer Model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Loss: -0.001458\n",
            "Epoch [2/5], Loss: -0.026020\n",
            "Epoch [3/5], Loss: -0.030113\n",
            "Epoch [4/5], Loss: -0.025826\n",
            "Epoch [5/5], Loss: -0.024897\n",
            "Cumulative Return: 0.040647\n",
            "Annualized Sharpe Ratio: 0.229768\n",
            "\n",
            "Training Portfolio Transformer Model\n",
            "Epoch [1/5], Loss: 0.006854\n",
            "Epoch [2/5], Loss: -0.027711\n",
            "Epoch [3/5], Loss: -0.010189\n",
            "Epoch [4/5], Loss: -0.016794\n",
            "Epoch [5/5], Loss: -0.026215\n",
            "Cumulative Return: -0.077065\n",
            "Annualized Sharpe Ratio: -0.370159\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import time\n",
        "\n",
        "# Parameters\n",
        "num_timesteps = 5000  # Total number of time steps\n",
        "num_assets = 10       # Number of assets in the portfolio\n",
        "seq_len = 20          # Length of input sequences\n",
        "d_model = 64          # Model dimension for Transformer\n",
        "batch_size = 64       # Batch size for training\n",
        "num_epochs = 5        # Number of training epochs\n",
        "\n",
        "# Generate synthetic returns data\n",
        "np.random.seed(42)\n",
        "returns_data = np.random.normal(0, 0.01, size=(num_timesteps, num_assets)).astype(np.float32)\n",
        "returns_data = torch.tensor(returns_data)\n",
        "\n",
        "# Dataset and DataLoader for training\n",
        "class ReturnsDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for asset returns.\n",
        "\n",
        "    Args:\n",
        "        data (Tensor): Asset returns data of shape (num_timesteps, num_assets).\n",
        "        seq_len (int): Length of input sequences.\n",
        "    \"\"\"\n",
        "    def __init__(self, data, seq_len):\n",
        "        self.data = data\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.seq_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        X = self.data[idx:idx + self.seq_len]\n",
        "        y = self.data[idx + self.seq_len]\n",
        "        return X, y\n",
        "\n",
        "# Split data into training and testing sets\n",
        "train_ratio = 0.8\n",
        "train_size = int(len(returns_data) * train_ratio)\n",
        "train_data = returns_data[:train_size]\n",
        "test_data = returns_data[train_size:]\n",
        "\n",
        "train_dataset = ReturnsDataset(train_data, seq_len)\n",
        "test_dataset = ReturnsDataset(test_data, seq_len)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)  # Use batch_size=1 for backtesting\n",
        "\n",
        "# Loss function: Negative Sharpe Ratio\n",
        "def sharpe_loss(portfolio_returns, portfolio_weights, prev_weights, transaction_cost=0.0002):\n",
        "    \"\"\"\n",
        "    Computes the negative Sharpe ratio as the loss function, including transaction costs.\n",
        "\n",
        "    Args:\n",
        "        portfolio_returns (Tensor): Portfolio returns of shape (batch_size,).\n",
        "        portfolio_weights (Tensor): Current portfolio weights of shape (batch_size, num_assets).\n",
        "        prev_weights (Tensor): Previous portfolio weights of shape (batch_size, num_assets).\n",
        "        transaction_cost (float): Transaction cost rate per unit weight change.\n",
        "\n",
        "    Returns:\n",
        "        Tensor: Loss value (negative Sharpe ratio).\n",
        "    \"\"\"\n",
        "    # Compute transaction costs based on weight changes\n",
        "    tc = transaction_cost * torch.sum(torch.abs(portfolio_weights - prev_weights), dim=1)\n",
        "    # Net returns after subtracting transaction costs\n",
        "    net_returns = portfolio_returns - tc\n",
        "    # Compute mean and standard deviation of returns\n",
        "    mean_return = torch.mean(net_returns)\n",
        "    std_return = torch.std(net_returns)\n",
        "    # Compute Sharpe ratio\n",
        "    sharpe_ratio = mean_return / (std_return + 1e-6)\n",
        "    # Return negative Sharpe ratio for minimization\n",
        "    return -sharpe_ratio\n",
        "\n",
        "# Base class for models\n",
        "class BaseModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Base class for asset allocation models.\n",
        "\n",
        "    Args:\n",
        "        num_assets (int): Number of assets in the portfolio.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_assets):\n",
        "        super(BaseModel, self).__init__()\n",
        "        self.num_assets = num_assets\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Input tensor of shape (batch_size, seq_len, num_assets).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Portfolio weights of shape (batch_size, num_assets).\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "# Logistic Regression Model\n",
        "class LogisticRegressionModel(BaseModel):\n",
        "    \"\"\"\n",
        "    Logistic Regression Model for asset allocation.\n",
        "\n",
        "    Args:\n",
        "        input_size (int): Size of input features (seq_len * num_assets).\n",
        "        num_assets (int): Number of assets.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, num_assets):\n",
        "        super(LogisticRegressionModel, self).__init__(num_assets)\n",
        "        self.linear = nn.Linear(input_size, num_assets)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        out = self.linear(x)\n",
        "        weights = torch.tanh(out)\n",
        "        weights = weights / torch.sum(torch.abs(weights), dim=1, keepdim=True)\n",
        "        return weights\n",
        "\n",
        "# LSTM Model\n",
        "class LSTMModel(BaseModel):\n",
        "    \"\"\"\n",
        "    LSTM Model for asset allocation.\n",
        "\n",
        "    Args:\n",
        "        num_assets (int): Number of assets.\n",
        "        hidden_size (int): Size of hidden state.\n",
        "        num_layers (int): Number of LSTM layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_assets, hidden_size=64, num_layers=2):\n",
        "        super(LSTMModel, self).__init__(num_assets)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size=num_assets, hidden_size=hidden_size,\n",
        "                            num_layers=num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_assets)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = out[:, -1, :]  # Last time step\n",
        "        out = self.fc(out)\n",
        "        weights = torch.tanh(out)\n",
        "        weights = weights / torch.sum(torch.abs(weights), dim=1, keepdim=True)\n",
        "        return weights\n",
        "\n",
        "# CNN Model\n",
        "class CNNModel(BaseModel):\n",
        "    \"\"\"\n",
        "    CNN Model for asset allocation.\n",
        "\n",
        "    Args:\n",
        "        num_assets (int): Number of assets.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_assets):\n",
        "        super(CNNModel, self).__init__(num_assets)\n",
        "        self.conv1 = nn.Conv1d(in_channels=num_assets, out_channels=32, kernel_size=3)\n",
        "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3)\n",
        "        self.fc = nn.Linear(64, num_assets)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)  # (batch_size, num_assets, seq_len)\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = torch.mean(x, dim=2)  # Global average pooling\n",
        "        x = self.fc(x)\n",
        "        weights = torch.tanh(x)\n",
        "        weights = weights / torch.sum(torch.abs(weights), dim=1, keepdim=True)\n",
        "        return weights\n",
        "\n",
        "# Transformer Model\n",
        "class TransformerModel(BaseModel):\n",
        "    \"\"\"\n",
        "    Transformer Model for asset allocation.\n",
        "\n",
        "    Args:\n",
        "        num_assets (int): Number of assets.\n",
        "        d_model (int): Dimension of the model.\n",
        "        nhead (int): Number of attention heads.\n",
        "        num_layers (int): Number of transformer encoder layers.\n",
        "        seq_len (int): Length of the input sequences.\n",
        "        dropout (float): Dropout probability.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_assets, d_model=64, nhead=4, num_layers=2, seq_len=20, dropout=0.1):\n",
        "        super(TransformerModel, self).__init__(num_assets)\n",
        "        self.d_model = d_model\n",
        "        self.input_proj = nn.Linear(num_assets, d_model)\n",
        "        self.pos_encoder = LearnablePositionalEncoding(seq_len, d_model)\n",
        "        encoder_layers = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dropout=dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
        "        self.fc_out = nn.Linear(d_model, num_assets)\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.input_proj.weight.data.uniform_(-initrange, initrange)\n",
        "        self.input_proj.bias.data.zero_()\n",
        "        self.fc_out.bias.data.zero_()\n",
        "        self.fc_out.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_proj(x) * np.sqrt(self.d_model)\n",
        "        x = self.pos_encoder(x)\n",
        "        x = x.permute(1, 0, 2)  # (seq_len, batch_size, d_model)\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = x.permute(1, 0, 2)  # (batch_size, seq_len, d_model)\n",
        "        x = x[:, -1, :]  # Last time step\n",
        "        x = self.fc_out(x)\n",
        "        weights = torch.tanh(x)\n",
        "        weights = weights / torch.sum(torch.abs(weights), dim=1, keepdim=True)\n",
        "        return weights\n",
        "\n",
        "# Learnable Positional Encoding\n",
        "class LearnablePositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements learnable positional encoding for sequences.\n",
        "\n",
        "    Args:\n",
        "        seq_len (int): The maximum length of the input sequences.\n",
        "        d_model (int): The dimension of the model.\n",
        "    \"\"\"\n",
        "    def __init__(self, seq_len, d_model):\n",
        "        super(LearnablePositionalEncoding, self).__init__()\n",
        "        self.position_embeddings = nn.Embedding(seq_len, d_model)\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def forward(self, x):\n",
        "        positions = torch.arange(0, x.size(1), device=x.device).unsqueeze(0).long()\n",
        "        pos_embed = self.position_embeddings(positions)\n",
        "        x = x + pos_embed\n",
        "        return x\n",
        "\n",
        "# Portfolio Transformer Model\n",
        "class PortfolioTransformer(BaseModel):\n",
        "    \"\"\"\n",
        "    Portfolio Transformer model for asset allocation.\n",
        "\n",
        "    Args:\n",
        "        num_assets (int): Number of assets in the portfolio.\n",
        "        d_model (int): Dimension of the model.\n",
        "        nhead (int): Number of attention heads.\n",
        "        num_layers (int): Number of transformer encoder layers.\n",
        "        seq_len (int): Length of the input sequences.\n",
        "        dropout (float): Dropout probability.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self, num_assets, d_model, nhead, num_layers, seq_len, dropout=0.1\n",
        "    ):\n",
        "        super(PortfolioTransformer, self).__init__(num_assets)\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Input projection layer to map asset returns to model dimension\n",
        "        self.input_proj = nn.Linear(num_assets, d_model)\n",
        "\n",
        "        # Learnable positional encoding\n",
        "        self.pos_encoder = LearnablePositionalEncoding(seq_len, d_model)\n",
        "\n",
        "        # Transformer encoder layers\n",
        "        encoder_layers = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model, nhead=nhead, dropout=dropout\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            encoder_layers, num_layers=num_layers\n",
        "        )\n",
        "\n",
        "        # Output layer to map back to asset space\n",
        "        self.fc_out = nn.Linear(d_model, num_assets)\n",
        "\n",
        "        # Initialize weights\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        \"\"\"\n",
        "        Initializes weights for better convergence.\n",
        "        \"\"\"\n",
        "        initrange = 0.1\n",
        "        self.input_proj.weight.data.uniform_(-initrange, initrange)\n",
        "        self.input_proj.bias.data.zero_()\n",
        "        self.fc_out.bias.data.zero_()\n",
        "        self.fc_out.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the model.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Input tensor of shape (batch_size, seq_len, num_assets).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Portfolio weights of shape (batch_size, num_assets).\n",
        "        \"\"\"\n",
        "        # Project input to model dimension and scale\n",
        "        x = self.input_proj(x) * np.sqrt(self.d_model)\n",
        "        # Add positional encoding\n",
        "        x = self.pos_encoder(x)\n",
        "        # Permute dimensions to match expected input of TransformerEncoder\n",
        "        x = x.permute(1, 0, 2)  # (seq_len, batch_size, d_model)\n",
        "        # Pass through transformer encoder\n",
        "        x = self.transformer_encoder(x)\n",
        "        # Permute back to (batch_size, seq_len, d_model)\n",
        "        x = x.permute(1, 0, 2)\n",
        "        # Get the last time step\n",
        "        x = x[:, -1, :]  # (batch_size, d_model)\n",
        "        # Map back to asset space\n",
        "        s_i_t = self.fc_out(x)  # Shape: (batch_size, num_assets)\n",
        "        # Apply activation and normalize weights\n",
        "        weights = torch.tanh(s_i_t)\n",
        "        weights = weights / torch.sum(torch.abs(weights), dim=1, keepdim=True)\n",
        "        return weights\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_loader, criterion, optimizer, num_epochs, device):\n",
        "    \"\"\"\n",
        "    Training loop for the model.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The model to train.\n",
        "        train_loader (DataLoader): DataLoader for training data.\n",
        "        criterion (function): Loss function.\n",
        "        optimizer (torch.optim.Optimizer): Optimizer.\n",
        "        num_epochs (int): Number of epochs.\n",
        "        device (torch.device): Computation device.\n",
        "    \"\"\"\n",
        "    model = model.to(device)\n",
        "    prev_weights = torch.zeros((batch_size, model.num_assets)).to(device)\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            # Forward pass\n",
        "            portfolio_weights = model(X_batch)\n",
        "            # Compute portfolio returns\n",
        "            portfolio_returns = torch.sum(portfolio_weights * y_batch, dim=1)\n",
        "            # Compute loss\n",
        "            loss = criterion(portfolio_returns, portfolio_weights, prev_weights[:portfolio_weights.size(0)])\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            # Update prev_weights\n",
        "            prev_weights[:portfolio_weights.size(0)] = portfolio_weights.detach()\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.6f}')\n",
        "\n",
        "# Backtesting function\n",
        "def backtest_model(model, test_loader, device):\n",
        "    \"\"\"\n",
        "    Backtesting the model on test data.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The trained model.\n",
        "        test_loader (DataLoader): DataLoader for test data.\n",
        "        device (torch.device): Computation device.\n",
        "    \"\"\"\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    portfolio_returns_history = []\n",
        "    prev_weights = torch.zeros((1, model.num_assets)).to(device)\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "            portfolio_weights = model(X_batch)\n",
        "            portfolio_returns = torch.sum(portfolio_weights * y_batch, dim=1)\n",
        "            portfolio_returns_history.extend(portfolio_returns.cpu().numpy())\n",
        "            # Update prev_weights\n",
        "            prev_weights = portfolio_weights.detach()\n",
        "    portfolio_returns = np.array(portfolio_returns_history)\n",
        "    cumulative_returns = np.cumprod(1 + portfolio_returns) - 1\n",
        "    mean_return = np.mean(portfolio_returns)\n",
        "    std_return = np.std(portfolio_returns)\n",
        "    sharpe_ratio = mean_return / (std_return + 1e-6) * np.sqrt(252)\n",
        "    print(f\"Cumulative Return: {cumulative_returns[-1]:.6f}\")\n",
        "    print(f\"Annualized Sharpe Ratio: {sharpe_ratio:.6f}\")\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Example usage with Logistic Regression Model\n",
        "print(\"\\nTraining Logistic Regression Model\")\n",
        "model = LogisticRegressionModel(input_size=seq_len * num_assets, num_assets=num_assets)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "train_model(model, train_loader, sharpe_loss, optimizer, num_epochs, device)\n",
        "backtest_model(model, test_loader, device)\n",
        "\n",
        "# Example usage with LSTM Model\n",
        "print(\"\\nTraining LSTM Model\")\n",
        "model = LSTMModel(num_assets=num_assets)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "train_model(model, train_loader, sharpe_loss, optimizer, num_epochs, device)\n",
        "backtest_model(model, test_loader, device)\n",
        "\n",
        "# Example usage with CNN Model\n",
        "print(\"\\nTraining CNN Model\")\n",
        "model = CNNModel(num_assets=num_assets)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "train_model(model, train_loader, sharpe_loss, optimizer, num_epochs, device)\n",
        "backtest_model(model, test_loader, device)\n",
        "\n",
        "# Example usage with Transformer Model\n",
        "print(\"\\nTraining Transformer Model\")\n",
        "model = TransformerModel(num_assets=num_assets, d_model=d_model, seq_len=seq_len)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "train_model(model, train_loader, sharpe_loss, optimizer, num_epochs, device)\n",
        "backtest_model(model, test_loader, device)\n",
        "\n",
        "# New Model provided by the user\n",
        "print(\"\\nTraining Portfolio Transformer Model\")\n",
        "model = PortfolioTransformer(num_assets=num_assets, d_model=d_model, nhead=4, num_layers=2, seq_len=seq_len)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "train_model(model, train_loader, sharpe_loss, optimizer, num_epochs, device)\n",
        "backtest_model(model, test_loader, device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch_timeseriues"
      ],
      "metadata": {
        "id": "fCNbWBmJv51t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tQzgBks3wD9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_forecasting pytorch_lightning torchcde"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dnu9Vg19wfqu",
        "outputId": "277fc4ee-fb94-4e02-85f0-e270dcea7324"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch_forecasting\n",
            "  Downloading pytorch_forecasting-1.1.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting pytorch_lightning\n",
            "  Downloading pytorch_lightning-2.4.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting torchcde\n",
            "  Downloading torchcde-0.2.5-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_forecasting) (1.26.4)\n",
            "Requirement already satisfied: torch!=2.0.1,<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_forecasting) (2.4.1+cu121)\n",
            "Collecting lightning<3.0.0,>=2.0.0 (from pytorch_forecasting)\n",
            "  Downloading lightning-2.4.0-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: scipy<2.0,>=1.8 in /usr/local/lib/python3.10/dist-packages (from pytorch_forecasting) (1.13.1)\n",
            "Requirement already satisfied: pandas<3.0.0,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_forecasting) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn<2.0,>=1.2 in /usr/local/lib/python3.10/dist-packages (from pytorch_forecasting) (1.5.2)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.66.5)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (2024.6.1)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch_lightning)\n",
            "  Downloading torchmetrics-1.5.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.12.2)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch_lightning)\n",
            "  Downloading lightning_utilities-0.11.8-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting torchdiffeq>=0.2.0 (from torchcde)\n",
            "  Downloading torchdiffeq-0.2.4-py3-none-any.whl.metadata (440 bytes)\n",
            "Collecting torchsde>=0.2.5 (from torchcde)\n",
            "  Downloading torchsde-0.2.6-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (3.10.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.10.0->pytorch_lightning) (75.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0.0,>=1.3.0->pytorch_forecasting) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0.0,>=1.3.0->pytorch_forecasting) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0.0,>=1.3.0->pytorch_forecasting) (2024.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0,>=1.2->pytorch_forecasting) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0,>=1.2->pytorch_forecasting) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch_forecasting) (3.16.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch_forecasting) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch_forecasting) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch_forecasting) (3.1.4)\n",
            "Collecting trampoline>=0.1.2 (from torchsde>=0.2.5->torchcde)\n",
            "  Downloading trampoline-0.1.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.15.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=1.3.0->pytorch_forecasting) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=2.0.1,<3.0.0,>=2.0.0->pytorch_forecasting) (3.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=2.0.1,<3.0.0,>=2.0.0->pytorch_forecasting) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (3.10)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (0.2.0)\n",
            "Downloading pytorch_forecasting-1.1.1-py3-none-any.whl (177 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_lightning-2.4.0-py3-none-any.whl (815 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchcde-0.2.5-py3-none-any.whl (28 kB)\n",
            "Downloading lightning-2.4.0-py3-none-any.whl (810 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m811.0/811.0 kB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.8-py3-none-any.whl (26 kB)\n",
            "Downloading torchdiffeq-0.2.4-py3-none-any.whl (32 kB)\n",
            "Downloading torchmetrics-1.5.0-py3-none-any.whl (890 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m890.5/890.5 kB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchsde-0.2.6-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trampoline-0.1.2-py3-none-any.whl (5.2 kB)\n",
            "Installing collected packages: trampoline, lightning-utilities, torchsde, torchmetrics, torchdiffeq, torchcde, pytorch_lightning, lightning, pytorch_forecasting\n",
            "Successfully installed lightning-2.4.0 lightning-utilities-0.11.8 pytorch_forecasting-1.1.1 pytorch_lightning-2.4.0 torchcde-0.2.5 torchdiffeq-0.2.4 torchmetrics-1.5.0 torchsde-0.2.6 trampoline-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from pytorch_forecasting.models.base_model import BaseModel\n",
        "from pytorch_forecasting.metrics import RMSE\n",
        "\n",
        "class AutoRegressiveModel(BaseModel):\n",
        "    \"\"\"\n",
        "    Simple Autoregressive model using a Linear layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size: int, output_size: int = 1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.linear = torch.nn.Linear(input_size, output_size)\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x[\"encoder_cont\"] shape: (batch_size, encoder_length, input_size)\n",
        "        encoder_output = x[\"encoder_cont\"][:, -1, :]  # Use last time step\n",
        "        prediction = self.linear(encoder_output)\n",
        "        return self.to_network_output(prediction=prediction)\n",
        "\n",
        "    @classmethod\n",
        "    def from_dataset(cls, dataset, **kwargs):\n",
        "        new_kwargs = {\n",
        "            \"input_size\": len(dataset.reals),\n",
        "            \"loss\": RMSE(),\n",
        "        }\n",
        "        new_kwargs.update(kwargs)\n",
        "        return super().from_dataset(dataset, **new_kwargs)"
      ],
      "metadata": {
        "id": "crYe-1JHwgrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from pytorch_forecasting.models.base_model import BaseModelWithCovariates\n",
        "from pytorch_forecasting.metrics import RMSE\n",
        "\n",
        "class LSTMForecaster(BaseModelWithCovariates):\n",
        "    \"\"\"\n",
        "    LSTM-based Forecaster.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size: int, hidden_size: int, num_layers: int, dropout: float = 0.0, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.lstm = torch.nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
        "        self.output_layer = torch.nn.Linear(hidden_size, 1)\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Concatenate continuous and categorical variables\n",
        "        encoder_input = x[\"encoder_cont\"]  # Shape: (batch_size, seq_len, input_size)\n",
        "        output, (hidden, cell) = self.lstm(encoder_input)\n",
        "        # Use the last hidden state\n",
        "        hidden_last = hidden[-1]\n",
        "        prediction = self.output_layer(hidden_last)\n",
        "        return self.to_network_output(prediction=prediction)\n",
        "\n",
        "    @classmethod\n",
        "    def from_dataset(cls, dataset, **kwargs):\n",
        "        new_kwargs = {\n",
        "            \"input_size\": len(dataset.reals),\n",
        "            \"loss\": RMSE(),\n",
        "        }\n",
        "        new_kwargs.update(kwargs)\n",
        "        return super().from_dataset(dataset, **new_kwargs)"
      ],
      "metadata": {
        "id": "Bwbo4x5JweTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from pytorch_forecasting.models.base_model import BaseModelWithCovariates\n",
        "from pytorch_forecasting.metrics import RMSE\n",
        "from torch.nn.utils import weight_norm\n",
        "\n",
        "class Chomp1d(torch.nn.Module):\n",
        "    def __init__(self, chomp_size):\n",
        "        super(Chomp1d, self).__init__()\n",
        "        self.chomp_size = chomp_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x[:, :, :-self.chomp_size].contiguous()\n",
        "\n",
        "class TemporalBlock(torch.nn.Module):\n",
        "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
        "        super(TemporalBlock, self).__init__()\n",
        "        self.conv1 = weight_norm(torch.nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
        "                                                 stride=stride, padding=padding, dilation=dilation))\n",
        "        self.chomp1 = Chomp1d(padding)\n",
        "        self.relu1 = torch.nn.ReLU()\n",
        "        self.dropout1 = torch.nn.Dropout(dropout)\n",
        "\n",
        "        self.net = torch.nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1)\n",
        "        self.downsample = torch.nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        self.conv1.weight.data.normal_(0, 0.01)\n",
        "        if self.downsample is not None:\n",
        "            self.downsample.weight.data.normal_(0, 0.01)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.net(x)\n",
        "        res = x if self.downsample is None else self.downsample(x)\n",
        "        return self.relu(out + res)\n",
        "\n",
        "class TemporalConvNet(torch.nn.Module):\n",
        "    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
        "        super(TemporalConvNet, self).__init__()\n",
        "        layers = []\n",
        "        num_levels = len(num_channels)\n",
        "        for i in range(num_levels):\n",
        "            dilation_size = 2 ** i\n",
        "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
        "            out_channels = num_channels[i]\n",
        "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
        "                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
        "        self.network = torch.nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "class TCNForecaster(BaseModelWithCovariates):\n",
        "    \"\"\"\n",
        "    Temporal Convolutional Network Forecaster.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size: int, num_channels: list, kernel_size: int = 2, dropout: float = 0.2, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size=kernel_size, dropout=dropout)\n",
        "        self.output_layer = torch.nn.Linear(num_channels[-1], 1)\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x[\"encoder_cont\"] shape: (batch_size, seq_len, input_size)\n",
        "        encoder_input = x[\"encoder_cont\"].permute(0, 2, 1)  # Reshape to (batch_size, input_size, seq_len)\n",
        "        tcn_output = self.tcn(encoder_input)\n",
        "        # Use the last time step\n",
        "        last_output = tcn_output[:, :, -1]\n",
        "        prediction = self.output_layer(last_output)\n",
        "        return self.to_network_output(prediction=prediction)\n",
        "\n",
        "    @classmethod\n",
        "    def from_dataset(cls, dataset, **kwargs):\n",
        "        new_kwargs = {\n",
        "            \"input_size\": len(dataset.reals),\n",
        "            \"loss\": RMSE(),\n",
        "        }\n",
        "        new_kwargs.update(kwargs)\n",
        "        return super().from_dataset(dataset, **new_kwargs)"
      ],
      "metadata": {
        "id": "ogy2N-DkwdUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from pytorch_forecasting.models.base_model import BaseModel\n",
        "from pytorch_forecasting.metrics import RMSE\n",
        "\n",
        "class ProphetLikeModel(BaseModel):\n",
        "    \"\"\"\n",
        "    Simplified Prophet-like model capturing trend and seasonality.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, seasonality: int, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.trend = torch.nn.Linear(1, 1)\n",
        "        self.seasonality = torch.nn.Linear(seasonality, 1)\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "    def forward(self, x):\n",
        "        time = x[\"encoder_cont\"][:, :, 0]  # Assuming the first real variable is time\n",
        "        time = time.unsqueeze(-1)\n",
        "        trend = self.trend(time)\n",
        "\n",
        "        seasonal_features = x[\"encoder_cont\"][:, :, 1:self.hparams.seasonality+1]\n",
        "        seasonality = self.seasonality(seasonal_features)\n",
        "\n",
        "        prediction = trend + seasonality\n",
        "        # Use the last time step\n",
        "        prediction = prediction[:, -1, :]\n",
        "        return self.to_network_output(prediction=prediction)\n",
        "\n",
        "    @classmethod\n",
        "    def from_dataset(cls, dataset, **kwargs):\n",
        "        new_kwargs = {\n",
        "            \"loss\": RMSE(),\n",
        "            \"seasonality\": 10,  # Example value, adjust as needed\n",
        "        }\n",
        "        new_kwargs.update(kwargs)\n",
        "        return super().from_dataset(dataset, **new_kwargs)"
      ],
      "metadata": {
        "id": "740wkaCawauX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AssetAllocationWrapper:\n",
        "    \"\"\"\n",
        "    Wrapper to train models for asset allocation based on returns.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_name, max_epochs=30):\n",
        "        \"\"\"\n",
        "        Initialize the wrapper.\n",
        "\n",
        "        Args:\n",
        "            model_name (str): Name of the model to use ('TFT', 'RNN', 'DeepAR', 'NBeats', 'NHiTS', 'Baseline', 'AutoRegressive', 'LSTM', 'TCN', 'ProphetLike').\n",
        "            max_epochs (int): Maximum number of training epochs.\n",
        "        \"\"\"\n",
        "        self.model_name = model_name\n",
        "        self.max_epochs = max_epochs\n",
        "        self.model = None\n",
        "\n",
        "    def prepare_data(self, df, time_idx, target, group_ids):\n",
        "        \"\"\"\n",
        "        Prepare dataset for asset allocation.\n",
        "\n",
        "        Args:\n",
        "            df (pd.DataFrame): Dataset containing returns.\n",
        "            time_idx (str): Name of the time index column.\n",
        "            target (str): Name of the target column (e.g., returns).\n",
        "            group_ids (list): List of group identifier column names (e.g., securities).\n",
        "        \"\"\"\n",
        "        # Ensure time_idx is integer and sort data\n",
        "        df[time_idx] = pd.to_datetime(df[time_idx])\n",
        "        df.sort_values(by=[group_ids[0], time_idx], inplace=True)\n",
        "        df[time_idx] = df[time_idx].dt.strftime('%Y%m%d').astype(int)\n",
        "\n",
        "        # Create TimeSeriesDataSet\n",
        "        max_encoder_length = 30\n",
        "        max_prediction_length = 7\n",
        "\n",
        "        self.training_cutoff = df[time_idx].max() - max_prediction_length\n",
        "\n",
        "        self.training = TimeSeriesDataSet(\n",
        "            df[df[time_idx] <= self.training_cutoff],\n",
        "            time_idx=time_idx,\n",
        "            target=target,\n",
        "            group_ids=group_ids,\n",
        "            max_encoder_length=max_encoder_length,\n",
        "            max_prediction_length=max_prediction_length,\n",
        "            static_categoricals=group_ids,\n",
        "            time_varying_known_reals=[time_idx],\n",
        "            time_varying_unknown_reals=[target],\n",
        "            target_normalizer=GroupNormalizer(groups=group_ids),\n",
        "            allow_missings=True,\n",
        "        )\n",
        "\n",
        "        self.validation = TimeSeriesDataSet.from_dataset(\n",
        "            self.training, df[df[time_idx] > self.training_cutoff], min_prediction_idx=self.training_cutoff + 1\n",
        "        )\n",
        "\n",
        "        self.batch_size = 64\n",
        "        self.train_dataloader = DataLoader(self.training, batch_size=self.batch_size, shuffle=True)\n",
        "        self.val_dataloader = DataLoader(self.validation, batch_size=self.batch_size)\n",
        "\n",
        "    def fit(self):\n",
        "        \"\"\"\n",
        "        Fit the selected model.\n",
        "        \"\"\"\n",
        "        # Choose the model\n",
        "        if self.model_name == 'TFT':\n",
        "            self.model = TemporalFusionTransformer.from_dataset(\n",
        "                self.training,\n",
        "                learning_rate=0.03,\n",
        "                hidden_size=16,\n",
        "                attention_head_size=1,\n",
        "                dropout=0.1,\n",
        "                hidden_continuous_size=8,\n",
        "                loss=QuantileLoss(),\n",
        "                log_interval=10,\n",
        "                reduce_on_plateau_patience=4,\n",
        "            )\n",
        "        elif self.model_name == 'RNN':\n",
        "            self.model = RecurrentNetwork.from_dataset(\n",
        "                self.training,\n",
        "                learning_rate=0.03,\n",
        "                hidden_size=16,\n",
        "                rnn_layers=2,\n",
        "                dropout=0.1,\n",
        "                loss=RMSE(),\n",
        "            )\n",
        "        elif self.model_name == 'DeepAR':\n",
        "            self.model = DeepAR.from_dataset(\n",
        "                self.training,\n",
        "                learning_rate=0.03,\n",
        "                rnn_layers=2,\n",
        "                hidden_size=16,\n",
        "                dropout=0.1,\n",
        "            )\n",
        "        elif self.model_name == 'NBeats':\n",
        "            self.model = NBeats.from_dataset(\n",
        "                self.training,\n",
        "                learning_rate=0.03,\n",
        "            )\n",
        "        elif self.model_name == 'NHiTS':\n",
        "            self.model = NHiTS.from_dataset(\n",
        "                self.training,\n",
        "                learning_rate=0.03,\n",
        "            )\n",
        "        elif self.model_name == 'Baseline':\n",
        "            self.model = Baseline()\n",
        "        elif self.model_name == 'AutoRegressive':\n",
        "            self.model = AutoRegressiveModel.from_dataset(\n",
        "                self.training,\n",
        "                learning_rate=0.03,\n",
        "            )\n",
        "        elif self.model_name == 'LSTM':\n",
        "            self.model = LSTMForecaster.from_dataset(\n",
        "                self.training,\n",
        "                learning_rate=0.03,\n",
        "                hidden_size=16,\n",
        "                num_layers=2,\n",
        "                dropout=0.1,\n",
        "            )\n",
        "        elif self.model_name == 'TCN':\n",
        "            self.model = TCNForecaster.from_dataset(\n",
        "                self.training,\n",
        "                learning_rate=0.03,\n",
        "                num_channels=[16]*3,\n",
        "                kernel_size=2,\n",
        "                dropout=0.1,\n",
        "            )\n",
        "        elif self.model_name == 'ProphetLike':\n",
        "            self.model = ProphetLikeModel.from_dataset(\n",
        "                self.training,\n",
        "                learning_rate=0.03,\n",
        "                seasonality=10,\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(f\"Model {self.model_name} is not supported.\")\n",
        "\n",
        "        # Trainer\n",
        "        early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=5, verbose=False, mode=\"min\")\n",
        "        trainer = pl.Trainer(\n",
        "            max_epochs=self.max_epochs,\n",
        "            callbacks=[early_stop_callback],\n",
        "            gradient_clip_val=0.1,\n",
        "        )\n",
        "\n",
        "        # Fit the model\n",
        "        trainer.fit(self.model, train_dataloaders=self.train_dataloader, val_dataloaders=self.val_dataloader)\n",
        "\n",
        "    # The predict and backtest methods remain the same."
      ],
      "metadata": {
        "id": "YGfyD5N3wXGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Include all the necessary imports and classes from previous code blocks\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# Generate synthetic data for demonstration\n",
        "def generate_synthetic_data(num_securities=10, num_days=500):\n",
        "    dates = pd.date_range(start='2020-01-01', periods=num_days, freq='D')\n",
        "    data = []\n",
        "    for security_id in range(num_securities):\n",
        "        price = 100 + np.cumsum(np.random.randn(num_days) * 0.5)\n",
        "        returns = np.diff(price) / price[:-1]\n",
        "        returns = np.insert(returns, 0, 0)  # Insert zero return for the first day\n",
        "        df = pd.DataFrame({\n",
        "            'date': dates,\n",
        "            'security_id': f'sec_{security_id}',\n",
        "            'price': price,\n",
        "            'returns': returns,\n",
        "        })\n",
        "        data.append(df)\n",
        "    df = pd.concat(data)\n",
        "    return df\n",
        "\n",
        "# Generate data\n",
        "df = generate_synthetic_data()\n",
        "\n",
        "# Initialize the wrapper\n",
        "wrapper = AssetAllocationWrapper(model_name='Signature', max_epochs=10) # add , max_leverage=2.0)\n",
        "wrapper.prepare_data(df, time_idx='date', target='returns', group_ids=['security_id'])\n",
        "wrapper.fit()\n",
        "backtest_results = wrapper.backtest()\n",
        "print(backtest_results.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "UeoipSntxbOP",
        "outputId": "bba0a7d5-bd51-438d-dc99-df43b6752dce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'TimeSeriesDataSet' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-d890adf1a2db>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Initialize the wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mwrapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAssetAllocationWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Signature'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# add , max_leverage=2.0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'returns'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'security_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mbacktest_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbacktest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-a72de70aa823>\u001b[0m in \u001b[0;36mprepare_data\u001b[0;34m(self, df, time_idx, target, group_ids)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_cutoff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtime_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmax_prediction_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         self.training = TimeSeriesDataSet(\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtime_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_cutoff\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mtime_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'TimeSeriesDataSet' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming df is your dataset with columns: 'time', 'group_id', 'returns'\n",
        "wrapper = AssetAllocationWrapper(model_name='LSTM', max_epochs=20)\n",
        "wrapper.prepare_data(df, time_idx='time', target='returns', group_ids=['group_id'])\n",
        "wrapper.fit()\n",
        "backtest_results = wrapper.backtest()\n",
        "print(backtest_results.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "SstxPxNjwT2n",
        "outputId": "918d5ef3-1f2a-4f2d-8429-2e85e3972726"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-3df1c3f648a5>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Assuming df is your dataset with columns: 'time', 'group_id', 'returns'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mwrapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAssetAllocationWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'LSTM'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'returns'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'group_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbacktest_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbacktest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## pytorch_forecdasting"
      ],
      "metadata": {
        "id": "DwECxFDux-LF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchcde\n",
        "from pytorch_forecasting.models.base_model import BaseModelWithCovariates\n",
        "from pytorch_forecasting.metrics import RMSE\n",
        "\n",
        "class SignatureModel(BaseModelWithCovariates):\n",
        "    \"\"\"\n",
        "    Signature-based model using Neural Controlled Differential Equations (Neural CDEs).\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size: int, hidden_size: int, output_size: int = 1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.hidden_size = hidden_size\n",
        "        # Define the vector field for the CDE\n",
        "        self.func = torch.nn.Sequential(\n",
        "            torch.nn.Linear(hidden_size, hidden_size),\n",
        "            torch.nn.Tanh(),\n",
        "            torch.nn.Linear(hidden_size, hidden_size * input_size)\n",
        "        )\n",
        "        self.initial_linear = torch.nn.Linear(input_size, hidden_size)\n",
        "        self.readout = torch.nn.Linear(hidden_size, output_size)\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the model.\n",
        "\n",
        "        Args:\n",
        "            x (Dict[str, Tensor]): Input dictionary from TimeSeriesDataSet.\n",
        "\n",
        "        Returns:\n",
        "            Prediction output.\n",
        "        \"\"\"\n",
        "        # x[\"encoder_cont\"] shape: (batch_size, seq_len, input_size)\n",
        "        batch_size, seq_len, input_size = x[\"encoder_cont\"].shape\n",
        "        # Create continuous path using cubic splines\n",
        "        coeffs = torchcde.hermite_cubic_coefficients_with_backward_differences(x[\"encoder_cont\"])\n",
        "        X = torchcde.CubicSpline(coeffs)\n",
        "        # Initial hidden state\n",
        "        z0 = self.initial_linear(X.evaluate(X.interval[0]))\n",
        "        # Solve the CDE\n",
        "        z_T = torchcde.cdeint(X=X, func=self.func, z0=z0, t=X.interval)\n",
        "        # z_T shape: (batch_size, 2, hidden_size)\n",
        "        # We take the last time point\n",
        "        z_T = z_T[:, -1, :]\n",
        "        prediction = self.readout(z_T)\n",
        "        return self.to_network_output(prediction=prediction)\n",
        "\n",
        "    @classmethod\n",
        "    def from_dataset(cls, dataset, **kwargs):\n",
        "        \"\"\"\n",
        "        Create the model from a TimeSeriesDataSet.\n",
        "\n",
        "        Args:\n",
        "            dataset (TimeSeriesDataSet): The dataset used for training.\n",
        "            **kwargs: Additional arguments.\n",
        "\n",
        "        Returns:\n",
        "            SignatureModel instance.\n",
        "        \"\"\"\n",
        "        new_kwargs = {\n",
        "            \"input_size\": len(dataset.reals),\n",
        "            \"loss\": RMSE(),\n",
        "        }\n",
        "        new_kwargs.update(kwargs)\n",
        "        return super().from_dataset(dataset, **new_kwargs)"
      ],
      "metadata": {
        "id": "QAYMB5x0wQfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pytorch_lightning as pl\n",
        "from pytorch_forecasting import (\n",
        "    TimeSeriesDataSet,\n",
        "    Baseline,\n",
        "    TemporalFusionTransformer,\n",
        "    RecurrentNetwork,\n",
        "    DeepAR,\n",
        "    NBeats,\n",
        "    NHiTS,\n",
        ")\n",
        "from pytorch_forecasting.data import GroupNormalizer\n",
        "from pytorch_forecasting.metrics import RMSE, QuantileLoss\n",
        "from pytorch_lightning.callbacks import EarlyStopping\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Include the SignatureModel class code here (from previous code block)\n",
        "\n",
        "class AssetAllocationWrapper:\n",
        "    \"\"\"\n",
        "    Wrapper to train models for asset allocation based on returns.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_name, max_epochs=30, max_leverage=2.0):\n",
        "        \"\"\"\n",
        "        Initialize the wrapper.\n",
        "\n",
        "        Args:\n",
        "            model_name (str): Name of the model to use.\n",
        "            max_epochs (int): Maximum number of training epochs.\n",
        "            max_leverage (float): Maximum leverage allowed.\n",
        "        \"\"\"\n",
        "        self.model_name = model_name\n",
        "        self.max_epochs = max_epochs\n",
        "        self.max_leverage = max_leverage\n",
        "        self.model = None\n",
        "\n",
        "    def prepare_data(self, df, time_idx, target, group_ids):\n",
        "        \"\"\"\n",
        "        Prepare dataset for asset allocation.\n",
        "\n",
        "        Args:\n",
        "            df (pd.DataFrame): Dataset containing returns.\n",
        "            time_idx (str): Name of the time index column.\n",
        "            target (str): Name of the target column (e.g., returns).\n",
        "            group_ids (list): List of group identifier column names (e.g., securities).\n",
        "        \"\"\"\n",
        "        # Ensure time_idx is integer and sort data\n",
        "        df[time_idx] = pd.to_datetime(df[time_idx])\n",
        "        df.sort_values(by=[group_ids[0], time_idx], inplace=True)\n",
        "        df[time_idx] = df[time_idx].dt.strftime('%Y%m%d').astype(int)\n",
        "\n",
        "        # Create TimeSeriesDataSet\n",
        "        max_encoder_length = 30\n",
        "        max_prediction_length = 7\n",
        "\n",
        "        self.training_cutoff = df[time_idx].max() - max_prediction_length\n",
        "\n",
        "        self.training = TimeSeriesDataSet(\n",
        "            df[df[time_idx] <= self.training_cutoff],\n",
        "            time_idx=time_idx,\n",
        "            target=target,\n",
        "            group_ids=group_ids,\n",
        "            max_encoder_length=max_encoder_length,\n",
        "            max_prediction_length=max_prediction_length,\n",
        "            static_categoricals=group_ids,\n",
        "            time_varying_known_reals=[time_idx],\n",
        "            time_varying_unknown_reals=[target],\n",
        "            target_normalizer=GroupNormalizer(groups=group_ids),\n",
        "            allow_missings=True,\n",
        "        )\n",
        "\n",
        "        self.validation = TimeSeriesDataSet.from_dataset(\n",
        "            self.training, df[df[time_idx] > self.training_cutoff], min_prediction_idx=self.training_cutoff + 1\n",
        "        )\n",
        "\n",
        "        self.batch_size = 64\n",
        "        self.train_dataloader = DataLoader(self.training, batch_size=self.batch_size, shuffle=True)\n",
        "        self.val_dataloader = DataLoader(self.validation, batch_size=self.batch_size)\n",
        "\n",
        "    def fit(self):\n",
        "        \"\"\"\n",
        "        Fit the selected model.\n",
        "        \"\"\"\n",
        "        # Choose the model\n",
        "        if self.model_name == 'TFT':\n",
        "            self.model = TemporalFusionTransformer.from_dataset(\n",
        "                self.training,\n",
        "                learning_rate=0.03,\n",
        "                hidden_size=16,\n",
        "                attention_head_size=1,\n",
        "                dropout=0.1,\n",
        "                hidden_continuous_size=8,\n",
        "                loss=QuantileLoss(),\n",
        "                log_interval=10,\n",
        "                reduce_on_plateau_patience=4,\n",
        "            )\n",
        "        elif self.model_name == 'RNN':\n",
        "            self.model = RecurrentNetwork.from_dataset(\n",
        "                self.training,\n",
        "                learning_rate=0.03,\n",
        "                hidden_size=16,\n",
        "                rnn_layers=2,\n",
        "                dropout=0.1,\n",
        "                loss=RMSE(),\n",
        "            )\n",
        "        elif self.model_name == 'DeepAR':\n",
        "            self.model = DeepAR.from_dataset(\n",
        "                self.training,\n",
        "                learning_rate=0.03,\n",
        "                rnn_layers=2,\n",
        "                hidden_size=16,\n",
        "                dropout=0.1,\n",
        "            )\n",
        "        elif self.model_name == 'NBeats':\n",
        "            self.model = NBeats.from_dataset(\n",
        "                self.training,\n",
        "                learning_rate=0.03,\n",
        "            )\n",
        "        elif self.model_name == 'NHiTS':\n",
        "            self.model = NHiTS.from_dataset(\n",
        "                self.training,\n",
        "                learning_rate=0.03,\n",
        "            )\n",
        "        elif self.model_name == 'Baseline':\n",
        "            self.model = Baseline()\n",
        "        elif self.model_name == 'AutoRegressive':\n",
        "            self.model = AutoRegressiveModel.from_dataset(\n",
        "                self.training,\n",
        "                learning_rate=0.03,\n",
        "            )\n",
        "        elif self.model_name == 'LSTM':\n",
        "            self.model = LSTMForecaster.from_dataset(\n",
        "                self.training,\n",
        "                learning_rate=0.03,\n",
        "                hidden_size=16,\n",
        "                num_layers=2,\n",
        "                dropout=0.1,\n",
        "            )\n",
        "        elif self.model_name == 'TCN':\n",
        "            self.model = TCNForecaster.from_dataset(\n",
        "                self.training,\n",
        "                learning_rate=0.03,\n",
        "                num_channels=[16]*3,\n",
        "                kernel_size=2,\n",
        "                dropout=0.1,\n",
        "            )\n",
        "        elif self.model_name == 'ProphetLike':\n",
        "            self.model = ProphetLikeModel.from_dataset(\n",
        "                self.training,\n",
        "                learning_rate=0.03,\n",
        "                seasonality=10,\n",
        "            )\n",
        "        elif self.model_name == 'Signature':\n",
        "            self.model = SignatureModel.from_dataset(\n",
        "                self.training,\n",
        "                learning_rate=0.03,\n",
        "                hidden_size=32,\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(f\"Model {self.model_name} is not supported.\")\n",
        "\n",
        "        # Trainer\n",
        "        early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=5, verbose=False, mode=\"min\")\n",
        "        trainer = pl.Trainer(\n",
        "            max_epochs=self.max_epochs,\n",
        "            callbacks=[early_stop_callback],\n",
        "            gradient_clip_val=0.1,\n",
        "        )\n",
        "\n",
        "        # Fit the model\n",
        "        trainer.fit(self.model, train_dataloaders=self.train_dataloader, val_dataloaders=self.val_dataloader)\n",
        "\n",
        "    def predict(self, df):\n",
        "        \"\"\"\n",
        "        Make predictions on new data.\n",
        "\n",
        "        Args:\n",
        "            df (pd.DataFrame): DataFrame containing new data.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: DataFrame with predictions.\n",
        "        \"\"\"\n",
        "        # Prepare dataset\n",
        "        raw_predictions, x = self.model.predict(df, mode=\"raw\", return_x=True)\n",
        "        predictions = self.model.to_prediction(raw_predictions)\n",
        "\n",
        "        # Combine predictions with input data\n",
        "        df_pred = df.copy()\n",
        "        df_pred['prediction'] = predictions\n",
        "        return df_pred\n",
        "\n",
        "    def backtest(self):\n",
        "        \"\"\"\n",
        "        Perform backtesting using the validation set.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: DataFrame with backtesting predictions.\n",
        "        \"\"\"\n",
        "        actuals = torch.cat([y[0] for x, y in iter(self.val_dataloader)])\n",
        "        predictions = self.model.predict(self.val_dataloader)\n",
        "        df_backtest = pd.DataFrame({\n",
        "            'actual': actuals.numpy().flatten(),\n",
        "            'prediction': predictions.numpy().flatten(),\n",
        "        })\n",
        "\n",
        "        # Adjust leverage based on confidence\n",
        "        df_backtest['confidence'] = self.calculate_confidence(df_backtest)\n",
        "        df_backtest['leverage'] = df_backtest['confidence'] * self.max_leverage\n",
        "        df_backtest['leverage'] = df_backtest['leverage'].clip(0, self.max_leverage)\n",
        "\n",
        "        # Apply leverage to predictions\n",
        "        df_backtest['adjusted_prediction'] = df_backtest['prediction'] * df_backtest['leverage']\n",
        "        return df_backtest\n",
        "\n",
        "    def calculate_confidence(self, df):\n",
        "        \"\"\"\n",
        "        Calculate confidence level based on prediction errors.\n",
        "\n",
        "        Args:\n",
        "            df (pd.DataFrame): DataFrame with 'actual' and 'prediction' columns.\n",
        "\n",
        "        Returns:\n",
        "            pd.Series: Confidence levels between 0 and 1.\n",
        "        \"\"\"\n",
        "        errors = np.abs(df['actual'] - df['prediction'])\n",
        "        max_error = errors.max()\n",
        "        confidence = 1 - (errors / (max_error + 1e-6))\n",
        "        return confidence"
      ],
      "metadata": {
        "id": "O_k-vzz6wMm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(self, df, time_idx, target, group_ids):\n",
        "    \"\"\"\n",
        "    Prepare dataset for asset allocation.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Dataset containing returns.\n",
        "        time_idx (str): Name of the time index column.\n",
        "        target (str): Name of the target column (e.g., returns).\n",
        "        group_ids (list): List of group identifier column names (e.g., securities).\n",
        "    \"\"\"\n",
        "    # Ensure time_idx is integer and sort data\n",
        "    df[time_idx] = pd.to_datetime(df[time_idx])\n",
        "    df.sort_values(by=[group_ids[0], time_idx], inplace=True)\n",
        "    df[time_idx] = df[time_idx].dt.strftime('%Y%m%d').astype(int)\n",
        "\n",
        "    # Create TimeSeriesDataSet\n",
        "    max_encoder_length = 30\n",
        "    max_prediction_length = 7\n",
        "\n",
        "    self.training_cutoff = df[time_idx].max() - max_prediction_length\n",
        "\n",
        "    self.training = TimeSeriesDataSet(\n",
        "        df[df[time_idx] <= self.training_cutoff],\n",
        "        time_idx=time_idx,\n",
        "        target=target,\n",
        "        group_ids=group_ids,\n",
        "        max_encoder_length=max_encoder_length,\n",
        "        max_prediction_length=max_prediction_length,\n",
        "        static_categoricals=group_ids,\n",
        "        time_varying_known_reals=[time_idx],\n",
        "        time_varying_unknown_reals=[target],\n",
        "        target_normalizer=GroupNormalizer(groups=group_ids),\n",
        "        allow_missing_timesteps=True,  # Corrected argument name\n",
        "    )\n",
        "\n",
        "    self.validation = TimeSeriesDataSet.from_dataset(\n",
        "        self.training, df[df[time_idx] > self.training_cutoff], min_prediction_idx=self.training_cutoff + 1\n",
        "    )\n",
        "\n",
        "    self.batch_size = 64\n",
        "    self.train_dataloader = DataLoader(self.training, batch_size=self.batch_size, shuffle=True)\n",
        "    self.val_dataloader = DataLoader(self.validation, batch_size=self.batch_size)"
      ],
      "metadata": {
        "id": "jhma5ed_wJtG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AssetAllocationWrapper:\n",
        "    \"\"\"\n",
        "    Wrapper to train models for asset allocation based on returns.\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    Wrapper to train models for asset allocation based on returns.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_name, max_epochs=30, max_leverage=2.0):\n",
        "        \"\"\"\n",
        "        Initialize the wrapper.\n",
        "\n",
        "        Args:\n",
        "            model_name (str): Name of the model to use.\n",
        "            max_epochs (int): Maximum number of training epochs.\n",
        "            max_leverage (float): Maximum leverage allowed.\n",
        "        \"\"\"\n",
        "        self.model_name = model_name\n",
        "        self.max_epochs = max_epochs\n",
        "        self.max_leverage = max_leverage\n",
        "        self.model = None\n",
        "\n",
        "    def prepare_data(self, df, time_idx, target, group_ids):\n",
        "        \"\"\"\n",
        "        Prepare dataset for asset allocation.\n",
        "        \"\"\"\n",
        "        # Ensure time_idx is datetime and sort data\n",
        "        df[time_idx] = pd.to_datetime(df[time_idx])\n",
        "        df.sort_values(by=group_ids + [time_idx], inplace=True)\n",
        "\n",
        "        # Assign a unique sequential integer time index within each group\n",
        "        df['time_idx'] = df.groupby(group_ids).cumcount()\n",
        "\n",
        "        # Reindex each group to fill missing time_idx\n",
        "        df_list = []\n",
        "        grouped = df.groupby(group_ids)\n",
        "        for name, group in grouped:\n",
        "            idx = pd.Index(range(group['time_idx'].min(), group['time_idx'].max() + 1), name='time_idx')\n",
        "            group = group.set_index('time_idx').reindex(idx).reset_index()\n",
        "            if isinstance(name, tuple):\n",
        "                for i, gid in enumerate(group_ids):\n",
        "                    group[gid] = name[i]\n",
        "            else:\n",
        "                group[group_ids[0]] = name\n",
        "            # Fill missing values as needed\n",
        "            group[target] = group[target].fillna(0)  # Or use appropriate method\n",
        "            df_list.append(group)\n",
        "        df = pd.concat(df_list).reset_index(drop=True)\n",
        "\n",
        "        # Now, time_idx should be consecutive within each group\n",
        "        max_encoder_length = 20  # Reduced from 30\n",
        "        max_prediction_length = 5  # Reduced from 7\n",
        "\n",
        "        self.training_cutoff = df['time_idx'].max() - max_prediction_length\n",
        "\n",
        "        self.training = TimeSeriesDataSet(\n",
        "            df[df['time_idx'] <= self.training_cutoff],\n",
        "            time_idx='time_idx',\n",
        "            target=target,\n",
        "            group_ids=group_ids,\n",
        "            max_encoder_length=max_encoder_length,\n",
        "            min_encoder_length=10,\n",
        "            max_prediction_length=max_prediction_length,\n",
        "            min_prediction_length=1,\n",
        "            static_categoricals=group_ids,\n",
        "            time_varying_known_reals=['time_idx'],\n",
        "            time_varying_unknown_reals=[target],\n",
        "            target_normalizer=GroupNormalizer(groups=group_ids),\n",
        "            allow_missing_timesteps=True,\n",
        "        )\n",
        "\n",
        "        self.validation = TimeSeriesDataSet.from_dataset(\n",
        "            self.training, df[df['time_idx'] > self.training_cutoff], min_prediction_idx=self.training_cutoff + 1\n",
        "        )\n",
        "\n",
        "        self.batch_size = 64\n",
        "        self.train_dataloader = DataLoader(self.training, batch_size=self.batch_size, shuffle=True)\n",
        "        self.val_dataloader = DataLoader(self.validation, batch_size=self.batch_size)\n",
        "    def fit(self):\n",
        "        \"\"\"\n",
        "        Fit the selected model.\n",
        "        \"\"\"\n",
        "        # Choose the model\n",
        "        if self.model_name == 'TFT':\n",
        "            self.model = TemporalFusionTransformer.from_dataset(\n",
        "                self.training,\n",
        "                learning_rate=0.03,\n",
        "                hidden_size=16,\n",
        "                attention_head_size=1,\n",
        "                dropout=0.1,\n",
        "                hidden_continuous_size=8,\n",
        "                loss=QuantileLoss(),\n",
        "                log_interval=10,\n",
        "                reduce_on_plateau_patience=4,\n",
        "            )\n",
        "        elif self.model_name == 'RNN':\n",
        "            self.model = RecurrentNetwork.from_dataset(\n",
        "                self.training,\n",
        "                learning_rate=0.03,\n",
        "                hidden_size=16,\n",
        "                rnn_layers=2,\n",
        "                dropout=0.1,\n",
        "                loss=RMSE(),\n",
        "            )\n",
        "        elif self.model_name == 'DeepAR':\n",
        "            self.model = DeepAR.from_dataset(\n",
        "                self.training,\n",
        "                learning_rate=0.03,\n",
        "                rnn_layers=2,\n",
        "                hidden_size=16,\n",
        "                dropout=0.1,\n",
        "            )\n",
        "        elif self.model_name == 'NBeats':\n",
        "            self.model = NBeats.from_dataset(\n",
        "                self.training,\n",
        "                learning_rate=0.03,\n",
        "            )\n",
        "        elif self.model_name == 'NHiTS':\n",
        "            self.model = NHiTS.from_dataset(\n",
        "                self.training,\n",
        "                learning_rate=0.03,\n",
        "            )\n",
        "        elif self.model_name == 'Baseline':\n",
        "            self.model = Baseline()\n",
        "        elif self.model_name == 'AutoRegressive':\n",
        "            self.model = AutoRegressiveModel.from_dataset(\n",
        "                self.training,\n",
        "                learning_rate=0.03,\n",
        "            )\n",
        "        elif self.model_name == 'LSTM':\n",
        "            self.model = LSTMForecaster.from_dataset(\n",
        "                self.training,\n",
        "                learning_rate=0.03,\n",
        "                hidden_size=16,\n",
        "                num_layers=2,\n",
        "                dropout=0.1,\n",
        "            )\n",
        "        elif self.model_name == 'TCN':\n",
        "            self.model = TCNForecaster.from_dataset(\n",
        "                self.training,\n",
        "                learning_rate=0.03,\n",
        "                num_channels=[16]*3,\n",
        "                kernel_size=2,\n",
        "                dropout=0.1,\n",
        "            )\n",
        "        elif self.model_name == 'ProphetLike':\n",
        "            self.model = ProphetLikeModel.from_dataset(\n",
        "                self.training,\n",
        "                learning_rate=0.03,\n",
        "                seasonality=10,\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(f\"Model {self.model_name} is not supported.\")\n",
        "\n",
        "        # Trainer\n",
        "        early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=5, verbose=False, mode=\"min\")\n",
        "        trainer = pl.Trainer(\n",
        "            max_epochs=self.max_epochs,\n",
        "            callbacks=[early_stop_callback],\n",
        "            gradient_clip_val=0.1,\n",
        "        )\n",
        "\n",
        "        # Fit the model\n",
        "        trainer.fit(self.model, train_dataloaders=self.train_dataloader, val_dataloaders=self.val_dataloader)\n",
        "\n",
        "    # The predict and backtest methods remain the same."
      ],
      "metadata": {
        "id": "pr9DU_lOzLaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate synthetic data for demonstration\n",
        "def generate_synthetic_data(num_securities=10, num_days=500):\n",
        "    dates = pd.date_range(start='2020-01-01', periods=num_days, freq='D')\n",
        "    data = []\n",
        "    for security_id in range(num_securities):\n",
        "        price = 100 + np.cumsum(np.random.randn(num_days) * 0.5)\n",
        "        returns = np.diff(price) / price[:-1]\n",
        "        returns = np.insert(returns, 0, 0)  # Insert zero return for the first day\n",
        "        df = pd.DataFrame({\n",
        "            'date': dates,\n",
        "            'security_id': f'sec_{security_id}',\n",
        "            'price': price,\n",
        "            'returns': returns,\n",
        "        })\n",
        "        data.append(df)\n",
        "    df = pd.concat(data)\n",
        "    return df\n",
        "max_encoder_length = 28\n",
        "max_prediction_length = 7\n",
        "# Generate data\n",
        "df = generate_synthetic_data()\n",
        "\n",
        "# Initialize the wrapper\n",
        "wrapper = AssetAllocationWrapper(model_name='Signature', max_epochs=10, max_leverage=2.0)\n",
        "wrapper.prepare_data(df, time_idx='date', target='returns', group_ids=['security_id'])\n",
        "wrapper.fit()\n",
        "backtest_results = wrapper.backtest()\n",
        "print(backtest_results.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "AyJfE1OI1BNR",
        "outputId": "31974f54-b038-4d47-b843-0b858c6b1818"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_forecasting/data/timeseries.py:1282: UserWarning: Min encoder length and/or min_prediction_idx and/or min prediction length and/or lags are too large for 10 series/groups which therefore are not present in the dataset index. This means no predictions can be made for those series. First 10 removed groups: [{'__group_id__security_id': 'sec_0'}, {'__group_id__security_id': 'sec_1'}, {'__group_id__security_id': 'sec_2'}, {'__group_id__security_id': 'sec_3'}, {'__group_id__security_id': 'sec_4'}, {'__group_id__security_id': 'sec_5'}, {'__group_id__security_id': 'sec_6'}, {'__group_id__security_id': 'sec_7'}, {'__group_id__security_id': 'sec_8'}, {'__group_id__security_id': 'sec_9'}]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "filters should not remove entries all entries - check encoder/decoder lengths and lags",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-06429bec2e1b>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Initialize the wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mwrapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAssetAllocationWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Signature'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_leverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'returns'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'security_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mbacktest_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbacktest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-3fca1145e440>\u001b[0m in \u001b[0;36mprepare_data\u001b[0;34m(self, df, time_idx, target, group_ids)\u001b[0m\n\u001b[1;32m     71\u001b[0m         )\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         self.validation = TimeSeriesDataSet.from_dataset(\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time_idx'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_cutoff\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_prediction_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_cutoff\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_forecasting/data/timeseries.py\u001b[0m in \u001b[0;36mfrom_dataset\u001b[0;34m(cls, dataset, data, stop_randomization, predict, **update_kwargs)\u001b[0m\n\u001b[1;32m   1153\u001b[0m             \u001b[0mTimeSeriesDataSet\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m         \"\"\"\n\u001b[0;32m-> 1155\u001b[0;31m         return cls.from_parameters(\n\u001b[0m\u001b[1;32m   1156\u001b[0m             \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_randomization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop_randomization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mupdate_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_forecasting/data/timeseries.py\u001b[0m in \u001b[0;36mfrom_parameters\u001b[0;34m(cls, parameters, data, stop_randomization, predict, **update_kwargs)\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1201\u001b[0;31m         \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1202\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_forecasting/data/timeseries.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, time_idx, target, group_ids, weight, max_encoder_length, min_encoder_length, min_prediction_idx, min_prediction_length, max_prediction_length, static_categoricals, static_reals, time_varying_known_categoricals, time_varying_known_reals, time_varying_unknown_categoricals, time_varying_unknown_reals, variable_groups, constant_fill_strategy, allow_missing_timesteps, lags, add_relative_time_idx, add_target_scales, add_encoder_length, target_normalizer, categorical_encoders, scalers, randomize_length, predict_mode)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;31m# create index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0;31m# convert to torch tensor for high performance data loading later\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_forecasting/data/timeseries.py\u001b[0m in \u001b[0;36m_construct_index\u001b[0;34m(self, data, predict_mode)\u001b[0m\n\u001b[1;32m   1289\u001b[0m             )\n\u001b[1;32m   1290\u001b[0m         assert (\n\u001b[0;32m-> 1291\u001b[0;31m             \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1292\u001b[0m         ), \"filters should not remove entries all entries - check encoder/decoder lengths and lags\"\n\u001b[1;32m   1293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: filters should not remove entries all entries - check encoder/decoder lengths and lags"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pytorch_lightning as pl\n",
        "from pytorch_forecasting import (\n",
        "    TimeSeriesDataSet,\n",
        "    Baseline,\n",
        "    TemporalFusionTransformer,\n",
        "    RecurrentNetwork,\n",
        "    DeepAR,\n",
        "    NBeats,\n",
        "    NHiTS,\n",
        ")\n",
        "from pytorch_forecasting.data import GroupNormalizer\n",
        "from pytorch_forecasting.metrics import RMSE, QuantileLoss\n",
        "from pytorch_lightning.callbacks import EarlyStopping\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Include the SignatureModel class code here (from previous code block)\n",
        "# Ensure that you have installed torchcde\n",
        "# pip install torchcde\n",
        "\n",
        "# The SignatureModel class remains the same...\n",
        "\n",
        "\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_forecasting import (\n",
        "    TimeSeriesDataSet,\n",
        "    Baseline,\n",
        "    TemporalFusionTransformer,\n",
        "    RecurrentNetwork,\n",
        "    DeepAR,\n",
        "    NBeats,\n",
        "    NHiTS,\n",
        ")\n",
        "from pytorch_forecasting.data import GroupNormalizer\n",
        "from pytorch_forecasting.metrics import RMSE, QuantileLoss\n",
        "from pytorch_lightning.callbacks import EarlyStopping\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Include the SignatureModel class code here (from previous code block)\n",
        "# Ensure that you have installed torchcde\n",
        "# pip install torchcde\n",
        "\n",
        "# The SignatureModel class remains the same...\n",
        "\n",
        "class AssetAllocationWrapper:\n",
        "    \"\"\"\n",
        "    Wrapper to train models for asset allocation based on returns.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_name, max_epochs=30, max_leverage=2.0):\n",
        "        \"\"\"\n",
        "        Initialize the wrapper.\n",
        "\n",
        "        Args:\n",
        "            model_name (str): Name of the model to use.\n",
        "            max_epochs (int): Maximum number of training epochs.\n",
        "            max_leverage (float): Maximum leverage allowed.\n",
        "        \"\"\"\n",
        "        self.model_name = model_name\n",
        "        self.max_epochs = max_epochs\n",
        "        self.max_leverage = max_leverage\n",
        "        self.model = None\n",
        "\n",
        "    def prepare_data(self, df, time_idx, target, group_ids):\n",
        "        \"\"\"\n",
        "        Prepare dataset for asset allocation.\n",
        "\n",
        "        Args:\n",
        "            df (pd.DataFrame): Dataset containing returns.\n",
        "            time_idx (str): Name of the time index column.\n",
        "            target (str): Name of the target column (e.g., returns).\n",
        "            group_ids (list): List of group identifier column names (e.g., securities).\n",
        "        \"\"\"\n",
        "        # Ensure time_idx is datetime and sort data\n",
        "        df[time_idx] = pd.to_datetime(df[time_idx])\n",
        "        df.sort_values(by=group_ids + [time_idx], inplace=True)\n",
        "\n",
        "        # Assign a unique sequential integer time index within each group\n",
        "        df['time_idx'] = df.groupby(group_ids).cumcount()\n",
        "\n",
        "        # Create TimeSeriesDataSet\n",
        "        max_encoder_length = 30\n",
        "        max_prediction_length = 7\n",
        "\n",
        "        self.training_cutoff = df['time_idx'].max() - max_prediction_length\n",
        "\n",
        "        self.training = TimeSeriesDataSet(\n",
        "            df[df['time_idx'] <= self.training_cutoff],\n",
        "            time_idx='time_idx',\n",
        "            target=target,\n",
        "            group_ids=group_ids,\n",
        "            max_encoder_length=max_encoder_length,\n",
        "            max_prediction_length=max_prediction_length,\n",
        "            static_categoricals=group_ids,\n",
        "            time_varying_known_reals=['time_idx'],\n",
        "            time_varying_unknown_reals=[target],\n",
        "            target_normalizer=GroupNormalizer(groups=group_ids),\n",
        "            allow_missing_timesteps=True,\n",
        "        )\n",
        "\n",
        "        self.validation = TimeSeriesDataSet.from_dataset(\n",
        "            self.training, df[df['time_idx'] > self.training_cutoff], min_prediction_idx=self.training_cutoff + 1\n",
        "        )\n",
        "\n",
        "        self.batch_size = 64\n",
        "        self.train_dataloader = DataLoader(self.training, batch_size=self.batch_size, shuffle=True)\n",
        "        self.val_dataloader = DataLoader(self.validation, batch_size=self.batch_size)\n",
        "\n",
        "    # The rest of the class remains the same...\n",
        "    def fit(self):\n",
        "        \"\"\"\n",
        "        Fit the selected model.\n",
        "        \"\"\"\n",
        "        # Choose the model\n",
        "        if self.model_name == 'TFT':\n",
        "            self.model = TemporalFusionTransformer.from_dataset(\n",
        "                self.training,\n",
        "                learning_rate=0.03,\n",
        "                hidden_size=16,\n",
        "                attention_head_size=1,\n",
        "                dropout=0.1,\n",
        "                hidden_continuous_size=8,\n",
        "                loss=QuantileLoss(),\n",
        "                log_interval=10,\n",
        "                reduce_on_plateau_patience=4,\n",
        "            )\n",
        "        elif self.model_name == 'RNN':\n",
        "            self.model = RecurrentNetwork.from_dataset(\n",
        "                self.training,\n",
        "                learning_rate=0.03,\n",
        "                hidden_size=16,\n",
        "                rnn_layers=2,\n",
        "                dropout=0.1,\n",
        "                loss=RMSE(),\n",
        "            )\n",
        "        elif self.model_name == 'DeepAR':\n",
        "            self.model = DeepAR.from_dataset(\n",
        "                self.training,\n",
        "                learning_rate=0.03,\n",
        "                rnn_layers=2,\n",
        "                hidden_size=16,\n",
        "                dropout=0.1,\n",
        "            )\n",
        "        elif self.model_name == 'NBeats':\n",
        "            self.model = NBeats.from_dataset(\n",
        "                self.training,\n",
        "                learning_rate=0.03,\n",
        "            )\n",
        "        elif self.model_name == 'NHiTS':\n",
        "            self.model = NHiTS.from_dataset(\n",
        "                self.training,\n",
        "                learning_rate=0.03,\n",
        "            )\n",
        "        elif self.model_name == 'Baseline':\n",
        "            self.model = Baseline()\n",
        "        elif self.model_name == 'AutoRegressive':\n",
        "            self.model = AutoRegressiveModel.from_dataset(\n",
        "                self.training,\n",
        "                learning_rate=0.03,\n",
        "            )\n",
        "        elif self.model_name == 'LSTM':\n",
        "            self.model = LSTMForecaster.from_dataset(\n",
        "                self.training,\n",
        "                learning_rate=0.03,\n",
        "                hidden_size=16,\n",
        "                num_layers=2,\n",
        "                dropout=0.1,\n",
        "            )\n",
        "        elif self.model_name == 'TCN':\n",
        "            self.model = TCNForecaster.from_dataset(\n",
        "                self.training,\n",
        "                learning_rate=0.03,\n",
        "                num_channels=[16]*3,\n",
        "                kernel_size=2,\n",
        "                dropout=0.1,\n",
        "            )\n",
        "        elif self.model_name == 'ProphetLike':\n",
        "            self.model = ProphetLikeModel.from_dataset(\n",
        "                self.training,\n",
        "                learning_rate=0.03,\n",
        "                seasonality=10,\n",
        "            )\n",
        "        elif self.model_name == 'Signature':\n",
        "            self.model = SignatureModel.from_dataset(\n",
        "                self.training,\n",
        "                learning_rate=0.03,\n",
        "                hidden_size=32,\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(f\"Model {self.model_name} is not supported.\")\n",
        "\n",
        "        # Trainer\n",
        "        early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=5, verbose=False, mode=\"min\")\n",
        "        trainer = pl.Trainer(\n",
        "            max_epochs=self.max_epochs,\n",
        "            callbacks=[early_stop_callback],\n",
        "            gradient_clip_val=0.1,\n",
        "        )\n",
        "\n",
        "        # Fit the model\n",
        "        trainer.fit(self.model, train_dataloaders=self.train_dataloader, val_dataloaders=self.val_dataloader)\n",
        "\n",
        "    def predict(self, df):\n",
        "        \"\"\"\n",
        "        Make predictions on new data.\n",
        "\n",
        "        Args:\n",
        "            df (pd.DataFrame): DataFrame containing new data.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: DataFrame with predictions.\n",
        "        \"\"\"\n",
        "        # Prepare dataset\n",
        "        raw_predictions, x = self.model.predict(df, mode=\"raw\", return_x=True)\n",
        "        predictions = self.model.to_prediction(raw_predictions)\n",
        "\n",
        "        # Combine predictions with input data\n",
        "        df_pred = df.copy()\n",
        "        df_pred['prediction'] = predictions\n",
        "        return df_pred\n",
        "\n",
        "    def backtest(self):\n",
        "        \"\"\"\n",
        "        Perform backtesting using the validation set.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: DataFrame with backtesting predictions.\n",
        "        \"\"\"\n",
        "        actuals = torch.cat([y[0] for x, y in iter(self.val_dataloader)])\n",
        "        predictions = self.model.predict(self.val_dataloader)\n",
        "        df_backtest = pd.DataFrame({\n",
        "            'actual': actuals.numpy().flatten(),\n",
        "            'prediction': predictions.numpy().flatten(),\n",
        "        })\n",
        "\n",
        "        # Adjust leverage based on confidence\n",
        "        df_backtest['confidence'] = self.calculate_confidence(df_backtest)\n",
        "        df_backtest['leverage'] = df_backtest['confidence'] * self.max_leverage\n",
        "        df_backtest['leverage'] = df_backtest['leverage'].clip(0, self.max_leverage)\n",
        "\n",
        "        # Apply leverage to predictions\n",
        "        df_backtest['adjusted_prediction'] = df_backtest['prediction'] * df_backtest['leverage']\n",
        "        return df_backtest\n",
        "\n",
        "    def calculate_confidence(self, df):\n",
        "        \"\"\"\n",
        "        Calculate confidence level based on prediction errors.\n",
        "\n",
        "        Args:\n",
        "            df (pd.DataFrame): DataFrame with 'actual' and 'prediction' columns.\n",
        "\n",
        "        Returns:\n",
        "            pd.Series: Confidence levels between 0 and 1.\n",
        "        \"\"\"\n",
        "        errors = np.abs(df['actual'] - df['prediction'])\n",
        "        max_error = errors.max()\n",
        "        confidence = 1 - (errors / (max_error + 1e-6))\n",
        "        return confidence\n",
        "\n",
        "# Include the implementations of the other models (AutoRegressiveModel, LSTMForecaster, TCNForecaster, ProphetLikeModel, SignatureModel)\n",
        "\n",
        "# Example usage:\n",
        "# Generate synthetic data for demonstration\n",
        "def generate_synthetic_data(num_securities=10, num_days=500):\n",
        "    dates = pd.date_range(start='2020-01-01', periods=num_days, freq='D')\n",
        "    data = []\n",
        "    for security_id in range(num_securities):\n",
        "        price = 100 + np.cumsum(np.random.randn(num_days) * 0.5)\n",
        "        returns = np.diff(price) / price[:-1]\n",
        "        returns = np.insert(returns, 0, 0)  # Insert zero return for the first day\n",
        "        df = pd.DataFrame({\n",
        "            'date': dates,\n",
        "            'security_id': f'sec_{security_id}',\n",
        "            'price': price,\n",
        "            'returns': returns,\n",
        "        })\n",
        "        data.append(df)\n",
        "    df = pd.concat(data)\n",
        "    return df\n",
        "\n",
        "# Generate data\n",
        "df = generate_synthetic_data()\n",
        "\n",
        "# Initialize the wrapper\n",
        "wrapper = AssetAllocationWrapper(model_name='Signature', max_epochs=10, max_leverage=2.0)\n",
        "wrapper.prepare_data(df, time_idx='date', target='returns', group_ids=['security_id'])\n",
        "wrapper.fit()\n",
        "backtest_results = wrapper.backtest()\n",
        "print(backtest_results.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "MF_G49Irz4os",
        "outputId": "512e7706-34a5-4cde-f401-43138fac0bcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "data index has to be unique",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-e86303694dba>\u001b[0m in \u001b[0;36m<cell line: 292>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;31m# Initialize the wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[0mwrapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAssetAllocationWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Signature'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_leverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'returns'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'security_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0mbacktest_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbacktest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-e86303694dba>\u001b[0m in \u001b[0;36mprepare_data\u001b[0;34m(self, df, time_idx, target, group_ids)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_cutoff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time_idx'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmax_prediction_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         self.training = TimeSeriesDataSet(\n\u001b[0m\u001b[1;32m     94\u001b[0m             \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time_idx'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_cutoff\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mtime_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'time_idx'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_forecasting/data/timeseries.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, time_idx, target, group_ids, weight, max_encoder_length, min_encoder_length, min_prediction_idx, min_prediction_length, max_prediction_length, static_categoricals, static_reals, time_varying_known_categoricals, time_varying_known_reals, time_varying_unknown_categoricals, time_varying_unknown_reals, variable_groups, constant_fill_strategy, allow_missing_timesteps, lags, add_relative_time_idx, add_target_scales, add_encoder_length, target_normalizer, categorical_encoders, scalers, randomize_length, predict_mode)\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;31m# validate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data index has to be unique\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;31m# add lags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: data index has to be unique"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.reset_index(inplace=True)"
      ],
      "metadata": {
        "id": "1eC4JHmW0CK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_names = ['TFT', 'RNN', 'DeepAR', 'NBeats', 'NHiTS', 'Baseline', 'AutoRegressive', 'LSTM', 'TCN', 'ProphetLike', 'Signature']\n",
        "results = {}\n",
        "\n",
        "for model_name in model_names:\n",
        "    print(f\"Training aand backtesting with model: {model_name}\")\n",
        "    wrapper = AssetAllocationWrapper(model_name=model_name, max_epochs=10, max_leverage=2.0)\n",
        "    wrapper.prepare_data(df, time_idx='date', target='returns', group_ids=['security_id'])\n",
        "    wrapper.fit()\n",
        "    backtest_results = wrapper.backtest()\n",
        "    results[model_name] = backtest_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "Rj72fdshwFbN",
        "outputId": "cc2d7cbf-688d-45f7-cd9c-7b433db27441"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training and backtesting with model: TFT\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required by StandardScaler.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-f9b2ba8f5c7c>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training and backtesting with model: {model_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mwrapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAssetAllocationWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_leverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'returns'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'security_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mbacktest_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbacktest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-d99fb32c04a9>\u001b[0m in \u001b[0;36mprepare_data\u001b[0;34m(self, df, time_idx, target, group_ids)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_cutoff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtime_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmax_prediction_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         self.training = TimeSeriesDataSet(\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtime_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_cutoff\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mtime_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_forecasting/data/timeseries.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, time_idx, target, group_ids, weight, max_encoder_length, min_encoder_length, min_prediction_idx, min_prediction_length, max_prediction_length, static_categoricals, static_reals, time_varying_known_categoricals, time_varying_known_reals, time_varying_unknown_categoricals, time_varying_unknown_reals, variable_groups, constant_fill_strategy, allow_missing_timesteps, lags, add_relative_time_idx, add_target_scales, add_encoder_length, target_normalizer, categorical_encoders, scalers, randomize_length, predict_mode)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;31m# preprocess data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Target normalizer is separate and not in scalers.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_forecasting/data/timeseries.py\u001b[0m in \u001b[0;36m_preprocess_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    817\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    876\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0m_fit_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefer_skip_nested_validation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1471\u001b[0m                 )\n\u001b[1;32m   1472\u001b[0m             ):\n\u001b[0;32m-> 1473\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    912\u001b[0m         \"\"\"\n\u001b[1;32m    913\u001b[0m         \u001b[0mfirst_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"n_samples_seen_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         X = self._validate_data(\n\u001b[0m\u001b[1;32m    915\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1087\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1088\u001b[0m                 \u001b[0;34m\"Found array with %d sample(s) (shape=%s) while a\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m                 \u001b[0;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required by StandardScaler."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C2WXLQWozKhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for model_name, df_backtest in results.items():\n",
        "    # Compute strategy returns\n",
        "    df_backtest['strategy_returns'] = df_backtest['adjusted_prediction'] * df_backtest['actual']\n",
        "    # Compute cumulative returns\n",
        "    df_backtest['cumulative_returns'] = (1 + df_backtest['strategy_returns']).cumprod() - 1\n",
        "    plt.plot(df_backtest['cumulative_returns'].values, label=model_name)\n",
        "\n",
        "plt.legend()\n",
        "plt.title('Cumulative Returns Comparison')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Cumulative Returns')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "w3imBbHwv9t2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##claude"
      ],
      "metadata": {
        "id": "DHgi-pQNsE7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Financial Time Series Forecasting Package\n",
        "---------------------------------------\n",
        "A comprehensive package for financial time series forecasting using various deep learning models.\n",
        "\n",
        "This package includes:\n",
        "- Multiple model architectures (AutoRegressive, LSTM, TCN, ProphetLike)\n",
        "- Data validation and preprocessing\n",
        "- Model checkpointing and logging\n",
        "- Configuration management\n",
        "- Comprehensive testing suite\n",
        "\n",
        "Author: Assistant\n",
        "Date: 2024-10-20\n",
        "Version: 1.0.0\n",
        "\"\"\"\n",
        "\n",
        "import logging\n",
        "import yaml\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pytorch_lightning as pl\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, List, Dict, Any, Tuple, Union\n",
        "from datetime import datetime, timedelta\n",
        "from pytorch_lightning.callbacks import EarlyStopping\n",
        "from torch.utils.data import DataLoader\n",
        "from pytorch_forecasting import TimeSeriesDataSet\n",
        "from pytorch_forecasting.metrics import RMSE, QuantileLoss\n",
        "from scipy import stats\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    \"\"\"Configuration class for model parameters.\n",
        "\n",
        "    Attributes:\n",
        "        model_name (str): Name of the model to use\n",
        "        max_epochs (int): Maximum number of training epochs\n",
        "        batch_size (int): Batch size for training\n",
        "        learning_rate (float): Learning rate for optimization\n",
        "        hidden_size (int): Number of hidden units in layers\n",
        "        dropout (float): Dropout rate for regularization\n",
        "        max_encoder_length (int): Maximum length of encoder sequence\n",
        "        max_prediction_length (int): Maximum length of prediction sequence\n",
        "    \"\"\"\n",
        "    model_name: str\n",
        "    max_epochs: int = 30\n",
        "    batch_size: int = 64\n",
        "    learning_rate: float = 0.03\n",
        "    hidden_size: int = 16\n",
        "    dropout: float = 0.1\n",
        "    max_encoder_length: int = 30\n",
        "    max_prediction_length: int = 7\n",
        "\n",
        "    @classmethod\n",
        "    def from_yaml(cls, yaml_path: str) -> 'ModelConfig':\n",
        "        \"\"\"Load configuration from YAML file.\n",
        "\n",
        "        Args:\n",
        "            yaml_path: Path to YAML configuration file\n",
        "\n",
        "        Returns:\n",
        "            ModelConfig instance\n",
        "        \"\"\"\n",
        "        with open(yaml_path, 'r') as f:\n",
        "            config_dict = yaml.safe_load(f)\n",
        "        return cls(**config_dict)\n",
        "\n",
        "    def to_yaml(self, yaml_path: str) -> None:\n",
        "        \"\"\"Save configuration to YAML file.\n",
        "\n",
        "        Args:\n",
        "            yaml_path: Path to save configuration\n",
        "        \"\"\"\n",
        "        with open(yaml_path, 'w') as f:\n",
        "            yaml.dump(self.__dict__, f)\n",
        "\n",
        "class DataValidator:\n",
        "    \"\"\"Data validation utilities.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def validate_dataframe(df: pd.DataFrame, required_columns: List[str]) -> bool:\n",
        "        \"\"\"Validate DataFrame structure and content.\n",
        "\n",
        "        Args:\n",
        "            df: Input DataFrame\n",
        "            required_columns: List of required column names\n",
        "\n",
        "        Returns:\n",
        "            bool: True if validation passes\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If validation fails\n",
        "        \"\"\"\n",
        "        # Check for required columns\n",
        "        missing_cols = set(required_columns) - set(df.columns)\n",
        "        if missing_cols:\n",
        "            raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
        "\n",
        "        # Check for null values\n",
        "        null_cols = df.columns[df.isnull().any()].tolist()\n",
        "        if null_cols:\n",
        "            raise ValueError(f\"Null values found in columns: {null_cols}\")\n",
        "\n",
        "        # Check for infinite values\n",
        "        inf_cols = df.columns[np.isinf(df.select_dtypes(include=np.number)).any()].tolist()\n",
        "        if inf_cols:\n",
        "            raise ValueError(f\"Infinite values found in columns: {inf_cols}\")\n",
        "\n",
        "        return True\n",
        "\n",
        "class ModelCheckpointer:\n",
        "    \"\"\"Model checkpointing utilities.\"\"\"\n",
        "\n",
        "    def __init__(self, checkpoint_dir: str):\n",
        "        \"\"\"Initialize checkpointer.\n",
        "\n",
        "        Args:\n",
        "            checkpoint_dir: Directory to store checkpoints\n",
        "        \"\"\"\n",
        "        self.checkpoint_dir = Path(checkpoint_dir)\n",
        "        self.checkpoint_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    def save_checkpoint(self, model: torch.nn.Module, epoch: int,\n",
        "                       optimizer: torch.optim.Optimizer, loss: float) -> str:\n",
        "        \"\"\"Save model checkpoint.\n",
        "\n",
        "        Args:\n",
        "            model: PyTorch model\n",
        "            epoch: Current epoch number\n",
        "            optimizer: PyTorch optimizer\n",
        "            loss: Current loss value\n",
        "\n",
        "        Returns:\n",
        "            str: Path to saved checkpoint\n",
        "        \"\"\"\n",
        "        checkpoint_path = self.checkpoint_dir / f\"checkpoint_epoch_{epoch}.pt\"\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "        }, checkpoint_path)\n",
        "        return str(checkpoint_path)\n",
        "\n",
        "    def load_checkpoint(self, model: torch.nn.Module,\n",
        "                       optimizer: torch.optim.Optimizer,\n",
        "                       checkpoint_path: str) -> Tuple[int, float]:\n",
        "        \"\"\"Load model checkpoint.\n",
        "\n",
        "        Args:\n",
        "            model: PyTorch model\n",
        "            optimizer: PyTorch optimizer\n",
        "            checkpoint_path: Path to checkpoint file\n",
        "\n",
        "        Returns:\n",
        "            Tuple containing (epoch_number, loss_value)\n",
        "        \"\"\"\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        return checkpoint['epoch'], checkpoint['loss']\n",
        "\n",
        "class AutoRegressiveModel(pl.LightningModule):\n",
        "    \"\"\"Simple Autoregressive model using a Linear layer.\"\"\"\n",
        "\n",
        "    def __init__(self, input_size: int, output_size: int = 1, **kwargs):\n",
        "        \"\"\"Initialize AR model.\n",
        "\n",
        "        Args:\n",
        "            input_size: Number of input features\n",
        "            output_size: Number of output features\n",
        "            **kwargs: Additional arguments\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.linear = torch.nn.Linear(input_size, output_size)\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "    def forward(self, x: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
        "        \"\"\"Forward pass.\n",
        "\n",
        "        Args:\n",
        "            x: Dictionary containing input tensors\n",
        "\n",
        "        Returns:\n",
        "            Model predictions\n",
        "        \"\"\"\n",
        "        encoder_output = x[\"encoder_cont\"][:, -1, :]  # Use last time step\n",
        "        prediction = self.linear(encoder_output)\n",
        "        return prediction\n",
        "\n",
        "class LSTMForecaster(pl.LightningModule):\n",
        "    \"\"\"LSTM-based Forecaster.\"\"\"\n",
        "\n",
        "    def __init__(self, input_size: int, hidden_size: int, num_layers: int,\n",
        "                 dropout: float = 0.0, **kwargs):\n",
        "        \"\"\"Initialize LSTM model.\n",
        "\n",
        "        Args:\n",
        "            input_size: Number of input features\n",
        "            hidden_size: Number of hidden units\n",
        "            num_layers: Number of LSTM layers\n",
        "            dropout: Dropout rate\n",
        "            **kwargs: Additional arguments\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.lstm = torch.nn.LSTM(\n",
        "            input_size, hidden_size, num_layers,\n",
        "            batch_first=True, dropout=dropout\n",
        "        )\n",
        "        self.output_layer = torch.nn.Linear(hidden_size, 1)\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "    def forward(self, x: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
        "        \"\"\"Forward pass.\"\"\"\n",
        "        encoder_input = x[\"encoder_cont\"]\n",
        "        output, (hidden, _) = self.lstm(encoder_input)\n",
        "        prediction = self.output_layer(hidden[-1])\n",
        "        return prediction\n",
        "\n",
        "class TCNBlock(torch.nn.Module):\n",
        "    \"\"\"Temporal Convolutional Network block.\"\"\"\n",
        "\n",
        "    def __init__(self, n_inputs: int, n_outputs: int, kernel_size: int,\n",
        "                 stride: int, dilation: int, padding: int, dropout: float = 0.2):\n",
        "        \"\"\"Initialize TCN block.\"\"\"\n",
        "        super().__init__()\n",
        "        self.conv1 = torch.nn.Conv1d(\n",
        "            n_inputs, n_outputs, kernel_size,\n",
        "            stride=stride, padding=padding, dilation=dilation\n",
        "        )\n",
        "        self.chomp1 = torch.nn.functional.pad  # Remove future timesteps\n",
        "        self.relu1 = torch.nn.ReLU()\n",
        "        self.dropout1 = torch.nn.Dropout(dropout)\n",
        "\n",
        "        self.net = torch.nn.Sequential(\n",
        "            self.conv1,\n",
        "            self.relu1,\n",
        "            self.dropout1\n",
        "        )\n",
        "\n",
        "        self.downsample = torch.nn.Conv1d(n_inputs, n_outputs, 1) \\\n",
        "            if n_inputs != n_outputs else None\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"Initialize network weights.\"\"\"\n",
        "        self.conv1.weight.data.normal_(0, 0.01)\n",
        "        if self.downsample is not None:\n",
        "            self.downsample.weight.data.normal_(0, 0.01)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Forward pass.\"\"\"\n",
        "        out = self.net(x)\n",
        "        res = x if self.downsample is None else self.downsample(x)\n",
        "        return self.relu(out + res)\n",
        "\n",
        "class TCNForecaster(pl.LightningModule):\n",
        "    \"\"\"Temporal Convolutional Network Forecaster.\"\"\"\n",
        "\n",
        "    def __init__(self, input_size: int, num_channels: List[int],\n",
        "                 kernel_size: int = 2, dropout: float = 0.2, **kwargs):\n",
        "        \"\"\"Initialize TCN model.\"\"\"\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        num_levels = len(num_channels)\n",
        "        for i in range(num_levels):\n",
        "            dilation_size = 2 ** i\n",
        "            in_channels = input_size if i == 0 else num_channels[i-1]\n",
        "            out_channels = num_channels[i]\n",
        "            layers += [TCNBlock(\n",
        "                in_channels, out_channels, kernel_size, stride=1,\n",
        "                dilation=dilation_size,\n",
        "                padding=(kernel_size-1) * dilation_size,\n",
        "                dropout=dropout\n",
        "            )]\n",
        "\n",
        "        self.network = torch.nn.Sequential(*layers)\n",
        "        self.output_layer = torch.nn.Linear(num_channels[-1], 1)\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "    def forward(self, x: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
        "        \"\"\"Forward pass.\"\"\"\n",
        "        encoder_input = x[\"encoder_cont\"].permute(0, 2, 1)\n",
        "        output = self.network(encoder_input)\n",
        "        output = output[:, :, -1]  # Take last time step\n",
        "        prediction = self.output_layer(output)\n",
        "        return prediction\n",
        "\n",
        "class ProphetLikeModel(pl.LightningModule):\n",
        "    \"\"\"Prophet-like model capturing trend and seasonality.\"\"\"\n",
        "\n",
        "    def __init__(self, seasonality: int, **kwargs):\n",
        "        \"\"\"Initialize Prophet-like model.\"\"\"\n",
        "        super().__init__()\n",
        "        self.trend = torch.nn.Linear(1, 1)\n",
        "        self.seasonality = torch.nn.Linear(seasonality, 1)\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "    def forward(self, x: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
        "        \"\"\"Forward pass.\"\"\"\n",
        "        time = x[\"encoder_cont\"][:, :, 0].unsqueeze(-1)\n",
        "        trend = self.trend(time)\n",
        "\n",
        "        seasonal_features = x[\"encoder_cont\"][:, :, 1:self.hparams.seasonality+1]\n",
        "        seasonality = self.seasonality(seasonal_features)\n",
        "\n",
        "        prediction = trend + seasonality\n",
        "        return prediction[:, -1, :]\n",
        "\n",
        "class BaseWrapper:\n",
        "    \"\"\"Base wrapper for all models.\"\"\"\n",
        "\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        \"\"\"Initialize wrapper.\"\"\"\n",
        "        self.config = config\n",
        "        self.logger = logging.getLogger(f\"{self.__class__.__name__}\")\n",
        "        self.validator = DataValidator()\n",
        "        self.checkpointer = ModelCheckpointer(\"checkpoints\")\n",
        "        self.model = None\n",
        "        self.training_metrics = []\n",
        "\n",
        "    def prepare_data(self, df: pd.DataFrame, time_idx: str,\n",
        "                    target: str, group_ids: List[str]) -> None:\n",
        "        \"\"\"Prepare data for training.\"\"\"\n",
        "        # Validate data\n",
        "        self.validator.validate_dataframe(df, [time_idx, target] + group_ids)\n",
        "\n",
        "        # Create TimeSeriesDataSet\n",
        "        self.training = TimeSeriesDataSet(\n",
        "            df[df[time_idx] <= df[time_idx].max() - self.config.max_prediction_length],\n",
        "            time_idx=time_idx,\n",
        "            target=target,\n",
        "            group_ids=group_ids,\n",
        "            max_encoder_length=self.config.max_encoder_length,\n",
        "            max_prediction_length=self.config.max_prediction_length,\n",
        "            static_categoricals=group_ids,\n",
        "            time_varying_known_reals=[time_idx],\n",
        "            time_varying_unknown_reals=[target],\n",
        "            target_normalizer=None,\n",
        "            allow_missings=True,\n",
        "        )\n",
        "\n",
        "        self.validation = TimeSeriesDataSet.from_dataset(\n",
        "            self.training,\n",
        "            df[df[time_idx] > df[time_idx].max() - self.config.max_prediction_length],\n",
        "            min_prediction_idx=df[time_idx].max() - self.config.max_prediction_length + 1\n",
        "        )\n",
        "\n",
        "        self.train_dataloader = DataLoader(\n",
        "            self.training, batch_size=self.config.batch_size, shuffle=True\n",
        "        )\n",
        "        self.val_dataloader = DataLoader(\n",
        "            self.validation, batch_size=self.config.batch_size\n",
        "        )\n",
        "\n",
        "    def fit(self) -> None:\n",
        "        \"\"\"Train the model.\"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model not initialized\")\n",
        "\n",
        "        trainer = pl.Trainer(\n",
        "            max_epochs=self.config.max_epochs,\n",
        "            callbacks=[\n",
        "                EarlyStopping(\n",
        "                    monitor=\"val_loss\",\n",
        "                    min_delta=1e-4,\n",
        "                    patience=5,\n",
        "                    verbose=False,\n",
        "                    mode=\"min\"\n",
        "                )\n",
        "            ],\n",
        "            gradient_clip_val=0.1,\n",
        "        )\n",
        "\n",
        "        trainer.fit(\n",
        "            self.model,\n",
        "            train_dataloaders=self.train_dataloader,\n",
        "            val_dataloaders=self.val_dataloader\n",
        "        )\n",
        "\n",
        "    def predict(self, df: pd.DataFrame) -> np.ndarray:\n",
        "        \"\"\"Make predictions.\"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model not trained\")\n",
        "\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            predictions = self.model(\n",
        "                self.validation.to_dataloader(df, batch_size=self.config.batch_size)\n",
        "            )\n",
        "        return predictions.numpy()\n",
        "\n",
        "def generate_sample_data(n_assets: int = 5, n_days: int = 100,\n",
        "                        seed: int = 42) -> pd.DataFrame:\n",
        "    \"\"\"Generate sample financial data for testing.\n",
        "\n",
        "    Args:\n",
        "        n_assets: Number of assets\n",
        "        n_days: Number of days\n",
        "        seed: Random seed\n",
        "\n",
        "    Returns:\n",
        "        DataFrame containing sample data\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    dates = [datetime.today() - timedelta(days=x) for x in range(n_days)]\n",
        "    dates.reverse()\n",
        "\n",
        "    data = {\n",
        "        'time': dates * n_assets,\n",
        "        'group_id': np"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "VHXn706ZsGPW",
        "outputId": "d2f40578-18eb-497e-df00-8edef42cab89"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (<ipython-input-1-92dd88ec71fe>, line 423)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-92dd88ec71fe>\"\u001b[0;36m, line \u001b[0;32m423\u001b[0m\n\u001b[0;31m    'group_id': np\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import yaml\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, List, Dict, Any\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    \"\"\"Base configuration for all models.\"\"\"\n",
        "    model_name: str\n",
        "    max_epochs: int = 30\n",
        "    batch_size: int = 64\n",
        "    learning_rate: float = 0.03\n",
        "    hidden_size: int = 16\n",
        "    dropout: float = 0.1\n",
        "    max_encoder_length: int = 30\n",
        "    max_prediction_length: int = 7\n",
        "\n",
        "    @classmethod\n",
        "    def from_yaml(cls, yaml_path: str) -> 'ModelConfig':\n",
        "        \"\"\"Load configuration from YAML file.\"\"\"\n",
        "        with open(yaml_path, 'r') as f:\n",
        "            config_dict = yaml.safe_load(f)\n",
        "        return cls(**config_dict)\n",
        "\n",
        "    def to_yaml(self, yaml_path: str) -> None:\n",
        "        \"\"\"Save configuration to YAML file.\"\"\"\n",
        "        with open(yaml_path, 'w') as f:\n",
        "            yaml.dump(self.__dict__, f)\n",
        "\n",
        "class DataValidator:\n",
        "    \"\"\"Validates input data quality.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def validate_dataframe(df: pd.DataFrame, required_columns: List[str]) -> bool:\n",
        "        \"\"\"\n",
        "        Validate DataFrame structure and content.\n",
        "\n",
        "        Args:\n",
        "            df: Input DataFrame\n",
        "            required_columns: List of required column names\n",
        "\n",
        "        Returns:\n",
        "            bool: True if validation passes\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If validation fails\n",
        "        \"\"\"\n",
        "        # Check for required columns\n",
        "        missing_cols = set(required_columns) - set(df.columns)\n",
        "        if missing_cols:\n",
        "            raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
        "\n",
        "        # Check for null values\n",
        "        null_cols = df.columns[df.isnull().any()].tolist()\n",
        "        if null_cols:\n",
        "            raise ValueError(f\"Null values found in columns: {null_cols}\")\n",
        "\n",
        "        # Check for infinite values\n",
        "        inf_cols = df.columns[np.isinf(df.select_dtypes(include=np.number)).any()].tolist()\n",
        "        if inf_cols:\n",
        "            raise ValueError(f\"Infinite values found in columns: {inf_cols}\")\n",
        "\n",
        "        return True\n",
        "\n",
        "class ModelCheckpointer:\n",
        "    \"\"\"Handles model checkpointing and recovery.\"\"\"\n",
        "\n",
        "    def __init__(self, checkpoint_dir: str):\n",
        "        self.checkpoint_dir = Path(checkpoint_dir)\n",
        "        self.checkpoint_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    def save_checkpoint(self, model: torch.nn.Module, epoch: int,\n",
        "                       optimizer: torch.optim.Optimizer, loss: float) -> str:\n",
        "        \"\"\"Save model checkpoint.\"\"\"\n",
        "        checkpoint_path = self.checkpoint_dir / f\"checkpoint_epoch_{epoch}.pt\"\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "        }, checkpoint_path)\n",
        "        return str(checkpoint_path)\n",
        "\n",
        "    def load_checkpoint(self, model: torch.nn.Module,\n",
        "                       optimizer: torch.optim.Optimizer,\n",
        "                       checkpoint_path: str) -> tuple:\n",
        "        \"\"\"Load model checkpoint.\"\"\"\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        return checkpoint['epoch'], checkpoint['loss']\n",
        "\n",
        "class BaseWrapper:\n",
        "    \"\"\"Enhanced base wrapper with improved functionality.\"\"\"\n",
        "\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        self.config = config\n",
        "        self.logger = logging.getLogger(f\"{self.__class__.__name__}\")\n",
        "        self.validator = DataValidator()\n",
        "        self.checkpointer = ModelCheckpointer(\"checkpoints\")\n",
        "        self.model = None\n",
        "        self.training_metrics = []\n",
        "\n",
        "    def log_metrics(self, metrics: Dict[str, Any]) -> None:\n",
        "        \"\"\"Log training metrics.\"\"\"\n",
        "        self.training_metrics.append(metrics)\n",
        "        self.logger.info(f\"Metrics: {metrics}\")\n",
        "\n",
        "    def save_metrics(self, path: str) -> None:\n",
        "        \"\"\"Save training metrics to file.\"\"\"\n",
        "        pd.DataFrame(self.training_metrics).to_csv(path, index=False)"
      ],
      "metadata": {
        "id": "5Gv7_lJ4s_CY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def generate_sample_data(n_assets: int = 5, n_days: int = 100, seed: int = 42) -> pd.DataFrame:\n",
        "    \"\"\"Generate sample financial data for testing.\"\"\"\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Create dates\n",
        "    dates = [datetime.today() - timedelta(days=x) for x in range(n_days)]\n",
        "    dates.reverse()\n",
        "\n",
        "    # Generate data\n",
        "    data = {\n",
        "        'time': dates * n_assets,\n",
        "        'group_id': np.repeat(range(n_assets), n_days),\n",
        "        'returns': np.random.normal(0, 0.02, n_assets * n_days),\n",
        "        'volume': np.random.lognormal(0, 1, n_assets * n_days),\n",
        "        'price': np.random.lognormal(4, 0.1, n_assets * n_days),\n",
        "        'volatility': np.random.gamma(2, 0.1, n_assets * n_days)\n",
        "    }\n",
        "\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Example 1: AutoRegressive Model\n",
        "def run_autoregressive_example():\n",
        "    # Generate data\n",
        "    df = generate_sample_data()\n",
        "\n",
        "    # Configure model\n",
        "    config = ModelConfig(\n",
        "        model_name='AutoRegressive',\n",
        "        max_epochs=20,\n",
        "        hidden_size=16\n",
        "    )\n",
        "\n",
        "    # Initialize and train\n",
        "    model = AutoRegressiveModel(\n",
        "        input_size=len(df.columns) - 2,  # Exclude time and group_id\n",
        "        output_size=1\n",
        "    )\n",
        "\n",
        "    wrapper = BaseWrapper(config)\n",
        "    wrapper.model = model\n",
        "\n",
        "    # Train and evaluate\n",
        "    wrapper.prepare_data(df, time_idx='time', target='returns', group_ids=['group_id'])\n",
        "    wrapper.fit()\n",
        "    predictions = wrapper.predict(df.tail(10))\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# Example 2: LSTM Model\n",
        "def run_lstm_example():\n",
        "    df = generate_sample_data()\n",
        "\n",
        "    config = ModelConfig(\n",
        "        model_name='LSTM',\n",
        "        max_epochs=20,\n",
        "        hidden_size=32,\n",
        "        num_layers=2\n",
        "    )\n",
        "\n",
        "    model = LSTMForecaster(\n",
        "        input_size=len(df.columns) - 2,\n",
        "        hidden_size=config.hidden_size,\n",
        "        num_layers=2\n",
        "    )\n",
        "\n",
        "    wrapper = BaseWrapper(config)\n",
        "    wrapper.model = model\n",
        "    wrapper.prepare_data(df, time_idx='time', target='returns', group_ids=['group_id'])\n",
        "    wrapper.fit()\n",
        "\n",
        "    return wrapper.predict(df.tail(10))\n",
        "\n",
        "# Example 3: TCN Model\n",
        "def run_tcn_example():\n",
        "    df = generate_sample_data()\n",
        "\n",
        "    config = ModelConfig(\n",
        "        model_name='TCN',\n",
        "        max_epochs=20,\n",
        "        hidden_size=32\n",
        "    )\n",
        "\n",
        "    model = TCNForecaster(\n",
        "        input_size=len(df.columns) - 2,\n",
        "        num_channels=[32, 32, 32],\n",
        "        kernel_size=3\n",
        "    )\n",
        "\n",
        "    wrapper = BaseWrapper(config)\n",
        "    wrapper.model = model\n",
        "    wrapper.prepare_data(df, time_idx='time', target='returns', group_ids=['group_id'])\n",
        "    wrapper.fit()\n",
        "\n",
        "    return wrapper.predict(df.tail(10))\n",
        "\n",
        "# Example 4: ProphetLike Model\n",
        "def run_prophet_example():\n",
        "    df = generate_sample_data()\n",
        "\n",
        "    config = ModelConfig(\n",
        "        model_name='ProphetLike',\n",
        "        max_epochs=20,\n",
        "        seasonality=10\n",
        "    )\n",
        "\n",
        "    model = ProphetLikeModel(seasonality=10)\n",
        "\n",
        "    wrapper = BaseWrapper(config)\n",
        "    wrapper.model = model\n",
        "    wrapper.prepare_data(df, time_idx='time', target='returns', group_ids=['group_id'])\n",
        "    wrapper.fit()\n",
        "\n",
        "    return wrapper.predict(df.tail(10))\n",
        "\n",
        "# Run all examples\n",
        "if __name__ == \"__main__\":\n",
        "    # Set random seeds for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "    np.random.seed(42)\n",
        "\n",
        "    models = {\n",
        "        'AutoRegressive': run_autoregressive_example,\n",
        "        'LSTM': run_lstm_example,\n",
        "        'TCN': run_tcn_example,\n",
        "        'ProphetLike': run_prophet_example\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "    for model_name, run_func in models.items():\n",
        "        try:\n",
        "            logger.info(f\"Running {model_name} example...\")\n",
        "            predictions = run_func()\n",
        "            results[model_name] = predictions\n",
        "            logger.info(f\"{model_name} completed successfully\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error running {model_name}: {str(e)}\")\n",
        "\n",
        "    # Compare results\n",
        "    comparison_df = pd.DataFrame({\n",
        "        model_name: results[model_name]['predictions'].mean()\n",
        "        for model_name in results.keys()\n",
        "    })\n",
        "\n",
        "    print(\"\\nModel Comparison:\")\n",
        "    print(comparison_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4CY_aDUtiod",
        "outputId": "dd01a447-426d-4247-a3a0-12909a8b98d1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error running AutoRegressive: name 'AutoRegressiveModel' is not defined\n",
            "ERROR:__main__:Error running LSTM: ModelConfig.__init__() got an unexpected keyword argument 'num_layers'\n",
            "ERROR:__main__:Error running TCN: name 'TCNForecaster' is not defined\n",
            "ERROR:__main__:Error running ProphetLike: ModelConfig.__init__() got an unexpected keyword argument 'seasonality'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Comparison:\n",
            "Empty DataFrame\n",
            "Columns: []\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#example configuration file"
      ],
      "metadata": {
        "id": "jfgv1Ut6trh7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# config.yaml\n",
        "model_name: LSTM\n",
        "max_epochs: 30\n",
        "batch_size: 64\n",
        "learning_rate: 0.03\n",
        "hidden_size: 32\n",
        "dropout: 0.1\n",
        "max_encoder_length: 30\n",
        "max_prediction_length: 7\n",
        "num_layers: 2  # LSTM specific\n",
        "kernel_size: 3  # TCN specific\n",
        "seasonality: 10  # ProphetLike specific"
      ],
      "metadata": {
        "id": "wuDOCuTetvME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import unittest\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "class TestModelBaseStructure(unittest.TestCase):\n",
        "    def setUp(self):\n",
        "        self.config = ModelConfig(model_name='LSTM')\n",
        "        self.sample_data = generate_sample_data(n_assets=2, n_days=50)\n",
        "\n",
        "    def test_data_validator(self):\n",
        "        validator = DataValidator()\n",
        "\n",
        "        # Test valid data\n",
        "        self.assertTrue(validator.validate_dataframe(\n",
        "            self.sample_data,\n",
        "            ['time', 'group_id', 'returns']\n",
        "        ))\n",
        "\n",
        "        # Test invalid data\n",
        "        invalid_df = self.sample_data.copy()\n",
        "        invalid_df.loc[0, 'returns'] = np.nan\n",
        "        with self.assertRaises(ValueError):\n",
        "            validator.validate_dataframe(invalid_df, ['time', 'group_id', 'returns'])\n",
        "\n",
        "    def test_model_checkpointing(self):\n",
        "        checkpointer = ModelCheckpointer(\"test_checkpoints\")\n",
        "        model = LSTMForecaster(input_size=5, hidden_size=16, num_layers=2)\n",
        "        optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "        # Test saving\n",
        "        checkpoint_path = checkpointer.save_checkpoint(model, 1, optimizer, 0.5)\n",
        "        self.assertTrue(Path(checkpoint_path).exists())\n",
        "\n",
        "        # Test loading\n",
        "        new_model = LSTMForecaster(input_size=5, hidden_size=16, num_layers=2)\n",
        "        new_optimizer = torch.optim.Adam(new_model.parameters())\n",
        "        epoch, loss = checkpointer.load_checkpoint(new_model, new_optimizer, checkpoint_path)\n",
        "        self.assertEqual(epoch, 1)\n",
        "        self.assertEqual(loss, 0.5)\n",
        "\n",
        "    def tearDown(self):\n",
        "        # Clean up test checkpoints\n",
        "        for file in Path(\"test_checkpoints\").glob(\"*.pt\"):\n",
        "            file.unlink()\n",
        "        Path(\"test_checkpoints\").rmdir()\n",
        "\n",
        "class TestModels(unittest.TestCase):\n",
        "    def setUp(self):\n",
        "        self.sample_data = generate_sample_data()\n",
        "\n",
        "    def test_autoregressive_model(self):\n",
        "        model = AutoRegressiveModel(input_size=5)\n",
        "        self.assertIsInstance(model, torch.nn.Module)\n",
        "\n",
        "        # Test forward pass\n",
        "        x = {\"encoder_cont\": torch.randn(2, 10, 5)}\n",
        "        output = model(x)\n",
        "        self.assertEqual(output.shape, (2, 1))\n",
        "\n",
        "    def test_lstm_model(self):\n",
        "        model = LSTMForecaster(input_size=5, hidden_size=16, num_layers=2)\n",
        "        self.assertIsInstance(model, torch.nn.Module)\n",
        "\n",
        "        # Test forward pass\n",
        "        x = {\"encoder_cont\": torch.randn(2, 10, 5)}\n",
        "        output = model(x)\n",
        "        self.assertEqual(output.shape, (2, 1))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    unittest.main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "OPGBA4f4ty9m",
        "outputId": "65aa293d-c46c-468f-f1f7-875fa7816d6f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "E\n",
            "======================================================================\n",
            "ERROR: /root/ (unittest.loader._FailedTest)\n",
            "----------------------------------------------------------------------\n",
            "AttributeError: module '__main__' has no attribute '/root/'\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.004s\n",
            "\n",
            "FAILED (errors=1)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "True",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNcMZ6JY+sV8SBnvmWQc6e8",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}