{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxeTKXVGelWfOV0u0TEQSY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thegallier/timeseries/blob/main/timeseries.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "V2VMbtr1zFbZ",
        "outputId": "3dc674e0-b9d4-4874-87a9-a0d873e8e9db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Combined Model with Transformer and LSTM...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Expected floating point type for target with class probabilities, got Long",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-178d1a708573>\u001b[0m in \u001b[0;36m<cell line: 375>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimary_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecondary_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimary_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecondary_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-178d1a708573>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model1, model2, train_loader, criterion, optimizer, num_epochs, device)\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_securities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1188\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1189\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m                                label_smoothing=self.label_smoothing)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3102\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3103\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3104\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected floating point type for target with class probabilities, got Long"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Parameters\n",
        "num_timesteps = 50000  # Reduced for practical purposes\n",
        "num_securities = 10\n",
        "num_features_per_security = 4\n",
        "num_classes = 3\n",
        "num_features = num_securities * num_features_per_security\n",
        "\n",
        "# Generate timestamps\n",
        "timestamps = np.arange(num_timesteps)\n",
        "\n",
        "# Generate random data for the primary dataset\n",
        "X_data = np.random.rand(num_timesteps, num_features).astype(np.float32)\n",
        "y_data = np.random.randint(0, num_classes, size=(num_timesteps, num_securities)).astype(np.int64)\n",
        "\n",
        "# Generate the second dataset\n",
        "# 3 strings and 2 floats per timestamp\n",
        "str_columns = ['str1', 'str2', 'str3']\n",
        "float_columns = ['float1', 'float2']\n",
        "second_dataset = {\n",
        "    'timestamp': timestamps,\n",
        "    'str1': np.random.choice(['A', 'B', 'C'], num_timesteps),\n",
        "    'str2': np.random.choice(['D', 'E', 'F'], num_timesteps),\n",
        "    'str3': np.random.choice(['G', 'H', 'I'], num_timesteps),\n",
        "    'float1': np.random.rand(num_timesteps),\n",
        "    'float2': np.random.rand(num_timesteps),\n",
        "}\n",
        "\n",
        "# Encode string columns\n",
        "label_encoders = {}\n",
        "for col in str_columns:\n",
        "    le = LabelEncoder()\n",
        "    second_dataset[col] = le.fit_transform(second_dataset[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# Combine all features from the second dataset\n",
        "second_X_data = np.column_stack([second_dataset[col] for col in str_columns + float_columns]).astype(np.float32)\n",
        "\n",
        "# Min-max scaling for both datasets\n",
        "scaler_X = MinMaxScaler()\n",
        "X_data = scaler_X.fit_transform(X_data)\n",
        "\n",
        "scaler_second_X = MinMaxScaler()\n",
        "second_X_data = scaler_second_X.fit_transform(second_X_data)\n",
        "\n",
        "# Add positional encoding options\n",
        "def add_positional_encoding(X, timestamps, option='shared'):\n",
        "    if option == 'shared':\n",
        "        pe = np.sin(timestamps[:, None] / 10000 ** (np.arange(X.shape[1]) / X.shape[1]))\n",
        "        X_pe = X + pe.astype(np.float32)\n",
        "    elif option == 'per_security':\n",
        "        pe_list = []\n",
        "        for i in range(num_securities):\n",
        "            pe = np.sin(timestamps[:, None] / 10000 ** (np.arange(num_features_per_security) / num_features_per_security))\n",
        "            pe_list.append(pe)\n",
        "        pe_concat = np.hstack(pe_list)\n",
        "        X_pe = X + pe_concat.astype(np.float32)\n",
        "    else:\n",
        "        X_pe = X  # No positional encoding\n",
        "    return X_pe\n",
        "\n",
        "# Apply positional encoding\n",
        "positional_encoding_option = 'shared'  # 'shared' or 'per_security'\n",
        "X_data = add_positional_encoding(X_data, timestamps, positional_encoding_option)\n",
        "\n",
        "# Function to create windows (updated to handle second dataset)\n",
        "def create_windows(X1, X2, y, window_size, horizon):\n",
        "    X1_windows = []\n",
        "    X2_windows = []\n",
        "    y_windows = []\n",
        "    for i in range(len(X1) - window_size - horizon + 1):\n",
        "        X1_windows.append(X1[i:i+window_size])\n",
        "        X2_windows.append(X2[i:i+window_size])\n",
        "        y_windows.append(y[i+window_size+horizon-1])\n",
        "    return np.array(X1_windows), np.array(X2_windows), np.array(y_windows)\n",
        "\n",
        "# Define window sizes\n",
        "window_size = 20\n",
        "horizon = 1\n",
        "\n",
        "# Create windows\n",
        "X1_windows, X2_windows, y_windows = create_windows(X_data, second_X_data, y_data, window_size, horizon)\n",
        "\n",
        "# Split into train and test sets\n",
        "train_ratio = 0.8\n",
        "train_size = int(len(X1_windows) * train_ratio)\n",
        "\n",
        "X1_train = X1_windows[:train_size]\n",
        "X2_train = X2_windows[:train_size]\n",
        "y_train = y_windows[:train_size]\n",
        "\n",
        "X1_test = X1_windows[train_size:]\n",
        "X2_test = X2_windows[train_size:]\n",
        "y_test = y_windows[train_size:]\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X1_train_tensor = torch.tensor(X1_train)\n",
        "X2_train_tensor = torch.tensor(X2_train)\n",
        "y_train_tensor = torch.tensor(y_train)\n",
        "\n",
        "X1_test_tensor = torch.tensor(X1_test)\n",
        "X2_test_tensor = torch.tensor(X2_test)\n",
        "y_test_tensor = torch.tensor(y_test)\n",
        "\n",
        "# Custom Dataset (updated for two datasets)\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, X1, X2, y):\n",
        "        self.X1 = X1.float()\n",
        "        self.X2 = X2.float()\n",
        "        self.y = y.long()\n",
        "    def __len__(self):\n",
        "        return len(self.X1)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X1[idx], self.X2[idx], self.y[idx]\n",
        "\n",
        "# DataLoaders\n",
        "batch_size = 64\n",
        "\n",
        "train_dataset = TimeSeriesDataset(X1_train_tensor, X2_train_tensor, y_train_tensor)\n",
        "test_dataset = TimeSeriesDataset(X1_test_tensor, X2_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Matrix Regression Model (Y = A * X * B)\n",
        "class MatrixRegressionModel(nn.Module):\n",
        "    def __init__(self, num_securities, num_features):\n",
        "        super(MatrixRegressionModel, self).__init__()\n",
        "        self.A = nn.Parameter(torch.randn(1, num_securities))\n",
        "        self.B = nn.Parameter(torch.randn(num_features, num_classes))\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)  # Flatten over time\n",
        "        out = self.A @ x @ self.B  # Shape: (batch_size, num_classes)\n",
        "        out = out.view(-1, num_securities, num_classes)\n",
        "        return out\n",
        "\n",
        "class LogisticRegressionModel(nn.Module):\n",
        "    def __init__(self, input_size, num_securities, num_classes):\n",
        "        super(LogisticRegressionModel, self).__init__()\n",
        "        self.num_securities = num_securities\n",
        "        self.num_classes = num_classes\n",
        "        self.linear = nn.Linear(input_size, num_securities * num_classes)\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, window_size, num_features)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        out = self.linear(x)\n",
        "        out = out.view(-1, self.num_securities, self.num_classes)\n",
        "        return out\n",
        "\n",
        "# CNN Model\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self, num_features, num_securities, num_classes):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.num_securities = num_securities\n",
        "        self.num_classes = num_classes\n",
        "        self.conv1 = nn.Conv1d(in_channels=num_features, out_channels=64, kernel_size=3)\n",
        "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc = nn.Linear(128, num_securities * num_classes)\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)  # (batch_size, num_features, window_size)\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = torch.mean(x, dim=2)  # Global average pooling\n",
        "        x = self.fc(x)\n",
        "        out = x.view(-1, self.num_securities, self.num_classes)\n",
        "        return out\n",
        "\n",
        "# LSTM Model\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_securities, num_classes):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.num_securities = num_securities\n",
        "        self.num_classes = num_classes\n",
        "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_securities * num_classes)\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = out[:, -1, :]  # Last time step\n",
        "        out = self.fc(out)\n",
        "        out = out.view(-1, self.num_securities, self.num_classes)\n",
        "        return out\n",
        "\n",
        "# Graph Neural Network Model\n",
        "class GNNModel(nn.Module):\n",
        "    def __init__(self, num_securities, num_features_per_security, num_classes):\n",
        "        super(GNNModel, self).__init__()\n",
        "        self.num_securities = num_securities\n",
        "        self.gcn_layers = nn.ModuleList([nn.Linear(num_features_per_security, 64) for _ in range(num_securities)])\n",
        "        self.attention = nn.MultiheadAttention(embed_dim=64, num_heads=4)\n",
        "        self.fc = nn.Linear(64, num_classes)\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, window_size, num_features)\n",
        "        batch_size = x.size(0)\n",
        "        x = x[:, -1, :]  # Use the last time step\n",
        "        x = x.view(batch_size, self.num_securities, -1)  # (batch_size, num_securities, num_features_per_security)\n",
        "        node_embeddings = []\n",
        "        for i in range(self.num_securities):\n",
        "            h = self.gcn_layers[i](x[:, i, :])\n",
        "            node_embeddings.append(h)\n",
        "        h = torch.stack(node_embeddings, dim=0)  # (num_securities, batch_size, 64)\n",
        "        attn_output, _ = self.attention(h, h, h)\n",
        "        attn_output = attn_output.permute(1, 0, 2)  # (batch_size, num_securities, 64)\n",
        "        out = self.fc(attn_output)  # (batch_size, num_securities, num_classes)\n",
        "        return out\n",
        "\n",
        "# Transformer Model\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, num_features, num_securities, num_classes):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embedding = nn.Linear(num_features, 128)\n",
        "        self.pos_encoder = PositionalEncoding(128)\n",
        "        encoder_layers = nn.TransformerEncoderLayer(d_model=128, nhead=8)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=2)\n",
        "        self.decoder = nn.Linear(128, num_securities * num_classes)\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), x.size(1), -1)  # (batch_size, window_size, num_features)\n",
        "        x = self.embedding(x)\n",
        "        x = self.pos_encoder(x)\n",
        "        x = x.permute(1, 0, 2)  # Transformer expects (sequence_length, batch_size, embedding_dim)\n",
        "        output = self.transformer_encoder(x)\n",
        "        output = output[-1, :, :]  # Use the last output\n",
        "        output = self.decoder(output)\n",
        "        output = output.view(-1, num_securities, num_classes)\n",
        "        return output\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        if d_model % 2 == 1:\n",
        "            # If odd, last dimension is sine\n",
        "            pe[:, 1::2] = torch.sin(position * div_term)\n",
        "        else:\n",
        "            pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(1)\n",
        "        self.register_buffer('pe', pe)\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return x\n",
        "\n",
        "# Mamba Model (Assuming Mamba refers to a model combining multiple methods)\n",
        "class MambaModel(nn.Module):\n",
        "    def __init__(self, num_features, num_securities, num_classes):\n",
        "        super(MambaModel, self).__init__()\n",
        "        # Combine CNN and LSTM\n",
        "        self.cnn = nn.Conv1d(in_channels=num_features, out_channels=64, kernel_size=3)\n",
        "        self.lstm = nn.LSTM(input_size=64, hidden_size=128, num_layers=2, batch_first=True)\n",
        "        self.fc = nn.Linear(128, num_securities * num_classes)\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)  # (batch_size, num_features, window_size)\n",
        "        x = F.relu(self.cnn(x))\n",
        "        x = x.permute(0, 2, 1)  # (batch_size, window_size, channels)\n",
        "        h0 = torch.zeros(2, x.size(0), 128).to(x.device)\n",
        "        c0 = torch.zeros(2, x.size(0), 128).to(x.device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = out[:, -1, :]\n",
        "        out = self.fc(out)\n",
        "        out = out.view(-1, num_securities, num_classes)\n",
        "        return out\n",
        "\n",
        "# Liquid Neural Network Model\n",
        "class LiquidNetModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_securities, num_classes):\n",
        "        super(LiquidNetModel, self).__init__()\n",
        "        self.rnn_cell = nn.RNNCell(input_size, hidden_size, nonlinearity='relu')\n",
        "        self.fc = nn.Linear(hidden_size, num_securities * num_classes)\n",
        "    def forward(self, x):\n",
        "        h_t = torch.zeros(x.size(0), self.rnn_cell.hidden_size).to(x.device)\n",
        "        for t in range(x.size(1)):\n",
        "            h_t = self.rnn_cell(x[:, t, :], h_t)\n",
        "        out = self.fc(h_t)\n",
        "        out = out.view(-1, num_securities, num_classes)\n",
        "        return out\n",
        "\n",
        "# Hidden Markov Model (simplified implementation)\n",
        "class HiddenMarkovModel(nn.Module):\n",
        "    def __init__(self, num_states, num_securities, num_classes):\n",
        "        super(HiddenMarkovModel, self).__init__()\n",
        "        self.num_states = num_states\n",
        "        self.start_prob = nn.Parameter(torch.randn(num_states))\n",
        "        self.transition_prob = nn.Parameter(torch.randn(num_states, num_states))\n",
        "        self.emission_prob = nn.Parameter(torch.randn(num_states, num_classes))\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        # Simplified; in practice, you'd implement the forward algorithm\n",
        "        out = torch.softmax(self.emission_prob, dim=1)\n",
        "        out = out.unsqueeze(0).repeat(batch_size, self.num_securities, 1)\n",
        "        return out\n",
        "\n",
        "# Common training loop (updated for two datasets and model combination)\n",
        "def train_model(model1, model2, train_loader, criterion, optimizer, num_epochs, device):\n",
        "    model1 = model1.to(device)\n",
        "    model2 = model2.to(device)\n",
        "    for epoch in range(num_epochs):\n",
        "        model1.train()\n",
        "        model2.train()\n",
        "        total_loss = 0\n",
        "        for X1_batch, X2_batch, y_batch in train_loader:\n",
        "            X1_batch = X1_batch.to(device)\n",
        "            X2_batch = X2_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs1 = model1(X1_batch)\n",
        "            outputs2 = model2(X2_batch)\n",
        "            # Combine outputs\n",
        "            outputs = torch.cat((outputs1, outputs2), dim=2)  # Concatenate over feature dimension\n",
        "            # Final linear layer to predict y\n",
        "            final_output = outputs.mean(dim=2)\n",
        "            loss = 0\n",
        "            for i in range(num_securities):\n",
        "                loss += criterion(final_output[:, i], y_batch[:, i])\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
        "\n",
        "# Common evaluation function\n",
        "def evaluate_model(model1, model2, test_loader, device):\n",
        "    model1 = model1.to(device)\n",
        "    model2 = model2.to(device)\n",
        "    model1.eval()\n",
        "    model2.eval()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for X1_batch, X2_batch, y_batch in test_loader:\n",
        "            X1_batch = X1_batch.to(device)\n",
        "            X2_batch = X2_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "            outputs1 = model1(X1_batch)\n",
        "            outputs2 = model2(X2_batch)\n",
        "            outputs = torch.cat((outputs1, outputs2), dim=2)\n",
        "            final_output = outputs.mean(dim=2)\n",
        "            _, predicted = torch.max(final_output.data, 2)\n",
        "            total += y_batch.numel()\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy on test set: {accuracy:.2f}%')\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = num_features\n",
        "second_input_size = second_X_data.shape[1]\n",
        "num_epochs = 5\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Instantiate models for primary and second datasets\n",
        "print(\"Training Combined Model with Transformer and LSTM...\")\n",
        "\n",
        "# Primary model\n",
        "primary_model = TransformerModel(num_features=num_features, num_securities=num_securities, num_classes=num_classes)\n",
        "# Secondary model\n",
        "secondary_model = LSTMModel(input_size=second_input_size, hidden_size=128, num_layers=2, num_securities=num_securities, num_classes=num_classes)\n",
        "\n",
        "# Combine the parameters of both models\n",
        "params = list(primary_model.parameters()) + list(secondary_model.parameters())\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params, lr=learning_rate)\n",
        "\n",
        "train_model(primary_model, secondary_model, train_loader, criterion, optimizer, num_epochs, device)\n",
        "evaluate_model(primary_model, secondary_model, test_loader, device)\n",
        "\n",
        "# Similarly, you can instantiate and train other combinations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ... [Previous code remains the same up to model definitions]\n",
        "\n",
        "# Instantiate models for primary and second datasets\n",
        "print(\"Training Combined Model with Transformer and LSTM...\")\n",
        "\n",
        "# Primary model\n",
        "primary_model = TransformerModel(num_features=num_features, num_securities=num_securities, num_classes=num_classes)\n",
        "# Secondary model\n",
        "secondary_model = LSTMModel(input_size=second_input_size, hidden_size=128, num_layers=2, num_securities=num_securities, num_classes=num_classes)\n",
        "\n",
        "# Define final linear layer\n",
        "class FinalModel(nn.Module):\n",
        "    def __init__(self, num_classes, num_securities):\n",
        "        super(FinalModel, self).__init__()\n",
        "        self.fc = nn.Linear(num_classes * 2, num_classes)\n",
        "        self.num_securities = num_securities\n",
        "    def forward(self, outputs1, outputs2):\n",
        "        # Concatenate over class dimension\n",
        "        outputs = torch.cat((outputs1, outputs2), dim=2)\n",
        "        # Pass through linear layer\n",
        "        batch_size = outputs.size(0)\n",
        "        outputs = outputs.view(-1, outputs.size(2))\n",
        "        final_output = self.fc(outputs)\n",
        "        final_output = final_output.view(batch_size, self.num_securities, -1)\n",
        "        return final_output\n",
        "\n",
        "final_model = FinalModel(num_classes=num_classes, num_securities=num_securities)\n",
        "\n",
        "# Combine the parameters of all models\n",
        "params = list(primary_model.parameters()) + list(secondary_model.parameters()) + list(final_model.parameters())\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params, lr=learning_rate)\n",
        "\n",
        "# Updated training function\n",
        "def train_model(model1, model2, final_model, train_loader, criterion, optimizer, num_epochs, device):\n",
        "    model1 = model1.to(device)\n",
        "    model2 = model2.to(device)\n",
        "    final_model = final_model.to(device)\n",
        "    for epoch in range(num_epochs):\n",
        "        model1.train()\n",
        "        model2.train()\n",
        "        final_model.train()\n",
        "        total_loss = 0\n",
        "        for X1_batch, X2_batch, y_batch in train_loader:\n",
        "            X1_batch = X1_batch.to(device)\n",
        "            X2_batch = X2_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs1 = model1(X1_batch)\n",
        "            outputs2 = model2(X2_batch)\n",
        "            final_output = final_model(outputs1, outputs2)\n",
        "            loss = 0\n",
        "            for i in range(num_securities):\n",
        "                loss += criterion(final_output[:, i, :], y_batch[:, i])\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
        "\n",
        "# Updated evaluation function\n",
        "def evaluate_model(model1, model2, final_model, test_loader, device):\n",
        "    model1 = model1.to(device)\n",
        "    model2 = model2.to(device)\n",
        "    final_model = final_model.to(device)\n",
        "    model1.eval()\n",
        "    model2.eval()\n",
        "    final_model.eval()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for X1_batch, X2_batch, y_batch in test_loader:\n",
        "            X1_batch = X1_batch.to(device)\n",
        "            X2_batch = X2_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "            outputs1 = model1(X1_batch)\n",
        "            outputs2 = model2(X2_batch)\n",
        "            final_output = final_model(outputs1, outputs2)\n",
        "            _, predicted = torch.max(final_output.data, 2)\n",
        "            total += y_batch.numel()\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy on test set: {accuracy:.2f}%')\n",
        "\n",
        "# Start training\n",
        "train_model(primary_model, secondary_model, final_model, train_loader, criterion, optimizer, num_epochs, device)\n",
        "evaluate_model(primary_model, secondary_model, final_model, test_loader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        },
        "id": "C_ZCKOPo0MRM",
        "outputId": "6836a9e1-7e1f-4cce-e6f6-1bd56385198e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Combined Model with Transformer and LSTM...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Loss: 11.0111\n",
            "Epoch [2/5], Loss: 10.9914\n",
            "Epoch [3/5], Loss: 10.9887\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-7b9d35fc1484>\u001b[0m in \u001b[0;36m<cell line: 86>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;31m# Start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimary_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecondary_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimary_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecondary_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-7b9d35fc1484>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model1, model2, final_model, train_loader, criterion, optimizer, num_epochs, device)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0moutputs1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0moutputs2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX2_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mfinal_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-178d1a708573>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Transformer expects (sequence_length, batch_size, embedding_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Use the last output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_causal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_causal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_key_padding_mask_for_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconvert_to_nested\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    748\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sa_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_causal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_causal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ff_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36m_ff_block\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    763\u001b[0m     \u001b[0;31m# feed forward block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_ff_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/dropout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1293\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"dropout probability has to be between 0 and 1, but got {p}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mamba-ssm[dev]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Q63BomR20ts",
        "outputId": "f589912b-c135-4cc5-d549-3c2747b33b50"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mamba-ssm[dev]\n",
            "  Downloading mamba_ssm-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m959.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33mWARNING: mamba-ssm 2.2.2 does not provide the extra 'dev'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from mamba-ssm[dev]) (2.4.0+cu121)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mamba-ssm[dev]) (24.1)\n",
            "Collecting ninja (from mamba-ssm[dev])\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from mamba-ssm[dev]) (0.8.0)\n",
            "Collecting triton (from mamba-ssm[dev])\n",
            "  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from mamba-ssm[dev]) (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm[dev]) (3.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm[dev]) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm[dev]) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm[dev]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm[dev]) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm[dev]) (2024.6.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm[dev]) (0.24.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm[dev]) (1.26.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm[dev]) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm[dev]) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm[dev]) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm[dev]) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm[dev]) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm[dev]) (4.66.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->mamba-ssm[dev]) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm[dev]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm[dev]) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm[dev]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm[dev]) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->mamba-ssm[dev]) (1.3.0)\n",
            "Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: mamba-ssm\n",
            "  Building wheel for mamba-ssm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mamba-ssm: filename=mamba_ssm-2.2.2-cp310-cp310-linux_x86_64.whl size=323988104 sha256=6b082468a6abb6f6bc50c99263f17c6c7f5a2e8f6b275ed7998b81fb25279229\n",
            "  Stored in directory: /root/.cache/pip/wheels/57/7c/90/9f963468ecc3791e36e388f9e7b4a4e1e3f90fbb340055aa4d\n",
            "Successfully built mamba-ssm\n",
            "Installing collected packages: ninja, triton, mamba-ssm\n",
            "Successfully installed mamba-ssm-2.2.2 ninja-1.11.1.1 triton-3.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class HiddenMarkovModel(nn.Module):\n",
        "    def __init__(self, num_states, num_securities, num_classes, num_features_per_security):\n",
        "        super(HiddenMarkovModel, self).__init__()\n",
        "        self.num_states = num_states\n",
        "        self.num_securities = num_securities\n",
        "        self.num_classes = num_classes\n",
        "        self.num_features_per_security = num_features_per_security\n",
        "\n",
        "        # Define parameters for start, transition, and emission probabilities\n",
        "        self.start_logits = nn.Parameter(torch.randn(num_securities, num_states))\n",
        "        self.trans_logits = nn.Parameter(torch.randn(num_securities, num_states, num_states))\n",
        "        self.emission_logits = nn.Parameter(torch.randn(num_securities, num_states, num_classes))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, window_size, num_features)\n",
        "        batch_size, window_size, num_features = x.size()\n",
        "        device = x.device\n",
        "\n",
        "        # Reshape x to separate securities\n",
        "        x = x.view(batch_size, window_size, self.num_securities, self.num_features_per_security)\n",
        "        # For simplicity, we'll use the last time step\n",
        "        x_last = x[:, -1, :, :]  # Shape: (batch_size, num_securities, num_features_per_security)\n",
        "\n",
        "        # We'll compute the logits for each class per security\n",
        "        # For this simplified HMM, we'll combine start and emission logits to produce class logits\n",
        "        outputs = []\n",
        "        for s in range(self.num_securities):\n",
        "            # Compute the log probabilities for each class\n",
        "            # We use log-sum-exp over the states to compute the logits for each class\n",
        "            # start_logits[s]: (num_states,)\n",
        "            # emission_logits[s]: (num_states, num_classes)\n",
        "            start_log_probs = self.start_logits[s]  # (num_states,)\n",
        "            emission_log_probs = self.emission_logits[s]  # (num_states, num_classes)\n",
        "\n",
        "            # Compute logits for classes\n",
        "            logits = torch.logsumexp(start_log_probs.unsqueeze(1) + emission_log_probs, dim=0)  # (num_classes,)\n",
        "            outputs.append(logits.unsqueeze(0))  # (1, num_classes)\n",
        "\n",
        "        # Stack outputs over securities and expand to batch size\n",
        "        outputs = torch.cat(outputs, dim=0)  # (num_securities, num_classes)\n",
        "        outputs = outputs.unsqueeze(0).expand(batch_size, -1, -1)  # (batch_size, num_securities, num_classes)\n",
        "\n",
        "        return outputs  # Shape: (batch_size, num_securities, num_classes)\n",
        "\n",
        "\n",
        "# Adjusted training loop for HMM with continuous emissions\n",
        "def train_hmm_model(model, train_loader, criterion, optimizer, num_epochs, device):\n",
        "    model = model.to(device)\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for _, _, y_batch in train_loader:\n",
        "            y_batch = y_batch.to(device)  # Shape: (batch_size, num_securities)\n",
        "            y_batch = y_batch.unsqueeze(1).expand(-1, window_size, -1)  # (batch_size, window_size, num_securities)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(y_batch)  # outputs shape: (batch_size, window_size, num_securities)\n",
        "            loss = criterion(outputs.view(-1), torch.zeros_like(outputs.view(-1)))  # Dummy target\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
        "# Mamba Model Integration\n",
        "# Since you will install the Mamba package, we'll import and use it directly\n",
        "try:\n",
        "    from mamba_ssm.modules.mamba2 import Mamba2\n",
        "except ImportError:\n",
        "    raise ImportError(\"Please install the mamba_ssm package to use the MambaModel.\")\n",
        "\n",
        "class MambaModel(nn.Module):\n",
        "    def __init__(self, num_features, num_securities, num_classes):\n",
        "        super(MambaModel, self).__init__()\n",
        "        self.num_securities = num_securities\n",
        "        self.num_classes = num_classes\n",
        "        self.mamba = Mamba2(\n",
        "            d_model=num_features,\n",
        "            d_state=64,\n",
        "            d_conv=4,\n",
        "            expand=2,\n",
        "        )\n",
        "\n",
        "        # Final linear layer to map to the desired output size\n",
        "        self.fc = nn.Linear(num_features, num_securities * num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, window_size, num_features)\n",
        "        batch_size = x.size(0)\n",
        "        x = x.view(batch_size, -1, x.size(-1))  # Ensure proper shape\n",
        "        y = self.mamba(x)\n",
        "        y = self.fc(y[:, -1, :])  # Use the last time step\n",
        "        y = y.view(batch_size, self.num_securities, self.num_classes)\n",
        "        return y\n",
        "\n"
      ],
      "metadata": {
        "id": "DG5yBaKs2x73"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_single_model(model, train_loader, criterion, optimizer, num_epochs, device):\n",
        "    model = model.to(device)\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for X_batch, _, y_batch in train_loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X_batch)\n",
        "            loss = 0\n",
        "            for i in range(num_securities):\n",
        "                loss += criterion(outputs[:, i, :], y_batch[:, i])\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
        "\n",
        "def evaluate_single_model(model, test_loader, device):\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, _, y_batch in test_loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "            outputs = model(X_batch)\n",
        "            _, predicted = torch.max(outputs.data, 2)\n",
        "            total += y_batch.numel()\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy on test set: {accuracy:.2f}%')"
      ],
      "metadata": {
        "id": "fZgoErtS708b"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate and train Hidden Markov Model\n",
        "print(\"\\nTraining Hidden Markov Model...\")\n",
        "num_states = 5  # Number of hidden states\n",
        "\n",
        "hmm_model = HiddenMarkovModel(\n",
        "    num_states=num_states,\n",
        "    num_securities=num_securities,\n",
        "    num_classes=num_classes,\n",
        "    num_features_per_security=num_features//num_securities,\n",
        ")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(hmm_model.parameters(), lr=learning_rate)\n",
        "\n",
        "train_single_model(hmm_model, train_loader, criterion, optimizer, num_epochs, device)\n",
        "evaluate_single_model(hmm_model, test_loader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJU9C_j97QJN",
        "outputId": "c828d9d5-f237-4cbb-f166-7a8314da7b2b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Hidden Markov Model...\n",
            "Epoch [1/5], Loss: 11.9516\n",
            "Epoch [2/5], Loss: 11.0472\n",
            "Epoch [3/5], Loss: 10.9873\n",
            "Epoch [4/5], Loss: 10.9865\n",
            "Epoch [5/5], Loss: 10.9865\n",
            "Accuracy on test set: 33.44%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModelData2(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_securities, num_classes):\n",
        "        super(LSTMModelData2, self).__init__()\n",
        "        self.num_securities = num_securities\n",
        "        self.num_classes = num_classes\n",
        "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_securities * num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        h0 = torch.zeros(self.lstm.num_layers, batch_size, self.lstm.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.lstm.num_layers, batch_size, self.lstm.hidden_size).to(x.device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.fc(out[:, -1, :])  # Use the last time step\n",
        "        out = out.view(batch_size, self.num_securities, self.num_classes)\n",
        "        return out"
      ],
      "metadata": {
        "id": "8P0vjHUM9YBv"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, X1, X2, y):\n",
        "        self.X1 = X1.float()\n",
        "        self.X2 = X2.float()\n",
        "        self.y = y.long()\n",
        "    def __len__(self):\n",
        "        return len(self.X1)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X1[idx], self.X2[idx], self.y[idx]\n",
        "\n",
        "# DataLoaders\n",
        "batch_size = 64\n",
        "\n",
        "train_dataset = TimeSeriesDataset(X1_train_tensor, X2_train_tensor, y_train_tensor)\n",
        "test_dataset = TimeSeriesDataset(X1_test_tensor, X2_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "7BEaWLS58CZO"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CombinedModel(nn.Module):\n",
        "    def __init__(self, model1, model2):\n",
        "        super(CombinedModel, self).__init__()\n",
        "        self.model1 = model1  # HMM for data1\n",
        "        self.model2 = model2  # LSTM for data2\n",
        "        self.num_securities = model1.num_securities\n",
        "        self.num_classes = model1.num_classes\n",
        "        # Final linear layer to combine the outputs\n",
        "        self.fc = nn.Linear(self.num_classes * 2, self.num_classes)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        # x1: Input for data1 (HMM)\n",
        "        # x2: Input for data2 (LSTM)\n",
        "        outputs1 = self.model1(x1)  # (batch_size, num_securities, num_classes)\n",
        "        outputs2 = self.model2(x2)  # (batch_size, num_securities, num_classes)\n",
        "        # Concatenate the outputs over the class dimension\n",
        "        combined_outputs = torch.cat((outputs1, outputs2), dim=2)  # (batch_size, num_securities, num_classes * 2)\n",
        "        # Pass through the final linear layer\n",
        "        final_outputs = self.fc(combined_outputs)  # (batch_size, num_securities, num_classes)\n",
        "        return final_outputs"
      ],
      "metadata": {
        "id": "VVNbSMbG9gwx"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_combined_model(model, train_loader, criterion, optimizer, num_epochs, device):\n",
        "    model = model.to(device)\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for X1_batch, X2_batch, y_batch in train_loader:\n",
        "            X1_batch = X1_batch.to(device)\n",
        "            X2_batch = X2_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X1_batch, X2_batch)\n",
        "            loss = 0\n",
        "            for i in range(model.num_securities):\n",
        "                loss += criterion(outputs[:, i, :], y_batch[:, i])\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
        "def evaluate_combined_model(model, test_loader, device):\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for X1_batch, X2_batch, y_batch in test_loader:\n",
        "            X1_batch = X1_batch.to(device)\n",
        "            X2_batch = X2_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "            outputs = model(X1_batch, X2_batch)\n",
        "            _, predicted = torch.max(outputs.data, 2)\n",
        "            total += y_batch.numel()\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy on test set: {accuracy:.2f}%')"
      ],
      "metadata": {
        "id": "gV51uE6i9sA5"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_states = 5  # Number of hidden states\n",
        "hmm_model = HiddenMarkovModel(\n",
        "    num_states=num_states,\n",
        "    num_securities=num_securities,\n",
        "    num_classes=num_classes,\n",
        "    num_features_per_security=num_features_per_security,\n",
        ")\n",
        "\n",
        "# LSTM for data2\n",
        "input_size_data2 = X2_train_tensor.size(2)  # Number of features for data2\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "lstm_model_data2 = LSTMModelData2(\n",
        "    input_size=input_size_data2,\n",
        "    hidden_size=hidden_size,\n",
        "    num_layers=num_layers,\n",
        "    num_securities=num_securities,\n",
        "    num_classes=num_classes,\n",
        ")\n",
        "\n",
        "# Combined model\n",
        "combined_model = CombinedModel(hmm_model, lstm_model_data2)\n",
        "\n",
        "# Combine parameters for optimization\n",
        "params = list(combined_model.parameters())\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params, lr=learning_rate)\n",
        "\n",
        "# Train the combined model\n",
        "train_combined_model(combined_model, train_loader, criterion, optimizer, num_epochs, device)\n",
        "evaluate_combined_model(combined_model, test_loader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSu4WIrD53xB",
        "outputId": "f7a9a29c-6884-4323-edd3-ca67ff515986"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test set: 33.37%\n"
          ]
        }
      ]
    }
  ]
}