{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwUdHAw0mAbPAt6MrPT7Ig",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thegallier/timeseries/blob/main/controlled_rough_paths.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvXKFX-PT5Rt",
        "outputId": "ed592ecd-928c-49a3-b121-2266afa40a73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Remainder term (R_{s,t}): tensor([[ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [-5.9605e-08],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [-2.9802e-08],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [ 1.4901e-08],\n",
            "        [-5.9605e-08],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [-2.9802e-08],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [-2.9802e-08],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [-9.3132e-09],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [-1.4901e-08],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [ 2.9802e-08],\n",
            "        [ 3.7253e-09],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [ 2.9802e-08],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [ 2.9802e-08],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [ 5.9605e-08],\n",
            "        [-1.4901e-08],\n",
            "        [ 0.0000e+00],\n",
            "        [ 7.4506e-09],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [-5.9605e-08],\n",
            "        [ 2.9802e-08],\n",
            "        [ 0.0000e+00],\n",
            "        [ 1.4901e-08],\n",
            "        [-5.9605e-08],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [-1.4901e-08],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [ 0.0000e+00],\n",
            "        [-1.4901e-08]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "class ControlledRoughPath:\n",
        "    def __init__(self, driving_path):\n",
        "        \"\"\"\n",
        "        Initialize a controlled rough path with a driving rough path.\n",
        "        :param driving_path: Tensor representing the driving path (n_samples, d_features).\n",
        "        \"\"\"\n",
        "        self.driving_path = driving_path\n",
        "        self.gubinelli_derivative = None\n",
        "\n",
        "    def compute_gubinelli_derivative(self, controlled_path):\n",
        "        \"\"\"\n",
        "        Compute the Gubinelli derivative of the controlled path with respect to the driving path.\n",
        "        :param controlled_path: Tensor representing the controlled path (n_samples, d_features).\n",
        "        :return: Gubinelli derivative as a tensor.\n",
        "        \"\"\"\n",
        "        delta_x = self.driving_path[1:] - self.driving_path[:-1]\n",
        "        delta_y = controlled_path[1:] - controlled_path[:-1]\n",
        "        self.gubinelli_derivative = delta_y / delta_x  # Approximation\n",
        "        return self.gubinelli_derivative\n",
        "\n",
        "    def control_decomposition(self, controlled_path):\n",
        "        \"\"\"\n",
        "        Perform the decomposition Y(t) = Y(s) + G(s)(X(t) - X(s)) + R_{s,t}.\n",
        "        :param controlled_path: Tensor representing the controlled path.\n",
        "        :return: Remainder term R_{s,t}.\n",
        "        \"\"\"\n",
        "        g_derivative = self.compute_gubinelli_derivative(controlled_path)\n",
        "        approx_y = controlled_path[:-1] + g_derivative * (self.driving_path[1:] - self.driving_path[:-1])\n",
        "        remainder = controlled_path[1:] - approx_y\n",
        "        return remainder\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Driving path (e.g., a Brownian motion approximation)\n",
        "    driving_path = torch.cumsum(torch.randn(100, 1), dim=0)\n",
        "\n",
        "    # Controlled path (e.g., evolving in response to the driving path)\n",
        "    controlled_path = torch.sin(driving_path)\n",
        "\n",
        "    # Controlled rough path analysis\n",
        "    crp = ControlledRoughPath(driving_path)\n",
        "    remainder = crp.control_decomposition(controlled_path)\n",
        "\n",
        "    print(\"Remainder term (R_{s,t}):\", remainder)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TnAYYcPdWWPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class ControlledRoughPath:\n",
        "    def __init__(self, driving_path):\n",
        "        \"\"\"\n",
        "        Initialize a controlled rough path with a driving rough path.\n",
        "        :param driving_path: Tensor representing the driving path (seq_len, d_features).\n",
        "        \"\"\"\n",
        "        self.driving_path = driving_path\n",
        "\n",
        "    def compute_gubinelli_derivative(self, controlled_path):\n",
        "        \"\"\"\n",
        "        Compute the Gubinelli derivative of the controlled path with respect to the driving path.\n",
        "        :param controlled_path: Tensor representing the controlled path (seq_len, d_features).\n",
        "        :return: Gubinelli derivative as a tensor.\n",
        "        \"\"\"\n",
        "        # Compute differences along the time axis\n",
        "        delta_x = self.driving_path[1:, :] - self.driving_path[:-1, :]\n",
        "        delta_y = controlled_path[1:, :] - controlled_path[:-1, :]\n",
        "\n",
        "        # Ensure shape compatibility\n",
        "        if delta_x.shape != delta_y.shape:\n",
        "            raise ValueError(f\"Shape mismatch: delta_x {delta_x.shape}, delta_y {delta_y.shape}\")\n",
        "\n",
        "        # Compute Gubinelli derivative\n",
        "        gubinelli_derivative = delta_y / (delta_x + 1e-8)  # Avoid division by zero\n",
        "        return gubinelli_derivative\n",
        "\n",
        "    def control_decomposition(self, controlled_path):\n",
        "        \"\"\"\n",
        "        Perform the decomposition Y(t) = Y(s) + G(s)(X(t) - X(s)) + R_{s,t}.\n",
        "        :param controlled_path: Tensor representing the controlled path.\n",
        "        :return: Tuple of Gubinelli derivative and remainder term.\n",
        "        \"\"\"\n",
        "        g_derivative = self.compute_gubinelli_derivative(controlled_path)\n",
        "        approx_y = controlled_path[:-1, :] + g_derivative * (self.driving_path[1:, :] - self.driving_path[:-1, :])\n",
        "        remainder = controlled_path[1:, :] - approx_y\n",
        "        return g_derivative, remainder\n",
        "\n",
        "# Neural network for learning\n",
        "class CRPNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(CRPNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Generate synthetic data\n",
        "def generate_synthetic_data(seq_len=50):\n",
        "    \"\"\"\n",
        "    Generate synthetic data for a driving path, controlled path, and target function.\n",
        "    \"\"\"\n",
        "    driving_path = torch.cumsum(torch.randn(seq_len, 1), dim=0)  # Driving path\n",
        "    controlled_path = torch.sin(driving_path)  # Controlled path\n",
        "    target_function = torch.mean(controlled_path) + 0.1 * torch.sum(driving_path)  # Target\n",
        "    return driving_path, controlled_path, target_function\n",
        "\n",
        "# Main function\n",
        "if __name__ == \"__main__\":\n",
        "    # Generate data\n",
        "    seq_len = 50\n",
        "    n_samples = 500\n",
        "    driving_paths, controlled_paths, targets = [], [], []\n",
        "    for _ in range(n_samples):\n",
        "        dp, cp, tgt = generate_synthetic_data(seq_len)\n",
        "        driving_paths.append(dp)\n",
        "        controlled_paths.append(cp)\n",
        "        targets.append(tgt)\n",
        "    driving_paths = torch.stack(driving_paths)  # Shape: (n_samples, seq_len, 1)\n",
        "    controlled_paths = torch.stack(controlled_paths)  # Shape: (n_samples, seq_len, 1)\n",
        "    targets = torch.tensor(targets).float()  # Shape: (n_samples,)\n",
        "\n",
        "    # Prepare training data\n",
        "    gubinelli_derivatives, remainders = [], []\n",
        "    for i in range(n_samples):\n",
        "        crp = ControlledRoughPath(driving_paths[i])\n",
        "        gd, r = crp.control_decomposition(controlled_paths[i])\n",
        "        gubinelli_derivatives.append(gd)\n",
        "        remainders.append(r)\n",
        "    gubinelli_derivatives = torch.stack(gubinelli_derivatives)  # Shape: (n_samples, seq_len-1, d_features)\n",
        "    remainders = torch.stack(remainders)  # Shape: (n_samples, seq_len-1, d_features)\n",
        "\n",
        "    # Flatten features\n",
        "    features = torch.cat([gubinelli_derivatives, remainders], dim=2).view(n_samples, -1)  # Flatten per sample\n",
        "\n",
        "    # Train-test split\n",
        "    train_size = int(0.8 * n_samples)\n",
        "    train_features, test_features = features[:train_size], features[train_size:]\n",
        "    train_targets, test_targets = targets[:train_size], targets[train_size:]\n",
        "\n",
        "    # Initialize the neural network\n",
        "    input_dim = features.shape[1]\n",
        "    hidden_dim = 64\n",
        "    output_dim = 1\n",
        "    model = CRPNet(input_dim, hidden_dim, output_dim)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Train the neural network\n",
        "    n_epochs = 1000\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(train_features)\n",
        "        loss = criterion(outputs.squeeze(), train_targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Evaluate on test data\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            test_outputs = model(test_features)\n",
        "            test_loss = criterion(test_outputs.squeeze(), test_targets).item()\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch [{epoch + 1}/{n_epochs}], Loss: {loss.item():.4f}, Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "    print(\"Training complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jklhtMCUUkkw",
        "outputId": "1640950f-a6de-43cc-969d-039d71b5c174"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/1000], Loss: 428.6774, Test Loss: 466.4459\n",
            "Epoch [20/1000], Loss: 426.3701, Test Loss: 465.2843\n",
            "Epoch [30/1000], Loss: 423.5721, Test Loss: 463.9201\n",
            "Epoch [40/1000], Loss: 420.1528, Test Loss: 462.2733\n",
            "Epoch [50/1000], Loss: 416.1228, Test Loss: 460.4987\n",
            "Epoch [60/1000], Loss: 411.5464, Test Loss: 458.5879\n",
            "Epoch [70/1000], Loss: 406.5643, Test Loss: 456.6032\n",
            "Epoch [80/1000], Loss: 401.3019, Test Loss: 454.6432\n",
            "Epoch [90/1000], Loss: 395.8738, Test Loss: 452.7639\n",
            "Epoch [100/1000], Loss: 390.3503, Test Loss: 450.8517\n",
            "Epoch [110/1000], Loss: 384.7849, Test Loss: 448.9601\n",
            "Epoch [120/1000], Loss: 379.1637, Test Loss: 447.1736\n",
            "Epoch [130/1000], Loss: 373.4743, Test Loss: 445.4374\n",
            "Epoch [140/1000], Loss: 367.7364, Test Loss: 444.1407\n",
            "Epoch [150/1000], Loss: 361.9531, Test Loss: 443.1862\n",
            "Epoch [160/1000], Loss: 356.1065, Test Loss: 442.4530\n",
            "Epoch [170/1000], Loss: 350.1543, Test Loss: 441.5591\n",
            "Epoch [180/1000], Loss: 344.0637, Test Loss: 440.5117\n",
            "Epoch [190/1000], Loss: 337.8832, Test Loss: 439.5063\n",
            "Epoch [200/1000], Loss: 331.6233, Test Loss: 438.4231\n",
            "Epoch [210/1000], Loss: 325.3391, Test Loss: 437.4791\n",
            "Epoch [220/1000], Loss: 319.0260, Test Loss: 436.5771\n",
            "Epoch [230/1000], Loss: 312.7173, Test Loss: 435.9884\n",
            "Epoch [240/1000], Loss: 306.3499, Test Loss: 435.7696\n",
            "Epoch [250/1000], Loss: 299.9666, Test Loss: 435.8196\n",
            "Epoch [260/1000], Loss: 293.6430, Test Loss: 435.7922\n",
            "Epoch [270/1000], Loss: 287.4019, Test Loss: 435.9597\n",
            "Epoch [280/1000], Loss: 281.2579, Test Loss: 436.4000\n",
            "Epoch [290/1000], Loss: 275.1829, Test Loss: 436.8754\n",
            "Epoch [300/1000], Loss: 269.2024, Test Loss: 437.6555\n",
            "Epoch [310/1000], Loss: 263.3022, Test Loss: 438.2080\n",
            "Epoch [320/1000], Loss: 257.4142, Test Loss: 438.7279\n",
            "Epoch [330/1000], Loss: 251.5764, Test Loss: 439.1849\n",
            "Epoch [340/1000], Loss: 245.8504, Test Loss: 439.5914\n",
            "Epoch [350/1000], Loss: 240.2380, Test Loss: 440.2525\n",
            "Epoch [360/1000], Loss: 234.7371, Test Loss: 440.6769\n",
            "Epoch [370/1000], Loss: 229.3680, Test Loss: 441.3742\n",
            "Epoch [380/1000], Loss: 224.1516, Test Loss: 442.4915\n",
            "Epoch [390/1000], Loss: 219.0635, Test Loss: 443.7390\n",
            "Epoch [400/1000], Loss: 214.1005, Test Loss: 444.7680\n",
            "Epoch [410/1000], Loss: 209.2085, Test Loss: 445.7518\n",
            "Epoch [420/1000], Loss: 204.3988, Test Loss: 446.7136\n",
            "Epoch [430/1000], Loss: 199.6654, Test Loss: 447.6053\n",
            "Epoch [440/1000], Loss: 195.0169, Test Loss: 448.3010\n",
            "Epoch [450/1000], Loss: 190.4169, Test Loss: 448.8382\n",
            "Epoch [460/1000], Loss: 185.9072, Test Loss: 449.8618\n",
            "Epoch [470/1000], Loss: 181.4937, Test Loss: 451.1096\n",
            "Epoch [480/1000], Loss: 177.1510, Test Loss: 452.3220\n",
            "Epoch [490/1000], Loss: 172.9088, Test Loss: 453.8875\n",
            "Epoch [500/1000], Loss: 168.7558, Test Loss: 455.8509\n",
            "Epoch [510/1000], Loss: 164.6710, Test Loss: 457.6769\n",
            "Epoch [520/1000], Loss: 160.6533, Test Loss: 459.1514\n",
            "Epoch [530/1000], Loss: 156.7222, Test Loss: 460.4682\n",
            "Epoch [540/1000], Loss: 152.8403, Test Loss: 461.8762\n",
            "Epoch [550/1000], Loss: 149.0228, Test Loss: 463.6042\n",
            "Epoch [560/1000], Loss: 145.2566, Test Loss: 465.4698\n",
            "Epoch [570/1000], Loss: 141.5527, Test Loss: 467.1097\n",
            "Epoch [580/1000], Loss: 137.9565, Test Loss: 468.7060\n",
            "Epoch [590/1000], Loss: 134.4284, Test Loss: 470.3441\n",
            "Epoch [600/1000], Loss: 130.9877, Test Loss: 471.7674\n",
            "Epoch [610/1000], Loss: 127.6157, Test Loss: 473.4181\n",
            "Epoch [620/1000], Loss: 124.3141, Test Loss: 475.0582\n",
            "Epoch [630/1000], Loss: 121.0816, Test Loss: 476.5101\n",
            "Epoch [640/1000], Loss: 117.8923, Test Loss: 477.7779\n",
            "Epoch [650/1000], Loss: 114.7461, Test Loss: 479.0000\n",
            "Epoch [660/1000], Loss: 111.6458, Test Loss: 480.1006\n",
            "Epoch [670/1000], Loss: 108.5764, Test Loss: 481.2050\n",
            "Epoch [680/1000], Loss: 105.5892, Test Loss: 482.2410\n",
            "Epoch [690/1000], Loss: 102.6385, Test Loss: 483.3533\n",
            "Epoch [700/1000], Loss: 99.6799, Test Loss: 484.7764\n",
            "Epoch [710/1000], Loss: 96.7943, Test Loss: 486.3761\n",
            "Epoch [720/1000], Loss: 93.9694, Test Loss: 487.5956\n",
            "Epoch [730/1000], Loss: 91.2288, Test Loss: 488.5789\n",
            "Epoch [740/1000], Loss: 88.5605, Test Loss: 489.8699\n",
            "Epoch [750/1000], Loss: 85.9536, Test Loss: 491.1163\n",
            "Epoch [760/1000], Loss: 83.4029, Test Loss: 492.4287\n",
            "Epoch [770/1000], Loss: 80.9246, Test Loss: 493.5952\n",
            "Epoch [780/1000], Loss: 78.5074, Test Loss: 494.9126\n",
            "Epoch [790/1000], Loss: 76.1514, Test Loss: 496.1187\n",
            "Epoch [800/1000], Loss: 73.8717, Test Loss: 497.4504\n",
            "Epoch [810/1000], Loss: 71.6422, Test Loss: 498.8513\n",
            "Epoch [820/1000], Loss: 69.4669, Test Loss: 500.2587\n",
            "Epoch [830/1000], Loss: 67.3385, Test Loss: 501.3790\n",
            "Epoch [840/1000], Loss: 65.2500, Test Loss: 502.6499\n",
            "Epoch [850/1000], Loss: 63.1985, Test Loss: 503.8287\n",
            "Epoch [860/1000], Loss: 61.2079, Test Loss: 505.3145\n",
            "Epoch [870/1000], Loss: 59.2686, Test Loss: 506.9686\n",
            "Epoch [880/1000], Loss: 57.3762, Test Loss: 508.6284\n",
            "Epoch [890/1000], Loss: 55.5432, Test Loss: 510.1004\n",
            "Epoch [900/1000], Loss: 53.7504, Test Loss: 511.5296\n",
            "Epoch [910/1000], Loss: 52.0153, Test Loss: 512.6817\n",
            "Epoch [920/1000], Loss: 50.3384, Test Loss: 514.0390\n",
            "Epoch [930/1000], Loss: 48.6856, Test Loss: 515.4611\n",
            "Epoch [940/1000], Loss: 47.0740, Test Loss: 516.9178\n",
            "Epoch [950/1000], Loss: 45.4980, Test Loss: 518.2908\n",
            "Epoch [960/1000], Loss: 43.9755, Test Loss: 519.5298\n",
            "Epoch [970/1000], Loss: 42.4989, Test Loss: 520.7755\n",
            "Epoch [980/1000], Loss: 41.0643, Test Loss: 521.8754\n",
            "Epoch [990/1000], Loss: 39.6714, Test Loss: 523.0991\n",
            "Epoch [1000/1000], Loss: 38.3295, Test Loss: 524.4326\n",
            "Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from scipy.interpolate import interp1d\n",
        "\n",
        "def generate_interpolated_test_data(train_driving_paths, seq_len=50, n_samples=100):\n",
        "    \"\"\"\n",
        "    Generate test data by interpolating between existing training samples.\n",
        "    :param train_driving_paths: Tensor of driving paths used for training (n_train_samples, seq_len, 1).\n",
        "    :param seq_len: Length of the driving path sequences.\n",
        "    :param n_samples: Number of interpolated test samples to generate.\n",
        "    :return: Interpolated driving paths, controlled paths, and corresponding targets.\n",
        "    \"\"\"\n",
        "    # Select two random training samples for interpolation\n",
        "    idx1, idx2 = np.random.choice(len(train_driving_paths), size=2, replace=False)\n",
        "    driving_path1 = train_driving_paths[idx1].squeeze().numpy()\n",
        "    driving_path2 = train_driving_paths[idx2].squeeze().numpy()\n",
        "\n",
        "    # Interpolate between the two paths\n",
        "    interpolated_driving_paths = []\n",
        "    for alpha in np.linspace(0, 1, n_samples):\n",
        "        interp_path = (1 - alpha) * driving_path1 + alpha * driving_path2\n",
        "        interpolated_driving_paths.append(interp_path)\n",
        "\n",
        "    # Convert to tensor\n",
        "    interpolated_driving_paths = torch.tensor(interpolated_driving_paths).unsqueeze(-1)  # Shape: (n_samples, seq_len, 1)\n",
        "\n",
        "    # Generate controlled paths and targets\n",
        "    interpolated_controlled_paths = torch.sin(interpolated_driving_paths)  # Controlled path\n",
        "    interpolated_targets = torch.mean(interpolated_controlled_paths, dim=1) + 0.1 * torch.sum(interpolated_driving_paths, dim=1)  # Targets\n",
        "    return interpolated_driving_paths, interpolated_controlled_paths, interpolated_targets"
      ],
      "metadata": {
        "id": "frAq8Qc4WW7A"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Generate data\n",
        "    seq_len = 50\n",
        "    n_samples = 500\n",
        "    driving_paths, controlled_paths, targets = [], [], []\n",
        "    for _ in range(n_samples):\n",
        "        dp, cp, tgt = generate_synthetic_data(seq_len)\n",
        "        driving_paths.append(dp)\n",
        "        controlled_paths.append(cp)\n",
        "        targets.append(tgt)\n",
        "    driving_paths = torch.stack(driving_paths)  # Shape: (n_samples, seq_len, 1)\n",
        "    controlled_paths = torch.stack(controlled_paths)  # Shape: (n_samples, seq_len, 1)\n",
        "    targets = torch.tensor(targets).float()  # Shape: (n_samples,)\n",
        "\n",
        "    # Prepare training data\n",
        "    gubinelli_derivatives, remainders = [], []\n",
        "    for i in range(n_samples):\n",
        "        crp = ControlledRoughPath(driving_paths[i])\n",
        "        gd, r = crp.control_decomposition(controlled_paths[i])\n",
        "        gubinelli_derivatives.append(gd)\n",
        "        remainders.append(r)\n",
        "    gubinelli_derivatives = torch.stack(gubinelli_derivatives)  # Shape: (n_samples, seq_len-1, d_features)\n",
        "    remainders = torch.stack(remainders)  # Shape: (n_samples, seq_len-1, d_features)\n",
        "\n",
        "    # Flatten features\n",
        "    features = torch.cat([gubinelli_derivatives, remainders], dim=2).view(n_samples, -1)  # Flatten per sample\n",
        "\n",
        "    # Train-test split\n",
        "    train_size = int(0.8 * n_samples)\n",
        "    train_features, test_features = features[:train_size], features[train_size:]\n",
        "    train_targets, test_targets = targets[:train_size], targets[train_size:]\n",
        "\n",
        "    # Generate interpolated test data\n",
        "    interpolated_driving_paths, interpolated_controlled_paths, interpolated_targets = generate_interpolated_test_data(driving_paths[:train_size])\n",
        "    interpolated_features = []\n",
        "    for i in range(len(interpolated_driving_paths)):\n",
        "        crp = ControlledRoughPath(interpolated_driving_paths[i])\n",
        "        gd, r = crp.control_decomposition(interpolated_controlled_paths[i])\n",
        "        interpolated_features.append(torch.cat([gd, r], dim=1).view(-1))\n",
        "    interpolated_features = torch.stack(interpolated_features)  # Shape: (n_interpolated_samples, feature_dim)\n",
        "\n",
        "    # Initialize the neural network\n",
        "    input_dim = features.shape[1]\n",
        "    hidden_dim = 64\n",
        "    output_dim = 1\n",
        "    model = CRPNet(input_dim, hidden_dim, output_dim)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Train the neural network\n",
        "    n_epochs = 1000\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(train_features)\n",
        "        loss = criterion(outputs.squeeze(), train_targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Evaluate on test data\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            test_outputs = model(test_features)\n",
        "            interpolated_outputs = model(interpolated_features)\n",
        "            test_loss = criterion(test_outputs.squeeze(), test_targets).item()\n",
        "            interpolated_loss = criterion(interpolated_outputs.squeeze(), interpolated_targets).item()\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch [{epoch + 1}/{n_epochs}], Loss: {loss.item():.4f}, Test Loss: {test_loss:.4f}, Interpolated Loss: {interpolated_loss:.4f}\")\n",
        "\n",
        "    print(\"Training complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cks5JcMWWfAR",
        "outputId": "c8cd9b47-3e10-4c4d-98a4-cf004af0b56e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/1000], Loss: 409.3355, Test Loss: 418.7178, Interpolated Loss: 144.3172\n",
            "Epoch [20/1000], Loss: 407.8700, Test Loss: 418.6573, Interpolated Loss: 147.7181\n",
            "Epoch [30/1000], Loss: 406.1148, Test Loss: 418.6833, Interpolated Loss: 152.0197\n",
            "Epoch [40/1000], Loss: 403.9539, Test Loss: 418.8814, Interpolated Loss: 157.2930\n",
            "Epoch [50/1000], Loss: 401.3334, Test Loss: 419.1865, Interpolated Loss: 163.6399\n",
            "Epoch [60/1000], Loss: 398.2603, Test Loss: 419.6589, Interpolated Loss: 170.8922\n",
            "Epoch [70/1000], Loss: 394.8114, Test Loss: 420.2373, Interpolated Loss: 179.2038\n",
            "Epoch [80/1000], Loss: 391.0190, Test Loss: 421.1148, Interpolated Loss: 188.3498\n",
            "Epoch [90/1000], Loss: 386.9601, Test Loss: 422.2291, Interpolated Loss: 198.3192\n",
            "Epoch [100/1000], Loss: 382.6766, Test Loss: 423.5846, Interpolated Loss: 207.8900\n",
            "Epoch [110/1000], Loss: 378.2228, Test Loss: 425.1456, Interpolated Loss: 217.3708\n",
            "Epoch [120/1000], Loss: 373.5956, Test Loss: 426.9443, Interpolated Loss: 225.5290\n",
            "Epoch [130/1000], Loss: 368.7935, Test Loss: 428.8761, Interpolated Loss: 234.0258\n",
            "Epoch [140/1000], Loss: 363.8749, Test Loss: 430.9845, Interpolated Loss: 243.4299\n",
            "Epoch [150/1000], Loss: 358.8381, Test Loss: 433.2375, Interpolated Loss: 250.7018\n",
            "Epoch [160/1000], Loss: 353.6872, Test Loss: 435.4354, Interpolated Loss: 255.4769\n",
            "Epoch [170/1000], Loss: 348.4577, Test Loss: 437.5626, Interpolated Loss: 257.6844\n",
            "Epoch [180/1000], Loss: 343.0937, Test Loss: 439.6547, Interpolated Loss: 259.4756\n",
            "Epoch [190/1000], Loss: 337.6298, Test Loss: 441.9680, Interpolated Loss: 260.2239\n",
            "Epoch [200/1000], Loss: 332.1025, Test Loss: 444.0655, Interpolated Loss: 260.5992\n",
            "Epoch [210/1000], Loss: 326.5168, Test Loss: 446.1585, Interpolated Loss: 261.3601\n",
            "Epoch [220/1000], Loss: 320.8643, Test Loss: 448.4143, Interpolated Loss: 262.4213\n",
            "Epoch [230/1000], Loss: 315.1904, Test Loss: 450.7628, Interpolated Loss: 261.1028\n",
            "Epoch [240/1000], Loss: 309.5597, Test Loss: 453.2322, Interpolated Loss: 259.7867\n",
            "Epoch [250/1000], Loss: 303.9047, Test Loss: 455.5638, Interpolated Loss: 257.4893\n",
            "Epoch [260/1000], Loss: 298.1852, Test Loss: 457.9632, Interpolated Loss: 255.2246\n",
            "Epoch [270/1000], Loss: 292.4566, Test Loss: 460.4377, Interpolated Loss: 253.1067\n",
            "Epoch [280/1000], Loss: 286.7023, Test Loss: 462.9480, Interpolated Loss: 251.1886\n",
            "Epoch [290/1000], Loss: 280.9532, Test Loss: 465.5647, Interpolated Loss: 249.3638\n",
            "Epoch [300/1000], Loss: 275.2247, Test Loss: 468.2603, Interpolated Loss: 246.2486\n",
            "Epoch [310/1000], Loss: 269.5100, Test Loss: 471.0798, Interpolated Loss: 243.7943\n",
            "Epoch [320/1000], Loss: 263.8286, Test Loss: 474.1873, Interpolated Loss: 242.1542\n",
            "Epoch [330/1000], Loss: 258.1512, Test Loss: 477.4120, Interpolated Loss: 240.7744\n",
            "Epoch [340/1000], Loss: 252.5080, Test Loss: 480.7809, Interpolated Loss: 240.6962\n",
            "Epoch [350/1000], Loss: 246.8728, Test Loss: 484.3227, Interpolated Loss: 242.0022\n",
            "Epoch [360/1000], Loss: 241.3147, Test Loss: 487.9115, Interpolated Loss: 242.3773\n",
            "Epoch [370/1000], Loss: 235.8445, Test Loss: 491.5444, Interpolated Loss: 242.9725\n",
            "Epoch [380/1000], Loss: 230.4213, Test Loss: 495.4543, Interpolated Loss: 243.4384\n",
            "Epoch [390/1000], Loss: 225.0090, Test Loss: 499.1951, Interpolated Loss: 242.2910\n",
            "Epoch [400/1000], Loss: 219.6148, Test Loss: 502.8752, Interpolated Loss: 240.1756\n",
            "Epoch [410/1000], Loss: 214.2670, Test Loss: 506.9696, Interpolated Loss: 241.2290\n",
            "Epoch [420/1000], Loss: 208.9998, Test Loss: 510.9207, Interpolated Loss: 242.5987\n",
            "Epoch [430/1000], Loss: 203.7615, Test Loss: 514.6005, Interpolated Loss: 243.3120\n",
            "Epoch [440/1000], Loss: 198.5804, Test Loss: 518.3743, Interpolated Loss: 243.1659\n",
            "Epoch [450/1000], Loss: 193.5132, Test Loss: 522.1951, Interpolated Loss: 242.7566\n",
            "Epoch [460/1000], Loss: 188.4778, Test Loss: 526.0078, Interpolated Loss: 243.0303\n",
            "Epoch [470/1000], Loss: 183.4858, Test Loss: 529.8115, Interpolated Loss: 244.6137\n",
            "Epoch [480/1000], Loss: 178.5429, Test Loss: 533.6105, Interpolated Loss: 246.9150\n",
            "Epoch [490/1000], Loss: 173.6354, Test Loss: 537.1868, Interpolated Loss: 247.9883\n",
            "Epoch [500/1000], Loss: 168.7520, Test Loss: 540.6800, Interpolated Loss: 248.7066\n",
            "Epoch [510/1000], Loss: 163.9795, Test Loss: 544.3748, Interpolated Loss: 247.4557\n",
            "Epoch [520/1000], Loss: 159.3190, Test Loss: 547.9678, Interpolated Loss: 248.3575\n",
            "Epoch [530/1000], Loss: 154.7405, Test Loss: 551.5299, Interpolated Loss: 249.8421\n",
            "Epoch [540/1000], Loss: 150.2497, Test Loss: 554.9005, Interpolated Loss: 253.6247\n",
            "Epoch [550/1000], Loss: 145.8468, Test Loss: 558.6132, Interpolated Loss: 258.2988\n",
            "Epoch [560/1000], Loss: 141.5229, Test Loss: 562.2703, Interpolated Loss: 260.8565\n",
            "Epoch [570/1000], Loss: 137.2949, Test Loss: 566.5944, Interpolated Loss: 263.8578\n",
            "Epoch [580/1000], Loss: 133.1522, Test Loss: 571.0071, Interpolated Loss: 266.3904\n",
            "Epoch [590/1000], Loss: 129.0746, Test Loss: 575.6674, Interpolated Loss: 269.0850\n",
            "Epoch [600/1000], Loss: 125.0928, Test Loss: 580.4750, Interpolated Loss: 271.6682\n",
            "Epoch [610/1000], Loss: 121.2011, Test Loss: 585.0797, Interpolated Loss: 275.8692\n",
            "Epoch [620/1000], Loss: 117.4348, Test Loss: 589.8770, Interpolated Loss: 280.8225\n",
            "Epoch [630/1000], Loss: 113.7403, Test Loss: 594.4897, Interpolated Loss: 284.0839\n",
            "Epoch [640/1000], Loss: 110.1417, Test Loss: 599.0638, Interpolated Loss: 285.9238\n",
            "Epoch [650/1000], Loss: 106.6378, Test Loss: 603.6029, Interpolated Loss: 288.7964\n",
            "Epoch [660/1000], Loss: 103.2481, Test Loss: 608.2596, Interpolated Loss: 291.9028\n",
            "Epoch [670/1000], Loss: 99.9575, Test Loss: 612.8180, Interpolated Loss: 294.9274\n",
            "Epoch [680/1000], Loss: 96.7554, Test Loss: 617.2542, Interpolated Loss: 297.3530\n",
            "Epoch [690/1000], Loss: 93.6076, Test Loss: 621.2391, Interpolated Loss: 300.2868\n",
            "Epoch [700/1000], Loss: 90.5302, Test Loss: 625.2534, Interpolated Loss: 303.2621\n",
            "Epoch [710/1000], Loss: 87.5393, Test Loss: 629.4979, Interpolated Loss: 305.5832\n",
            "Epoch [720/1000], Loss: 84.6307, Test Loss: 633.3704, Interpolated Loss: 308.1822\n",
            "Epoch [730/1000], Loss: 81.8044, Test Loss: 637.4519, Interpolated Loss: 310.3992\n",
            "Epoch [740/1000], Loss: 79.0608, Test Loss: 641.5093, Interpolated Loss: 310.7707\n",
            "Epoch [750/1000], Loss: 76.3843, Test Loss: 645.8671, Interpolated Loss: 311.4607\n",
            "Epoch [760/1000], Loss: 73.7771, Test Loss: 650.0108, Interpolated Loss: 311.2853\n",
            "Epoch [770/1000], Loss: 71.2501, Test Loss: 654.1716, Interpolated Loss: 309.6051\n",
            "Epoch [780/1000], Loss: 68.7859, Test Loss: 658.1775, Interpolated Loss: 309.0558\n",
            "Epoch [790/1000], Loss: 66.3811, Test Loss: 661.8640, Interpolated Loss: 308.3424\n",
            "Epoch [800/1000], Loss: 64.0287, Test Loss: 665.7185, Interpolated Loss: 309.3643\n",
            "Epoch [810/1000], Loss: 61.7374, Test Loss: 669.4954, Interpolated Loss: 310.2116\n",
            "Epoch [820/1000], Loss: 59.5105, Test Loss: 673.4144, Interpolated Loss: 311.3125\n",
            "Epoch [830/1000], Loss: 57.3375, Test Loss: 676.9373, Interpolated Loss: 310.4379\n",
            "Epoch [840/1000], Loss: 55.2307, Test Loss: 680.4330, Interpolated Loss: 310.9290\n",
            "Epoch [850/1000], Loss: 53.1987, Test Loss: 683.8625, Interpolated Loss: 311.3869\n",
            "Epoch [860/1000], Loss: 51.2433, Test Loss: 687.2059, Interpolated Loss: 311.8164\n",
            "Epoch [870/1000], Loss: 49.3466, Test Loss: 690.7930, Interpolated Loss: 311.5147\n",
            "Epoch [880/1000], Loss: 47.5007, Test Loss: 694.4163, Interpolated Loss: 310.9232\n",
            "Epoch [890/1000], Loss: 45.7201, Test Loss: 697.9415, Interpolated Loss: 309.7760\n",
            "Epoch [900/1000], Loss: 43.9886, Test Loss: 701.3130, Interpolated Loss: 309.1778\n",
            "Epoch [910/1000], Loss: 42.2964, Test Loss: 704.7926, Interpolated Loss: 307.9675\n",
            "Epoch [920/1000], Loss: 40.6586, Test Loss: 708.1309, Interpolated Loss: 307.8658\n",
            "Epoch [930/1000], Loss: 39.0706, Test Loss: 711.3911, Interpolated Loss: 306.9005\n",
            "Epoch [940/1000], Loss: 37.5317, Test Loss: 714.9129, Interpolated Loss: 306.6076\n",
            "Epoch [950/1000], Loss: 36.0382, Test Loss: 718.3677, Interpolated Loss: 305.0116\n",
            "Epoch [960/1000], Loss: 34.5975, Test Loss: 721.9282, Interpolated Loss: 304.4818\n",
            "Epoch [970/1000], Loss: 33.2069, Test Loss: 725.5684, Interpolated Loss: 301.3621\n",
            "Epoch [980/1000], Loss: 31.8618, Test Loss: 729.0563, Interpolated Loss: 299.0134\n",
            "Epoch [990/1000], Loss: 30.5624, Test Loss: 732.3884, Interpolated Loss: 296.3260\n",
            "Epoch [1000/1000], Loss: 29.3077, Test Loss: 735.6721, Interpolated Loss: 294.4614\n",
            "Training complete.\n"
          ]
        }
      ]
    }
  ]
}