{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNoyZNwz/c8SACuCuP9jvDU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thegallier/timeseries/blob/main/lead_lag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install esig\n",
        "!pip install pywt\n",
        "!pip install dtaidistance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpzcZo32MAVG",
        "outputId": "d2abb9cc-ffc1-4262-af3e-106cecb20e45"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: esig in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from esig) (2.2.2)\n",
            "Requirement already satisfied: pyrecombine in /usr/local/lib/python3.11/dist-packages (from esig) (1.0.0)\n",
            "Requirement already satisfied: roughpy in /usr/local/lib/python3.11/dist-packages (from esig) (0.2.0)\n",
            "Requirement already satisfied: intel_openmp in /usr/local/lib/python3.11/dist-packages (from pyrecombine->esig) (2025.0.4)\n",
            "Requirement already satisfied: intel-cmplr-lib-ur==2025.0.4 in /usr/local/lib/python3.11/dist-packages (from intel_openmp->pyrecombine->esig) (2025.0.4)\n",
            "Requirement already satisfied: umf==0.9.* in /usr/local/lib/python3.11/dist-packages (from intel-cmplr-lib-ur==2025.0.4->intel_openmp->pyrecombine->esig) (0.9.1)\n",
            "Requirement already satisfied: tcmlib==1.2 in /usr/local/lib/python3.11/dist-packages (from umf==0.9.*->intel-cmplr-lib-ur==2025.0.4->intel_openmp->pyrecombine->esig) (1.2.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement pywt (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for pywt\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting dtaidistance\n",
            "  Downloading dtaidistance-2.3.12.tar.gz (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from dtaidistance) (2.2.2)\n",
            "Building wheels for collected packages: dtaidistance\n",
            "  Building wheel for dtaidistance (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dtaidistance: filename=dtaidistance-2.3.12-cp311-cp311-linux_x86_64.whl size=2628515 sha256=cb1bce401d036b53dd91fb98d3fb14b4926a3df5a8f531864f81e90404305a16\n",
            "  Stored in directory: /root/.cache/pip/wheels/d9/3e/79/f71e79f61d63670c1f7cf9df3e474d9d9ccb35e56b547f8896\n",
            "Successfully built dtaidistance\n",
            "Installing collected packages: dtaidistance\n",
            "Successfully installed dtaidistance-2.3.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.signal import correlate\n",
        "\n",
        "###############################################################################\n",
        "# 1) Real Data Creation\n",
        "###############################################################################\n",
        "\n",
        "def create_real_data(n=200, seed=42):\n",
        "    \"\"\"\n",
        "    Creates an asynchronous orders DataFrame with columns:\n",
        "      [datetime, symbol, price, quantity, side, orderType, data1, data2]\n",
        "\n",
        "    'data1' and 'data2' are additional numeric fields for demonstration.\n",
        "    We do NOT group by them. We'll group only by (symbol, side, orderType).\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Random exponential interarrival times\n",
        "    start = pd.Timestamp(\"2023-01-01 09:30:00\")\n",
        "    random_gaps = np.random.exponential(scale=30, size=n).cumsum()  # ~30s average gap\n",
        "    datetimes = start + pd.to_timedelta(random_gaps, unit='s')\n",
        "\n",
        "    # Symbol\n",
        "    symbols = np.random.choice(['AAPL','GOOG','TSLA'], size=n, p=[0.4,0.3,0.3])\n",
        "    # Price as random walk\n",
        "    price = 100 + np.cumsum(np.random.randn(n))\n",
        "    # Quantity\n",
        "    quantity = np.random.randint(1, 500, size=n)\n",
        "    # Side\n",
        "    side = np.random.choice(['buy','sell'], size=n)\n",
        "    # orderType\n",
        "    order_type = np.random.choice(['market','limit','stop'], size=n, p=[0.5,0.4,0.1])\n",
        "    # data1, data2 (extra numeric columns)\n",
        "    data1 = np.random.normal(loc=50, scale=10, size=n)\n",
        "    data2 = np.random.normal(loc=100, scale=20, size=n)\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        'datetime': datetimes,\n",
        "        'symbol': symbols,\n",
        "        'price': price,\n",
        "        'quantity': quantity,\n",
        "        'side': side,\n",
        "        'orderType': order_type,\n",
        "        'data1': data1,\n",
        "        'data2': data2\n",
        "    }).sort_values('datetime')\n",
        "    return df\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# 2) Synthetic Data Generation (Grouping only by symbol, side, orderType)\n",
        "###############################################################################\n",
        "\n",
        "def generate_synthetic_data(n=100,\n",
        "                            reference_df=None,\n",
        "                            use_features_from_reference=True,\n",
        "                            seed=999):\n",
        "    \"\"\"\n",
        "    Create a synthetic dataset with the same columns.\n",
        "    We only group by ['symbol','side','orderType'] so the group key is a 3-tuple.\n",
        "    We do NOT group by data1 or data2, but we DO store them from the chosen reference row.\n",
        "\n",
        "    Avoids the mismatch in shape that causes:\n",
        "        ValueError: must supply a tuple to get_group with multiple grouping keys\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # We'll produce random ascending datetimes for synthetic\n",
        "    start = pd.Timestamp(\"2023-01-02 09:30:00\")\n",
        "    random_gaps = np.random.exponential(scale=25, size=n).cumsum()\n",
        "    synthetic_times = start + pd.to_timedelta(random_gaps, unit='s')\n",
        "\n",
        "    if (reference_df is None) or (not use_features_from_reference):\n",
        "        # Unconditional\n",
        "        symbols = np.random.choice(['AAPL','GOOG','TSLA'], size=n)\n",
        "        price = 100 + np.cumsum(np.random.randn(n))\n",
        "        quantity = np.random.randint(1, 500, size=n)\n",
        "        side = np.random.choice(['buy','sell'], size=n)\n",
        "        order_type = np.random.choice(['market','limit','stop'], size=n, p=[0.5,0.4,0.1])\n",
        "        data1 = np.random.normal(loc=50, scale=10, size=n)\n",
        "        data2 = np.random.normal(loc=100, scale=20, size=n)\n",
        "\n",
        "        df_synth = pd.DataFrame({\n",
        "            'datetime': synthetic_times,\n",
        "            'symbol': symbols,\n",
        "            'price': price,\n",
        "            'quantity': quantity,\n",
        "            'side': side,\n",
        "            'orderType': order_type,\n",
        "            'data1': data1,\n",
        "            'data2': data2\n",
        "        }).sort_values('datetime')\n",
        "        return df_synth\n",
        "\n",
        "    else:\n",
        "        # Group only by these 3 columns => each group_key is a 3-tuple\n",
        "        groups = reference_df.groupby(['symbol','side','orderType'])\n",
        "        group_keys = list(groups.groups.keys())\n",
        "        # Must be object array, each element is a 3-tuple\n",
        "        group_keys = np.array(group_keys, dtype=object)\n",
        "\n",
        "        group_sizes = [len(groups.get_group(k)) for k in group_keys]\n",
        "        total = sum(group_sizes)\n",
        "        probs = [gs/total for gs in group_sizes]\n",
        "\n",
        "        rows = []\n",
        "        for i in range(n):\n",
        "            dt = synthetic_times[i]\n",
        "            choice_key = np.random.choice(group_keys, p=probs)\n",
        "            # e.g. choice_key = ('AAPL','buy','limit'), a 3-tuple\n",
        "            sub_df = groups.get_group(choice_key)\n",
        "            # sample 1 row from that subgroup\n",
        "            row_ref = sub_df.sample(n=1).iloc[0]\n",
        "            # copy relevant fields from that row\n",
        "            new_row = {\n",
        "                'datetime': dt,\n",
        "                'symbol': row_ref['symbol'],\n",
        "                'price': row_ref['price'],\n",
        "                'quantity': row_ref['quantity'],\n",
        "                'side': row_ref['side'],\n",
        "                'orderType': row_ref['orderType'],\n",
        "                'data1': row_ref['data1'],  # we store the same data1, or random from subgroup\n",
        "                'data2': row_ref['data2']\n",
        "            }\n",
        "            rows.append(new_row)\n",
        "\n",
        "        df_synth = pd.DataFrame(rows).sort_values('datetime')\n",
        "        return df_synth\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# 3) Example transforms and cross-correlation\n",
        "###############################################################################\n",
        "\n",
        "def transform_series(df, transform='returns', price_col='price'):\n",
        "    \"\"\"\n",
        "    We'll produce 'value' column. Options: 'returns', 'log_returns', 'diff', 'none'\n",
        "    Sort by datetime first (asynchronous).\n",
        "    \"\"\"\n",
        "    df = df.sort_values('datetime').copy()\n",
        "    if transform == 'returns':\n",
        "        df['value'] = df[price_col].pct_change().fillna(0)\n",
        "    elif transform == 'log_returns':\n",
        "        df['value'] = np.log(df[price_col]).diff().fillna(0)\n",
        "    elif transform == 'diff':\n",
        "        df['value'] = df[price_col].diff().fillna(0)\n",
        "    else:\n",
        "        # no transform\n",
        "        df['value'] = df[price_col]\n",
        "    return df[['datetime','value']]\n",
        "\n",
        "\n",
        "def cross_correlation_method(returns1, returns2, max_lag=20):\n",
        "    \"\"\"\n",
        "    Row-based cross-correlation from your snippet\n",
        "    (NOT time-based; best_lag is # of array elements).\n",
        "    \"\"\"\n",
        "    cross_corr = correlate(returns1, returns2, mode='full')\n",
        "    lags = np.arange(-len(returns1)+1, len(returns1))\n",
        "    best_lag = lags[np.argmax(np.abs(cross_corr))]\n",
        "    if abs(best_lag) > max_lag:\n",
        "        best_lag = max_lag * np.sign(best_lag)\n",
        "    return best_lag\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# DEMO\n",
        "###############################################################################\n",
        "\n",
        "def main():\n",
        "    # 1) Create real data\n",
        "    df_real = create_real_data(n=50, seed=42)\n",
        "    print(\"Real data example:\")\n",
        "    print(df_real.head(5))\n",
        "\n",
        "    # 2) Create synthetic data referencing real groups\n",
        "    df_synth = generate_synthetic_data2(n=30, reference_df=df_real, use_features_from_reference=True, seed=123)\n",
        "    print(\"\\nSynthetic data example:\")\n",
        "    print(df_synth.head(5))\n",
        "\n",
        "    # 3) Transform both => log_returns\n",
        "    real_trans  = transform_series(df_real, transform='log_returns')\n",
        "    synth_trans = transform_series(df_synth, transform='log_returns')\n",
        "\n",
        "    # 4) Row-based cross-correlation on 'value' arrays\n",
        "    arr_real  = real_trans['value'].values\n",
        "    arr_synth = synth_trans['value'].values\n",
        "    min_len = min(len(arr_real), len(arr_synth))\n",
        "    best_lag_rows = cross_correlation_method(arr_real[:min_len], arr_synth[:min_len], max_lag=10)\n",
        "    print(f\"\\nRow-based cross_correlation_method => best_lag_rows={best_lag_rows}\")\n",
        "\n",
        "if __name__=='__main__':\n",
        "    main()\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNbA4gX8PYwP",
        "outputId": "75ea260c-2a4f-45a9-d639-83c0dd661334"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Real data example:\n",
            "                       datetime symbol       price  quantity  side orderType  \\\n",
            "0 2023-01-01 09:30:14.078042699   TSLA  100.087047       240  sell    market   \n",
            "1 2023-01-01 09:31:44.381685627   TSLA   99.788040       144  sell    market   \n",
            "2 2023-01-01 09:32:23.884056433   TSLA   99.879800        97   buy    market   \n",
            "3 2023-01-01 09:32:51.272333046   TSLA   97.892232       201   buy     limit   \n",
            "4 2023-01-01 09:32:56.361079160   GOOG   97.672560       124  sell     limit   \n",
            "\n",
            "       data1       data2  \n",
            "0  54.497740  113.981848  \n",
            "1  67.789243  111.997933  \n",
            "2  43.330566   90.375037  \n",
            "3  33.081817   58.124120  \n",
            "4  56.567973   91.599051  \n",
            "\n",
            "Synthetic data example:\n",
            "                       datetime symbol      price  quantity  side orderType  \\\n",
            "0 2023-01-02 09:30:29.806803587   AAPL  96.994863       441   buy     limit   \n",
            "1 2023-01-02 09:30:38.233490654   AAPL  98.989296       464   buy     limit   \n",
            "2 2023-01-02 09:30:44.665592657   GOOG  98.181745         2   buy    market   \n",
            "3 2023-01-02 09:31:04.701434702   GOOG  99.972428       346  sell    market   \n",
            "4 2023-01-02 09:31:36.478208089   AAPL  98.550605       148   buy     limit   \n",
            "\n",
            "       data1       data2  \n",
            "0  42.673078   75.235383  \n",
            "1  54.949957  115.118706  \n",
            "2  52.289704   89.045530  \n",
            "3  45.449668  136.179137  \n",
            "4  30.084622  108.925787  \n",
            "\n",
            "Row-based cross_correlation_method => best_lag_rows=-10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install esig==0.6.9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "GVT2RWjRajWz",
        "outputId": "765a84f4-0a87-4948-e269-d0c1ef4c39a0"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting esig==0.6.9\n",
            "  Downloading esig-0.6.9.tar.gz (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.5/69.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.11/dist-packages (from esig==0.6.9) (2.2.2)\n",
            "Building wheels for collected packages: esig\n",
            "  Building wheel for esig (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for esig: filename=esig-0.6.9-cp311-cp311-linux_x86_64.whl size=19298398 sha256=7eba863de8ae18265f7d7a706d144506bfbbec5cda72e6207ba3a974fbb6b386\n",
            "  Stored in directory: /root/.cache/pip/wheels/27/39/83/e19f329e6cb503c72f6f1fecf7b11c936d3810c59739419a79\n",
            "Successfully built esig\n",
            "Installing collected packages: esig\n",
            "  Attempting uninstall: esig\n",
            "    Found existing installation: esig 1.0.0\n",
            "    Uninstalling esig-1.0.0:\n",
            "      Successfully uninstalled esig-1.0.0\n",
            "Successfully installed esig-0.6.9\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "esig"
                ]
              },
              "id": "db6b670d588f4fafbe85e5ea3c10a58c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from esig import *"
      ],
      "metadata": {
        "id": "SGiXcNbcZ9NM"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Optional libraries:\n",
        "try:\n",
        "    from dtaidistance import dtw  # for DTW\n",
        "    DTW_AVAILABLE = True\n",
        "except ImportError:\n",
        "    DTW_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    import PyWavelets as pywt  # for wavelets\n",
        "    PYWT_AVAILABLE = True\n",
        "except ImportError:\n",
        "    PYWT_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    from esig import tosig  # for signatures\n",
        "    ESIG_AVAILABLE = True\n",
        "except ImportError:\n",
        "    ESIG_AVAILABLE = False\n",
        "\n",
        "###############################################################################\n",
        "# 1) CORRECTED DATA GENERATION\n",
        "###############################################################################\n",
        "\n",
        "def create_real_data(n=200, seed=42):\n",
        "    \"\"\"\n",
        "    Creates asynchronous orders with columns:\n",
        "      [datetime, symbol, price, quantity, side, orderType, data1, data2].\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Random exponential interarrival times\n",
        "    start = pd.Timestamp(\"2023-01-01 09:30:00\")\n",
        "    random_gaps = np.random.exponential(scale=30, size=n).cumsum()\n",
        "    datetimes = start + pd.to_timedelta(random_gaps, unit='s')\n",
        "\n",
        "    symbols = np.random.choice(['AAPL','GOOG','TSLA'], size=n, p=[0.4,0.3,0.3])\n",
        "    price = 100 + np.cumsum(np.random.randn(n))\n",
        "    quantity = np.random.randint(1, 500, size=n)\n",
        "    side = np.random.choice(['buy','sell'], size=n)\n",
        "    order_type = np.random.choice(['market','limit','stop'], size=n, p=[0.5,0.4,0.1])\n",
        "    data1 = np.random.normal(loc=50, scale=10, size=n)\n",
        "    data2 = np.random.normal(loc=100, scale=20, size=n)\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        'datetime': datetimes,\n",
        "        'symbol': symbols,\n",
        "        'price': price,\n",
        "        'quantity': quantity,\n",
        "        'side': side,\n",
        "        'orderType': order_type,\n",
        "        'data1': data1,\n",
        "        'data2': data2\n",
        "    })\n",
        "    df.sort_values('datetime', inplace=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "def generate_synthetic_data(n=100,\n",
        "                            reference_df=None,\n",
        "                            use_features_from_reference=True,\n",
        "                            seed=999):\n",
        "    \"\"\"\n",
        "    Synthetic dataset with same columns.\n",
        "    Grouping only by (symbol, side, orderType) to avoid mismatch keys.\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    start = pd.Timestamp(\"2023-01-02 09:30:00\")\n",
        "    random_gaps = np.random.exponential(scale=25, size=n).cumsum()\n",
        "    synth_times = start + pd.to_timedelta(random_gaps, unit='s')\n",
        "\n",
        "    if (reference_df is None) or (not use_features_from_reference):\n",
        "        # Unconditional\n",
        "        symbols = np.random.choice(['AAPL','GOOG','TSLA'], size=n)\n",
        "        price   = 100 + np.cumsum(np.random.randn(n))\n",
        "        qty     = np.random.randint(1, 500, size=n)\n",
        "        side    = np.random.choice(['buy','sell'], size=n)\n",
        "        otype   = np.random.choice(['market','limit','stop'], size=n, p=[0.5,0.4,0.1])\n",
        "        data1   = np.random.normal(loc=50,  scale=10,  size=n)\n",
        "        data2   = np.random.normal(loc=100, scale=20,  size=n)\n",
        "\n",
        "        df_synth = pd.DataFrame({\n",
        "            'datetime': synth_times,\n",
        "            'symbol': symbols,\n",
        "            'price': price,\n",
        "            'quantity': qty,\n",
        "            'side': side,\n",
        "            'orderType': otype,\n",
        "            'data1': data1,\n",
        "            'data2': data2\n",
        "        })\n",
        "        df_synth.sort_values('datetime', inplace=True)\n",
        "        return df_synth\n",
        "\n",
        "    else:\n",
        "        # Group only by (symbol, side, orderType)\n",
        "        groups = reference_df.groupby(['symbol','side','orderType'])\n",
        "        group_keys = list(groups.groups.keys())\n",
        "        group_keys = np.array(group_keys, dtype=object)  # must be 1D object array\n",
        "\n",
        "        group_sizes = [len(groups.get_group(k)) for k in group_keys]\n",
        "        total = sum(group_sizes)\n",
        "        probs = [gs/total for gs in group_sizes]\n",
        "\n",
        "        rows = []\n",
        "        for i in range(n):\n",
        "            dt = synth_times[i]\n",
        "            choice_key = np.random.choice(group_keys, p=probs)\n",
        "            sub_df = groups.get_group(choice_key)\n",
        "            row_ref = sub_df.sample(n=1).iloc[0]\n",
        "\n",
        "            new_row = {\n",
        "                'datetime': dt,\n",
        "                'symbol': row_ref['symbol'],\n",
        "                'price': row_ref['price'],\n",
        "                'quantity': row_ref['quantity'],\n",
        "                'side': row_ref['side'],\n",
        "                'orderType': row_ref['orderType'],\n",
        "                'data1': row_ref['data1'],\n",
        "                'data2': row_ref['data2']\n",
        "            }\n",
        "            rows.append(new_row)\n",
        "\n",
        "        df_synth = pd.DataFrame(rows)\n",
        "        df_synth.sort_values('datetime', inplace=True)\n",
        "        return df_synth\n",
        "\n",
        "###############################################################################\n",
        "# 2) HELPER: TRANSFORM SERIES\n",
        "###############################################################################\n",
        "\n",
        "def transform_series(df, transform='log_returns', price_col='price'):\n",
        "    \"\"\"\n",
        "    Create a 'value' column from 'price' using the chosen transform.\n",
        "    Options: 'returns', 'log_returns', 'diff', 'none'\n",
        "    \"\"\"\n",
        "    df = df.sort_values('datetime').copy()\n",
        "    if transform == 'returns':\n",
        "        df['value'] = df[price_col].pct_change().fillna(0)\n",
        "    elif transform == 'log_returns':\n",
        "        df['value'] = np.log(df[price_col]).diff().fillna(0)\n",
        "    elif transform == 'diff':\n",
        "        df['value'] = df[price_col].diff().fillna(0)\n",
        "    else:\n",
        "        # no transform\n",
        "        df['value'] = df[price_col]\n",
        "    return df[['datetime','value']]\n",
        "\n",
        "###############################################################################\n",
        "# 3) TIME-BASED RESAMPLING & SHIFT\n",
        "###############################################################################\n",
        "\n",
        "def resample_to_uniform_grid(df, base_time, fill_method='ffill'):\n",
        "    \"\"\"\n",
        "    Convert 'datetime' -> ms from base_time, then create a Series\n",
        "    with 1ms steps from min to max. Forward-fill missing.\n",
        "    Return the (index array, series array).\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    df['time_ms'] = (df['datetime'] - base_time).dt.total_seconds()*1000\n",
        "    t_min = int(df['time_ms'].min())\n",
        "    t_max = int(df['time_ms'].max())\n",
        "    idx = np.arange(t_min, t_max+1, 1, dtype=int)\n",
        "\n",
        "    s = pd.Series(index=idx, dtype=float)\n",
        "    for (tm,val) in zip(df['time_ms'], df['value']):\n",
        "        s.loc[int(tm)] = val\n",
        "\n",
        "    if fill_method:\n",
        "        s = s.fillna(method='ffill').fillna(0)\n",
        "    return idx, s.values  # time index array, data array\n",
        "\n",
        "###############################################################################\n",
        "# 4) CROSS-CORRELATION (TIME-BASED)\n",
        "###############################################################################\n",
        "\n",
        "def find_lead_lag_crosscorr_time(df1, df2, max_lag_ms=2000, step_ms=100):\n",
        "    \"\"\"\n",
        "    For cross-correlation in time. Shift df2 from -max_lag..+max_lag by step_ms,\n",
        "    measure Pearson correlation, pick best. Return (best_lag_ms, best_corr).\n",
        "    \"\"\"\n",
        "    # 1) find a common base_time\n",
        "    base_t = min(df1['datetime'].min(), df2['datetime'].min())\n",
        "    # 2) uniform grid\n",
        "    idx1, arr1 = resample_to_uniform_grid(df1, base_t)\n",
        "    idx2, arr2 = resample_to_uniform_grid(df2, base_t)\n",
        "\n",
        "    best_lag, best_corr = 0, -999\n",
        "\n",
        "    for lag in range(-max_lag_ms, max_lag_ms + 1, step_ms):\n",
        "        # Ensure a1 and a2 have the same length after shifting\n",
        "        len_a1 = len(arr1) - abs(lag)  # Length of a1 after potential truncation\n",
        "        len_a2 = len(arr2) - abs(lag)  # Length of a2 after potential truncation\n",
        "        min_len = min(len_a1, len_a2)  # Take the minimum length\n",
        "\n",
        "        if lag > 0:\n",
        "            a1 = arr1[lag : lag + min_len]\n",
        "            a2 = arr2[:min_len]\n",
        "        elif lag < 0:\n",
        "            a1 = arr1[:min_len]\n",
        "            a2 = arr2[abs(lag) : abs(lag) + min_len]\n",
        "        else:\n",
        "            a1 = arr1[:min_len]\n",
        "            a2 = arr2[:min_len]\n",
        "\n",
        "        if len(a1) < 2 or len(a2) < 2:\n",
        "            continue\n",
        "\n",
        "        c = np.corrcoef(a1, a2)[0, 1]\n",
        "        if abs(c) > abs(best_corr):\n",
        "            best_corr = c\n",
        "            best_lag = lag\n",
        "    return best_lag, best_corr\n",
        "\n",
        "###############################################################################\n",
        "# 5) DTW (TIME-BASED)\n",
        "###############################################################################\n",
        "\n",
        "def find_lead_lag_dtw_time(df1, df2, max_lag_ms=2000, step_ms=100):\n",
        "    \"\"\"\n",
        "    For each shift in [-max_lag_ms..+max_lag_ms], measure DTW distance\n",
        "    after uniform resampling. Return (best_lag_ms, best_dist).\n",
        "    Minimizes DTW distance.\n",
        "    \"\"\"\n",
        "    if not DTW_AVAILABLE:\n",
        "        print(\"DTW library not installed. Returning None.\")\n",
        "        return None, None\n",
        "\n",
        "    base_t = min(df1['datetime'].min(), df2['datetime'].min())\n",
        "    idx1, arr1 = resample_to_uniform_grid(df1, base_t)\n",
        "    idx2, arr2 = resample_to_uniform_grid(df2, base_t)\n",
        "\n",
        "    best_lag, best_dist = 0, 999999999\n",
        "\n",
        "    for lag in range(-max_lag_ms, max_lag_ms+1, step_ms):\n",
        "        if lag > 0:\n",
        "            a1 = arr1[lag:]\n",
        "            a2 = arr2[:len(a1)]\n",
        "        elif lag < 0:\n",
        "            l_ = abs(lag)\n",
        "            a1 = arr1[:len(arr1)-l_]\n",
        "            a2 = arr2[l_:l_+len(a1)]\n",
        "        else:\n",
        "            a1 = arr1\n",
        "            a2 = arr2\n",
        "\n",
        "        if len(a1) < 2 or len(a2) < 2:\n",
        "            continue\n",
        "\n",
        "        dist = dtw.distance(a1, a2)\n",
        "        if dist < best_dist:\n",
        "            best_dist = dist\n",
        "            best_lag = lag\n",
        "    return best_lag, best_dist\n",
        "\n",
        "###############################################################################\n",
        "# 6) HAYASHI–YOSHIDA (TIME-BASED SHIFT)\n",
        "###############################################################################\n",
        "\n",
        "def hayashi_yoshida_covar(arrA, arrB, idxA, idxB):\n",
        "    \"\"\"\n",
        "    Naive HY covariance on two signals (arrA, arrB) with time indexes in ms (idxA, idxB).\n",
        "    We'll interpret each increment in arrA, arrB.\n",
        "    This is simplistic; real HY is typically on tick data.\n",
        "    \"\"\"\n",
        "    # increments for A\n",
        "    A_incr = []\n",
        "    for i in range(1, len(arrA)):\n",
        "        dX = arrA[i] - arrA[i-1]\n",
        "        t0A, t1A = idxA[i-1], idxA[i]\n",
        "        A_incr.append((t0A, t1A, dX))\n",
        "\n",
        "    # increments for B\n",
        "    B_incr = []\n",
        "    for j in range(1, len(arrB)):\n",
        "        dY = arrB[j] - arrB[j-1]\n",
        "        t0B, t1B = idxB[j-1], idxB[j]\n",
        "        B_incr.append((t0B, t1B, dY))\n",
        "\n",
        "    hy_sum = 0.0\n",
        "    for (A0, A1, dX) in A_incr:\n",
        "        for (B0, B1, dY) in B_incr:\n",
        "            if A1 > B0 and B1 > A0:  # intervals overlap\n",
        "                hy_sum += dX * dY\n",
        "    return hy_sum\n",
        "\n",
        "def find_lead_lag_hy_time(df1, df2, max_lag_ms=2000, step_ms=100):\n",
        "    \"\"\"\n",
        "    Shift df2 in [-max_lag_ms..+max_lag_ms], compute HY correlation, find the shift with highest corr.\n",
        "    Return (best_lag_ms, best_corr).\n",
        "    \"\"\"\n",
        "    base_t = min(df1['datetime'].min(), df2['datetime'].min())\n",
        "    idx1, arr1 = resample_to_uniform_grid(df1, base_t)\n",
        "    idx2, arr2 = resample_to_uniform_grid(df2, base_t)\n",
        "\n",
        "    best_lag, best_corr = 0, -999\n",
        "\n",
        "    # We'll define a helper to compute HY correlation\n",
        "    def hy_correlation(aA, iA, aB, iB):\n",
        "        covAB = hayashi_yoshida_covar(aA, aB, iA, iB)\n",
        "        varA  = hayashi_yoshida_covar(aA, aA, iA, iA)\n",
        "        varB  = hayashi_yoshida_covar(aB, aB, iB, iB)\n",
        "        if varA <= 0 or varB <= 0:\n",
        "            return 0\n",
        "        return covAB / np.sqrt(varA*varB)\n",
        "\n",
        "    for lag in range(-max_lag_ms, max_lag_ms+1, step_ms):\n",
        "        if lag > 0:\n",
        "            a1 = arr1[lag:]\n",
        "            i1 = idx1[lag:]\n",
        "            a2 = arr2[:len(a1)]\n",
        "            i2 = idx2[:len(a1)]\n",
        "        elif lag < 0:\n",
        "            l_ = abs(lag)\n",
        "            a1 = arr1[:len(arr1)-l_]\n",
        "            i1 = idx1[:len(arr1)-l_]\n",
        "            a2 = arr2[l_:l_+len(a1)]\n",
        "            i2 = idx2[l_:l_+len(a1)]\n",
        "        else:\n",
        "            a1 = arr1\n",
        "            i1 = idx1\n",
        "            a2 = arr2\n",
        "            i2 = idx2\n",
        "\n",
        "        if len(a1) < 2 or len(a2) < 2:\n",
        "            continue\n",
        "\n",
        "        corr = hy_correlation(a1, i1, a2, i2)\n",
        "        if abs(corr) > abs(best_corr):\n",
        "            best_corr = corr\n",
        "            best_lag = lag\n",
        "\n",
        "    return best_lag, best_corr\n",
        "\n",
        "###############################################################################\n",
        "# 7) WAVELET (TIME-BASED SHIFT)\n",
        "###############################################################################\n",
        "\n",
        "def find_lead_lag_wavelet_time(df1, df2, max_lag_ms=2000, step_ms=100, wavelet='morl', scales=range(1,6)):\n",
        "    \"\"\"\n",
        "    Shift df2, compute wavelet transform of both. Then measure correlation\n",
        "    between summed wavelet coeffs. Return shift that yields highest correlation in absolute value.\n",
        "    \"\"\"\n",
        "    if not PYWT_AVAILABLE:\n",
        "        print(\"PyWavelets not installed. Returning None.\")\n",
        "        return None, None\n",
        "\n",
        "    base_t = min(df1['datetime'].min(), df2['datetime'].min())\n",
        "    idx1, arr1 = resample_to_uniform_grid(df1, base_t)\n",
        "    idx2, arr2 = resample_to_uniform_grid(df2, base_t)\n",
        "\n",
        "    def wavelet_feats(sig):\n",
        "        # compute CWT, sum abs coefficients\n",
        "        coeffs, freqs = pywt.cwt(sig, scales, wavelet)\n",
        "        # shape = (len(scales), len(sig))\n",
        "        return np.sum(np.abs(coeffs), axis=1)  # 1D array of length=len(scales)\n",
        "\n",
        "    best_lag, best_corr = 0, -999\n",
        "\n",
        "    for lag in range(-max_lag_ms, max_lag_ms+1, step_ms):\n",
        "        if lag > 0:\n",
        "            a1 = arr1[lag:]\n",
        "            a2 = arr2[:len(a1)]\n",
        "        elif lag < 0:\n",
        "            l_ = abs(lag)\n",
        "            a1 = arr1[:len(arr1)-l_]\n",
        "            a2 = arr2[l_:l_+len(a1)]\n",
        "        else:\n",
        "            a1 = arr1\n",
        "            a2 = arr2\n",
        "\n",
        "        if len(a1) < 2 or len(a2) < 2:\n",
        "            continue\n",
        "\n",
        "        w1 = wavelet_feats(a1)\n",
        "        w2 = wavelet_feats(a2)\n",
        "\n",
        "        # correlation of wavelet feature vectors ( length=len(scales) )\n",
        "        c = np.corrcoef(w1, w2)[0,1]\n",
        "        if abs(c) > abs(best_corr):\n",
        "            best_corr = c\n",
        "            best_lag = lag\n",
        "    return best_lag, best_corr\n",
        "\n",
        "###############################################################################\n",
        "# 8) SIGNATURE (TIME-BASED SHIFT)\n",
        "###############################################################################\n",
        "\n",
        "def find_lead_lag_signature_time(df1, df2, max_lag_ms=2000, step_ms=100, level=2):\n",
        "    \"\"\"\n",
        "    Shift df2, compute signature for each series, measure correlation of signature vectors.\n",
        "    Return best shift.\n",
        "    Only works if 'esig' is installed.\n",
        "    \"\"\"\n",
        "    if not ESIG_AVAILABLE:\n",
        "        print(\"esig not installed. Returning None.\")\n",
        "        return None, None\n",
        "\n",
        "    base_t = min(df1['datetime'].min(), df2['datetime'].min())\n",
        "    idx1, arr1 = resample_to_uniform_grid(df1, base_t)\n",
        "    idx2, arr2 = resample_to_uniform_grid(df2, base_t)\n",
        "\n",
        "    # Helper to compute signature up to 'level'\n",
        "    def compute_sig(signal):\n",
        "        # shape => (len, 1) for esig\n",
        "        path = signal.reshape(-1,1)\n",
        "        sig = tosig.stream2sig(path, level)\n",
        "        return sig\n",
        "\n",
        "    best_lag, best_corr = 0, -999\n",
        "\n",
        "    for lag in range(-max_lag_ms, max_lag_ms+1, step_ms):\n",
        "        if lag > 0:\n",
        "            a1 = arr1[lag:]\n",
        "            a2 = arr2[:len(a1)]\n",
        "        elif lag < 0:\n",
        "            l_ = abs(lag)\n",
        "            a1 = arr1[:len(arr1)-l_]\n",
        "            a2 = arr2[l_:l_+len(a1)]\n",
        "        else:\n",
        "            a1 = arr1\n",
        "            a2 = arr2\n",
        "\n",
        "        if len(a1) < 2 or len(a2) < 2:\n",
        "            continue\n",
        "\n",
        "        s1 = compute_sig(a1)\n",
        "        s2 = compute_sig(a2)\n",
        "        # measure correlation of signature vectors (use Pearson or cos-sim)\n",
        "        if np.std(s1) < 1e-15 or np.std(s2) < 1e-15:\n",
        "            continue\n",
        "        c = np.corrcoef(s1, s2)[0,1]\n",
        "        if abs(c) > abs(best_corr):\n",
        "            best_corr = c\n",
        "            best_lag = lag\n",
        "\n",
        "    return best_lag, best_corr\n",
        "\n",
        "###############################################################################\n",
        "# 9) DEMO\n",
        "###############################################################################\n",
        "\n",
        "def main():\n",
        "    # A) Create real data\n",
        "    df_real = create_real_data(n=100, seed=42)\n",
        "    # B) Create synthetic referencing real subgroups\n",
        "    df_synth = generate_synthetic_data2(n=80, reference_df=df_real, use_features_from_reference=True, seed=123)\n",
        "\n",
        "    # Choose transformations (e.g. log_returns)\n",
        "    df_real_trans  = transform_series(df_real, transform='log_returns')\n",
        "    df_synth_trans = transform_series(df_synth, transform='log_returns')\n",
        "\n",
        "    # CROSS-CORRELATION\n",
        "    best_lag_cc, best_corr_cc = find_lead_lag_crosscorr_time(df_real_trans, df_synth_trans,\n",
        "                                                             max_lag_ms=2000, step_ms=100)\n",
        "    print(f\"[CrossCorr] best_lag={best_lag_cc} ms, corr={best_corr_cc:.4f}\")\n",
        "\n",
        "    # DTW\n",
        "    if DTW_AVAILABLE:\n",
        "        best_lag_dtw, best_dist_dtw = find_lead_lag_dtw_time(df_real_trans, df_synth_trans,\n",
        "                                                             max_lag_ms=2000, step_ms=100)\n",
        "        print(f\"[DTW] best_lag={best_lag_dtw} ms, distance={best_dist_dtw:.4f}\")\n",
        "\n",
        "    # Hayashi-Yoshida\n",
        "    best_lag_hy, best_corr_hy = find_lead_lag_hy_time(df_real_trans, df_synth_trans,\n",
        "                                                      max_lag_ms=2000, step_ms=100)\n",
        "    print(f\"[HY] best_lag={best_lag_hy} ms, corr={best_corr_hy:.4f}\")\n",
        "\n",
        "    # Wavelet\n",
        "    if PYWT_AVAILABLE:\n",
        "        best_lag_wave, best_corr_wave = find_lead_lag_wavelet_time(df_real_trans, df_synth_trans,\n",
        "                                                                   max_lag_ms=2000, step_ms=100,\n",
        "                                                                   wavelet='morl', scales=range(1,6))\n",
        "        print(f\"[Wavelet] best_lag={best_lag_wave} ms, corr={best_corr_wave:.4f}\")\n",
        "\n",
        "    # Signature\n",
        "    if ESIG_AVAILABLE:\n",
        "        best_lag_sig, best_corr_sig = find_lead_lag_signature_time(df_real_trans, df_synth_trans,\n",
        "                                                                   max_lag_ms=2000, step_ms=100, level=2)\n",
        "        print(f\"[Signature] best_lag={best_lag_sig} ms, corr={best_corr_sig:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5ni6l95RbqF",
        "outputId": "e3c825c8-f718-45b2-91c6-2c24047b84ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-e01adb05829d>:174: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  s = s.fillna(method='ffill').fillna(0)\n",
            "<ipython-input-4-e01adb05829d>:174: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  s = s.fillna(method='ffill').fillna(0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CrossCorr] best_lag=0 ms, corr=-999.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-e01adb05829d>:174: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  s = s.fillna(method='ffill').fillna(0)\n",
            "<ipython-input-4-e01adb05829d>:174: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  s = s.fillna(method='ffill').fillna(0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "umrxQWJNexdM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparison of Lead–Lag Models\n",
        "\n",
        "Below is a high-level comparison of multiple methods for estimating lead–lag relationships (in milliseconds) between two time series. Each method has different strengths in terms of performance, noise-robustness, and ability to handle asynchronous data. We also provide references to two relevant papers and links to packages that implement or support these methods.\n",
        "\n",
        "1. Cross-Correlation\n",
        "\n",
        "Overview\n",
        "\t•\tA classical measure that shifts one series in time (positive or negative lag) and computes the Pearson correlation at each shift.\n",
        "\t•\tIdentifies the shift (lag) that maximizes correlation (or minimizes negative correlation).\n",
        "\n",
        "Performance & Noise\n",
        "\t•\tRelatively fast (simple correlation operations).\n",
        "\t•\tLinear measure, so outliers and heavy noise can reduce effectiveness; pre-filtering often helps.\n",
        "\n",
        "Asynchronous Data\n",
        "\t•\tRequires resampling both time series onto a common time grid. Loss of fine-grained detail if data is highly asynchronous.\n",
        "\n",
        "Relevant Package\n",
        "\t•\tPart of standard libraries: NumPy/numpy.correlate or SciPy/scipy.signal.correlate.\n",
        "\n",
        "2. Dynamic Time Warping (DTW)\n",
        "\n",
        "Overview\n",
        "\t•\tAligns two time series by warping the time axis to minimize overall distance.\n",
        "\t•\tUnlike cross-correlation, DTW can handle local misalignments.\n",
        "\n",
        "Performance & Noise\n",
        "\t•\tSlower than cross-correlation, especially for large datasets (DTW is ￼ unless using advanced optimizations).\n",
        "\t•\tCan handle moderate levels of noise, but strong outliers might distort the distance measure; options like a Sakoe–Chiba band can improve robustness.\n",
        "\n",
        "Asynchronous Data\n",
        "\t•\tNaturally handles asynchronous or non-uniformly sampled signals by warping one series to the other.\n",
        "\n",
        "Relevant Package\n",
        "\t•\tdtaidistance on PyPI implements DTW in Python (including acceleration and visualization).\n",
        "\n",
        "3. Hayashi–Yoshida (HY)\n",
        "\n",
        "Overview\n",
        "\t•\tA finance-oriented approach that estimates integrated covariance or correlation for asynchronous time series.\n",
        "\t•\tSummation of overlapping increments captures how two assets co-move at high frequency without strict alignment.\n",
        "\n",
        "Performance & Noise\n",
        "\t•\tModerate computational effort in naive form (￼ for increment overlaps).\n",
        "\t•\tDesigned for stochastic processes in finance, so it’s somewhat robust to market microstructure noise, but large outliers still impact the increment sums.\n",
        "\n",
        "Asynchronous Data\n",
        "\t•\tExcellent: The HY method was specifically developed to handle irregular, tick-by-tick (asynchronous) data without resampling.\n",
        "\n",
        "Relevant Package\n",
        "\t•\tTypically implemented as custom code. See hyCov package (R) for a partial reference (though it’s not identical). Python versions are often user-coded.\n",
        "\n",
        "4. Wavelet Approach\n",
        "\n",
        "Overview\n",
        "\t•\tTransforms each time series into wavelet coefficients (multi-scale features).\n",
        "\t•\tMeasures correlation (or distance) of these coefficients across time shifts.\n",
        "\n",
        "Performance & Noise\n",
        "\t•\tWavelets can handle nonstationarity and multi-scale noise well.\n",
        "\t•\tComputational cost depends on transform length and the wavelet scales used. Typically faster than DTW for large data.\n",
        "\n",
        "Asynchronous Data\n",
        "\t•\tStill requires time alignment or resampling to a common grid.\n",
        "\t•\tWavelet transforms can handle missing data (with interpolation or partial reconstruction), but not inherently an asynchronous method.\n",
        "\n",
        "Relevant Package\n",
        "\t•\tPyWavelets is the standard Python library for wavelet transforms.\n",
        "\n",
        "5. Signature (Rough Paths)\n",
        "\n",
        "Overview\n",
        "\t•\tUses rough path theory to encode time series as signatures (iterated integrals).\n",
        "\t•\tThe lead–lag can be inferred by shifting one series and measuring some distance or correlation between their signature vectors.\n",
        "\n",
        "Performance & Noise\n",
        "\t•\tSignatures can capture complex path-dependent features, somewhat robust to moderate noise.\n",
        "\t•\tComputation grows exponentially in dimension/level, but for 1D or small dimension at low signature levels, it’s manageable.\n",
        "\n",
        "Asynchronous Data\n",
        "\t•\tIn principle, signatures can handle asynchronous time series by carefully constructing the piecewise or step path.\n",
        "\t•\tTypically requires user code or a library (like esig) to handle lead–lag transformations.\n",
        "\n",
        "Relevant Package\n",
        "\t•\tesig (Github) is a Python library for signatures.\n",
        "\t•\tFor an introduction, see this short doc.\n",
        "\n",
        "Papers & References\n",
        "\t1.\tSakoe, H., & Chiba, S. (1978). Dynamic programming algorithm optimization for spoken word recognition. IEEE Transactions on Acoustics, Speech, and Signal Processing, 26(1), 43–49.\n",
        "\t•\tClassic foundation for DTW-based time series alignment.\n",
        "\t2.\tHayashi, T., & Yoshida, N. (2005). On covariation estimation of non-synchronously observed diffusion processes. Bernoulli, 11(2), 359–379.\n",
        "\t•\tSeminal paper for the Hayashi–Yoshida estimator of covariance with asynchronous data in finance.\n",
        "\n",
        "(For wavelets, see also Daubechies, I. (1992). Ten Lectures on Wavelets. SIAM, which introduced the standard wavelet frameworks. For signatures, see Lyons, T. (1998). Differential equations driven by rough paths, in Ecole d’été de Probabilités de Saint-Flour. )\n",
        "\n",
        "Summary\n",
        "\t•\tCross-Correlation is fast and straightforward but can struggle if data is asynchronous unless carefully resampled.\n",
        "\t•\tDTW robustly aligns local time differences, good for moderate noise, but can be slow (￼) for large ￼.\n",
        "\t•\tHayashi–Yoshida is the go-to for finance tick data, elegantly handles asynchrony but can be ￼ in naive form.\n",
        "\t•\tWavelet approaches decompose data across multiple scales, handle nonstationarity, but still need time alignment.\n",
        "\t•\tSignature expansions (rough paths) capture nonlinear path features, can handle asynchrony by a lead–lag construction, but are more specialized and can grow in complexity.\n",
        "\n",
        "Each method has trade-offs in performance, noise handling, and asynchrony support. The choice often depends on the domain, data frequency, and the desired computational budget."
      ],
      "metadata": {
        "id": "Yp9rKbYNeyfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_synthetic_data2(n=100,\n",
        "                            reference_df=None,\n",
        "                            use_features_from_reference=True,\n",
        "                            seed=999):\n",
        "    \"\"\"\n",
        "    Create a synthetic dataset with the same columns.\n",
        "    We only group by ['symbol','side','orderType'] so the group key is a 3-tuple.\n",
        "    We do NOT group by data1 or data2, but we DO store them from the chosen reference row.\n",
        "\n",
        "    Avoids the mismatch in shape that causes:\n",
        "        ValueError: must supply a tuple to get_group with multiple grouping keys\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # We'll produce random ascending datetimes for synthetic\n",
        "    start = pd.Timestamp(\"2023-01-02 09:30:00\")\n",
        "    random_gaps = np.random.exponential(scale=25, size=n).cumsum()\n",
        "    synthetic_times = start + pd.to_timedelta(random_gaps, unit='s')\n",
        "\n",
        "    if (reference_df is None) or (not use_features_from_reference):\n",
        "        # Unconditional\n",
        "        symbols = np.random.choice(['AAPL','GOOG','TSLA'], size=n)\n",
        "        price = 100 + np.cumsum(np.random.randn(n))\n",
        "        quantity = np.random.randint(1, 500, size=n)\n",
        "        side = np.random.choice(['buy','sell'], size=n)\n",
        "        order_type = np.random.choice(['market','limit','stop'], size=n, p=[0.5,0.4,0.1])\n",
        "        data1 = np.random.normal(loc=50, scale=10, size=n)\n",
        "        data2 = np.random.normal(loc=100, scale=20, size=n)\n",
        "\n",
        "        df_synth = pd.DataFrame({\n",
        "            'datetime': synthetic_times,\n",
        "            'symbol': symbols,\n",
        "            'price': price,\n",
        "            'quantity': quantity,\n",
        "            'side': side,\n",
        "            'orderType': order_type,\n",
        "            'data1': data1,\n",
        "            'data2': data2\n",
        "        }).sort_values('datetime')\n",
        "        return df_synth\n",
        "\n",
        "    else:\n",
        "        # Group only by these 3 columns => each group_key is a 3-tuple\n",
        "        groups = reference_df.groupby(['symbol','side','orderType'])\n",
        "        group_keys = list(groups.groups.keys())  # Keep group_keys as a list of tuples\n",
        "\n",
        "        group_sizes = [len(groups.get_group(k)) for k in group_keys]\n",
        "        total = sum(group_sizes)\n",
        "        probs = [gs/total for gs in group_sizes]\n",
        "\n",
        "        rows = []\n",
        "        for i in range(n):\n",
        "            dt = synthetic_times[i]\n",
        "            # Sample from the range of group indices\n",
        "            choice_index = np.random.choice(range(len(group_keys)), p=probs)\n",
        "            # Get the actual group key using the sampled index\n",
        "            choice_key = group_keys[choice_index]\n",
        "\n",
        "            sub_df = groups.get_group(choice_key)\n",
        "            # sample 1 row from that subgroup\n",
        "            row_ref = sub_df.sample(n=1).iloc[0]\n",
        "            # copy relevant fields from that row\n",
        "            new_row = {\n",
        "                'datetime': dt,\n",
        "                'symbol': row_ref['symbol'],\n",
        "                'price': row_ref['price'],\n",
        "                'quantity': row_ref['quantity'],\n",
        "                'side': row_ref['side'],\n",
        "                'orderType': row_ref['orderType'],\n",
        "                'data1': row_ref['data1'],  # we store the same data1, or random from subgroup\n",
        "                'data2': row_ref['data2']\n",
        "            }\n",
        "            rows.append(new_row)\n",
        "\n",
        "        df_synth = pd.DataFrame(rows).sort_values('datetime')\n",
        "        return df_synth"
      ],
      "metadata": {
        "id": "yM-8bpNfQZyv"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}