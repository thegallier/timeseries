{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIqO4PSAb2htNIYLwjnmax",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thegallier/timeseries/blob/main/lead_lag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install esig\n",
        "!pip install pywt\n",
        "!pip install dtaidistance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpzcZo32MAVG",
        "outputId": "d2abb9cc-ffc1-4262-af3e-106cecb20e45"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: esig in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from esig) (2.2.2)\n",
            "Requirement already satisfied: pyrecombine in /usr/local/lib/python3.11/dist-packages (from esig) (1.0.0)\n",
            "Requirement already satisfied: roughpy in /usr/local/lib/python3.11/dist-packages (from esig) (0.2.0)\n",
            "Requirement already satisfied: intel_openmp in /usr/local/lib/python3.11/dist-packages (from pyrecombine->esig) (2025.0.4)\n",
            "Requirement already satisfied: intel-cmplr-lib-ur==2025.0.4 in /usr/local/lib/python3.11/dist-packages (from intel_openmp->pyrecombine->esig) (2025.0.4)\n",
            "Requirement already satisfied: umf==0.9.* in /usr/local/lib/python3.11/dist-packages (from intel-cmplr-lib-ur==2025.0.4->intel_openmp->pyrecombine->esig) (0.9.1)\n",
            "Requirement already satisfied: tcmlib==1.2 in /usr/local/lib/python3.11/dist-packages (from umf==0.9.*->intel-cmplr-lib-ur==2025.0.4->intel_openmp->pyrecombine->esig) (1.2.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement pywt (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for pywt\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting dtaidistance\n",
            "  Downloading dtaidistance-2.3.12.tar.gz (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from dtaidistance) (2.2.2)\n",
            "Building wheels for collected packages: dtaidistance\n",
            "  Building wheel for dtaidistance (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dtaidistance: filename=dtaidistance-2.3.12-cp311-cp311-linux_x86_64.whl size=2628515 sha256=cb1bce401d036b53dd91fb98d3fb14b4926a3df5a8f531864f81e90404305a16\n",
            "  Stored in directory: /root/.cache/pip/wheels/d9/3e/79/f71e79f61d63670c1f7cf9df3e474d9d9ccb35e56b547f8896\n",
            "Successfully built dtaidistance\n",
            "Installing collected packages: dtaidistance\n",
            "Successfully installed dtaidistance-2.3.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.signal import correlate\n",
        "\n",
        "###############################################################################\n",
        "# 1) Real Data Creation\n",
        "###############################################################################\n",
        "\n",
        "def create_real_data(n=200, seed=42):\n",
        "    \"\"\"\n",
        "    Creates an asynchronous orders DataFrame with columns:\n",
        "      [datetime, symbol, price, quantity, side, orderType, data1, data2]\n",
        "\n",
        "    'data1' and 'data2' are additional numeric fields for demonstration.\n",
        "    We do NOT group by them. We'll group only by (symbol, side, orderType).\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Random exponential interarrival times\n",
        "    start = pd.Timestamp(\"2023-01-01 09:30:00\")\n",
        "    random_gaps = np.random.exponential(scale=30, size=n).cumsum()  # ~30s average gap\n",
        "    datetimes = start + pd.to_timedelta(random_gaps, unit='s')\n",
        "\n",
        "    # Symbol\n",
        "    symbols = np.random.choice(['AAPL','GOOG','TSLA'], size=n, p=[0.4,0.3,0.3])\n",
        "    # Price as random walk\n",
        "    price = 100 + np.cumsum(np.random.randn(n))\n",
        "    # Quantity\n",
        "    quantity = np.random.randint(1, 500, size=n)\n",
        "    # Side\n",
        "    side = np.random.choice(['buy','sell'], size=n)\n",
        "    # orderType\n",
        "    order_type = np.random.choice(['market','limit','stop'], size=n, p=[0.5,0.4,0.1])\n",
        "    # data1, data2 (extra numeric columns)\n",
        "    data1 = np.random.normal(loc=50, scale=10, size=n)\n",
        "    data2 = np.random.normal(loc=100, scale=20, size=n)\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        'datetime': datetimes,\n",
        "        'symbol': symbols,\n",
        "        'price': price,\n",
        "        'quantity': quantity,\n",
        "        'side': side,\n",
        "        'orderType': order_type,\n",
        "        'data1': data1,\n",
        "        'data2': data2\n",
        "    }).sort_values('datetime')\n",
        "    return df\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# 2) Synthetic Data Generation (Grouping only by symbol, side, orderType)\n",
        "###############################################################################\n",
        "\n",
        "def generate_synthetic_data(n=100,\n",
        "                            reference_df=None,\n",
        "                            use_features_from_reference=True,\n",
        "                            seed=999):\n",
        "    \"\"\"\n",
        "    Create a synthetic dataset with the same columns.\n",
        "    We only group by ['symbol','side','orderType'] so the group key is a 3-tuple.\n",
        "    We do NOT group by data1 or data2, but we DO store them from the chosen reference row.\n",
        "\n",
        "    Avoids the mismatch in shape that causes:\n",
        "        ValueError: must supply a tuple to get_group with multiple grouping keys\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # We'll produce random ascending datetimes for synthetic\n",
        "    start = pd.Timestamp(\"2023-01-02 09:30:00\")\n",
        "    random_gaps = np.random.exponential(scale=25, size=n).cumsum()\n",
        "    synthetic_times = start + pd.to_timedelta(random_gaps, unit='s')\n",
        "\n",
        "    if (reference_df is None) or (not use_features_from_reference):\n",
        "        # Unconditional\n",
        "        symbols = np.random.choice(['AAPL','GOOG','TSLA'], size=n)\n",
        "        price = 100 + np.cumsum(np.random.randn(n))\n",
        "        quantity = np.random.randint(1, 500, size=n)\n",
        "        side = np.random.choice(['buy','sell'], size=n)\n",
        "        order_type = np.random.choice(['market','limit','stop'], size=n, p=[0.5,0.4,0.1])\n",
        "        data1 = np.random.normal(loc=50, scale=10, size=n)\n",
        "        data2 = np.random.normal(loc=100, scale=20, size=n)\n",
        "\n",
        "        df_synth = pd.DataFrame({\n",
        "            'datetime': synthetic_times,\n",
        "            'symbol': symbols,\n",
        "            'price': price,\n",
        "            'quantity': quantity,\n",
        "            'side': side,\n",
        "            'orderType': order_type,\n",
        "            'data1': data1,\n",
        "            'data2': data2\n",
        "        }).sort_values('datetime')\n",
        "        return df_synth\n",
        "\n",
        "    else:\n",
        "        # Group only by these 3 columns => each group_key is a 3-tuple\n",
        "        groups = reference_df.groupby(['symbol','side','orderType'])\n",
        "        group_keys = list(groups.groups.keys())\n",
        "        # Must be object array, each element is a 3-tuple\n",
        "        group_keys = np.array(group_keys, dtype=object)\n",
        "\n",
        "        group_sizes = [len(groups.get_group(k)) for k in group_keys]\n",
        "        total = sum(group_sizes)\n",
        "        probs = [gs/total for gs in group_sizes]\n",
        "\n",
        "        rows = []\n",
        "        for i in range(n):\n",
        "            dt = synthetic_times[i]\n",
        "            choice_key = np.random.choice(group_keys, p=probs)\n",
        "            # e.g. choice_key = ('AAPL','buy','limit'), a 3-tuple\n",
        "            sub_df = groups.get_group(choice_key)\n",
        "            # sample 1 row from that subgroup\n",
        "            row_ref = sub_df.sample(n=1).iloc[0]\n",
        "            # copy relevant fields from that row\n",
        "            new_row = {\n",
        "                'datetime': dt,\n",
        "                'symbol': row_ref['symbol'],\n",
        "                'price': row_ref['price'],\n",
        "                'quantity': row_ref['quantity'],\n",
        "                'side': row_ref['side'],\n",
        "                'orderType': row_ref['orderType'],\n",
        "                'data1': row_ref['data1'],  # we store the same data1, or random from subgroup\n",
        "                'data2': row_ref['data2']\n",
        "            }\n",
        "            rows.append(new_row)\n",
        "\n",
        "        df_synth = pd.DataFrame(rows).sort_values('datetime')\n",
        "        return df_synth\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# 3) Example transforms and cross-correlation\n",
        "###############################################################################\n",
        "\n",
        "def transform_series(df, transform='returns', price_col='price'):\n",
        "    \"\"\"\n",
        "    We'll produce 'value' column. Options: 'returns', 'log_returns', 'diff', 'none'\n",
        "    Sort by datetime first (asynchronous).\n",
        "    \"\"\"\n",
        "    df = df.sort_values('datetime').copy()\n",
        "    if transform == 'returns':\n",
        "        df['value'] = df[price_col].pct_change().fillna(0)\n",
        "    elif transform == 'log_returns':\n",
        "        df['value'] = np.log(df[price_col]).diff().fillna(0)\n",
        "    elif transform == 'diff':\n",
        "        df['value'] = df[price_col].diff().fillna(0)\n",
        "    else:\n",
        "        # no transform\n",
        "        df['value'] = df[price_col]\n",
        "    return df[['datetime','value']]\n",
        "\n",
        "\n",
        "def cross_correlation_method(returns1, returns2, max_lag=20):\n",
        "    \"\"\"\n",
        "    Row-based cross-correlation from your snippet\n",
        "    (NOT time-based; best_lag is # of array elements).\n",
        "    \"\"\"\n",
        "    cross_corr = correlate(returns1, returns2, mode='full')\n",
        "    lags = np.arange(-len(returns1)+1, len(returns1))\n",
        "    best_lag = lags[np.argmax(np.abs(cross_corr))]\n",
        "    if abs(best_lag) > max_lag:\n",
        "        best_lag = max_lag * np.sign(best_lag)\n",
        "    return best_lag\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# DEMO\n",
        "###############################################################################\n",
        "\n",
        "def main():\n",
        "    # 1) Create real data\n",
        "    df_real = create_real_data(n=50, seed=42)\n",
        "    print(\"Real data example:\")\n",
        "    print(df_real.head(5))\n",
        "\n",
        "    # 2) Create synthetic data referencing real groups\n",
        "    df_synth = generate_synthetic_data2(n=30, reference_df=df_real, use_features_from_reference=True, seed=123)\n",
        "    print(\"\\nSynthetic data example:\")\n",
        "    print(df_synth.head(5))\n",
        "\n",
        "    # 3) Transform both => log_returns\n",
        "    real_trans  = transform_series(df_real, transform='log_returns')\n",
        "    synth_trans = transform_series(df_synth, transform='log_returns')\n",
        "\n",
        "    # 4) Row-based cross-correlation on 'value' arrays\n",
        "    arr_real  = real_trans['value'].values\n",
        "    arr_synth = synth_trans['value'].values\n",
        "    min_len = min(len(arr_real), len(arr_synth))\n",
        "    best_lag_rows = cross_correlation_method(arr_real[:min_len], arr_synth[:min_len], max_lag=10)\n",
        "    print(f\"\\nRow-based cross_correlation_method => best_lag_rows={best_lag_rows}\")\n",
        "\n",
        "if __name__=='__main__':\n",
        "    main()\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNbA4gX8PYwP",
        "outputId": "75ea260c-2a4f-45a9-d639-83c0dd661334"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Real data example:\n",
            "                       datetime symbol       price  quantity  side orderType  \\\n",
            "0 2023-01-01 09:30:14.078042699   TSLA  100.087047       240  sell    market   \n",
            "1 2023-01-01 09:31:44.381685627   TSLA   99.788040       144  sell    market   \n",
            "2 2023-01-01 09:32:23.884056433   TSLA   99.879800        97   buy    market   \n",
            "3 2023-01-01 09:32:51.272333046   TSLA   97.892232       201   buy     limit   \n",
            "4 2023-01-01 09:32:56.361079160   GOOG   97.672560       124  sell     limit   \n",
            "\n",
            "       data1       data2  \n",
            "0  54.497740  113.981848  \n",
            "1  67.789243  111.997933  \n",
            "2  43.330566   90.375037  \n",
            "3  33.081817   58.124120  \n",
            "4  56.567973   91.599051  \n",
            "\n",
            "Synthetic data example:\n",
            "                       datetime symbol      price  quantity  side orderType  \\\n",
            "0 2023-01-02 09:30:29.806803587   AAPL  96.994863       441   buy     limit   \n",
            "1 2023-01-02 09:30:38.233490654   AAPL  98.989296       464   buy     limit   \n",
            "2 2023-01-02 09:30:44.665592657   GOOG  98.181745         2   buy    market   \n",
            "3 2023-01-02 09:31:04.701434702   GOOG  99.972428       346  sell    market   \n",
            "4 2023-01-02 09:31:36.478208089   AAPL  98.550605       148   buy     limit   \n",
            "\n",
            "       data1       data2  \n",
            "0  42.673078   75.235383  \n",
            "1  54.949957  115.118706  \n",
            "2  52.289704   89.045530  \n",
            "3  45.449668  136.179137  \n",
            "4  30.084622  108.925787  \n",
            "\n",
            "Row-based cross_correlation_method => best_lag_rows=-10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Optional libraries:\n",
        "try:\n",
        "    from dtaidistance import dtw  # for DTW\n",
        "    DTW_AVAILABLE = True\n",
        "except ImportError:\n",
        "    DTW_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    import pywt  # for wavelets\n",
        "    PYWT_AVAILABLE = True\n",
        "except ImportError:\n",
        "    PYWT_AVAILABLE = False\n",
        "\n",
        "# try:\n",
        "#     from esig import tosig  # for signatures\n",
        "#     ESIG_AVAILABLE = True\n",
        "# except ImportError:\n",
        "ESIG_AVAILABLE = False\n",
        "\n",
        "###############################################################################\n",
        "# 1) CORRECTED DATA GENERATION\n",
        "###############################################################################\n",
        "\n",
        "def create_real_data(n=200, seed=42):\n",
        "    \"\"\"\n",
        "    Creates asynchronous orders with columns:\n",
        "      [datetime, symbol, price, quantity, side, orderType, data1, data2].\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Random exponential interarrival times\n",
        "    start = pd.Timestamp(\"2023-01-01 09:30:00\")\n",
        "    random_gaps = np.random.exponential(scale=30, size=n).cumsum()\n",
        "    datetimes = start + pd.to_timedelta(random_gaps, unit='s')\n",
        "\n",
        "    symbols = np.random.choice(['AAPL','GOOG','TSLA'], size=n, p=[0.4,0.3,0.3])\n",
        "    price = 100 + np.cumsum(np.random.randn(n))\n",
        "    quantity = np.random.randint(1, 500, size=n)\n",
        "    side = np.random.choice(['buy','sell'], size=n)\n",
        "    order_type = np.random.choice(['market','limit','stop'], size=n, p=[0.5,0.4,0.1])\n",
        "    data1 = np.random.normal(loc=50, scale=10, size=n)\n",
        "    data2 = np.random.normal(loc=100, scale=20, size=n)\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        'datetime': datetimes,\n",
        "        'symbol': symbols,\n",
        "        'price': price,\n",
        "        'quantity': quantity,\n",
        "        'side': side,\n",
        "        'orderType': order_type,\n",
        "        'data1': data1,\n",
        "        'data2': data2\n",
        "    })\n",
        "    df.sort_values('datetime', inplace=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "def generate_synthetic_data(n=100,\n",
        "                            reference_df=None,\n",
        "                            use_features_from_reference=True,\n",
        "                            seed=999):\n",
        "    \"\"\"\n",
        "    Synthetic dataset with same columns.\n",
        "    Grouping only by (symbol, side, orderType) to avoid mismatch keys.\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    start = pd.Timestamp(\"2023-01-02 09:30:00\")\n",
        "    random_gaps = np.random.exponential(scale=25, size=n).cumsum()\n",
        "    synth_times = start + pd.to_timedelta(random_gaps, unit='s')\n",
        "\n",
        "    if (reference_df is None) or (not use_features_from_reference):\n",
        "        # Unconditional\n",
        "        symbols = np.random.choice(['AAPL','GOOG','TSLA'], size=n)\n",
        "        price   = 100 + np.cumsum(np.random.randn(n))\n",
        "        qty     = np.random.randint(1, 500, size=n)\n",
        "        side    = np.random.choice(['buy','sell'], size=n)\n",
        "        otype   = np.random.choice(['market','limit','stop'], size=n, p=[0.5,0.4,0.1])\n",
        "        data1   = np.random.normal(loc=50,  scale=10,  size=n)\n",
        "        data2   = np.random.normal(loc=100, scale=20,  size=n)\n",
        "\n",
        "        df_synth = pd.DataFrame({\n",
        "            'datetime': synth_times,\n",
        "            'symbol': symbols,\n",
        "            'price': price,\n",
        "            'quantity': qty,\n",
        "            'side': side,\n",
        "            'orderType': otype,\n",
        "            'data1': data1,\n",
        "            'data2': data2\n",
        "        })\n",
        "        df_synth.sort_values('datetime', inplace=True)\n",
        "        return df_synth\n",
        "\n",
        "    else:\n",
        "        # Group only by (symbol, side, orderType)\n",
        "        groups = reference_df.groupby(['symbol','side','orderType'])\n",
        "        group_keys = list(groups.groups.keys())\n",
        "        group_keys = np.array(group_keys, dtype=object)  # must be 1D object array\n",
        "\n",
        "        group_sizes = [len(groups.get_group(k)) for k in group_keys]\n",
        "        total = sum(group_sizes)\n",
        "        probs = [gs/total for gs in group_sizes]\n",
        "\n",
        "        rows = []\n",
        "        for i in range(n):\n",
        "            dt = synth_times[i]\n",
        "            choice_key = np.random.choice(group_keys, p=probs)\n",
        "            sub_df = groups.get_group(choice_key)\n",
        "            row_ref = sub_df.sample(n=1).iloc[0]\n",
        "\n",
        "            new_row = {\n",
        "                'datetime': dt,\n",
        "                'symbol': row_ref['symbol'],\n",
        "                'price': row_ref['price'],\n",
        "                'quantity': row_ref['quantity'],\n",
        "                'side': row_ref['side'],\n",
        "                'orderType': row_ref['orderType'],\n",
        "                'data1': row_ref['data1'],\n",
        "                'data2': row_ref['data2']\n",
        "            }\n",
        "            rows.append(new_row)\n",
        "\n",
        "        df_synth = pd.DataFrame(rows)\n",
        "        df_synth.sort_values('datetime', inplace=True)\n",
        "        return df_synth\n",
        "\n",
        "###############################################################################\n",
        "# 2) HELPER: TRANSFORM SERIES\n",
        "###############################################################################\n",
        "\n",
        "def transform_series(df, transform='log_returns', price_col='price'):\n",
        "    \"\"\"\n",
        "    Create a 'value' column from 'price' using the chosen transform.\n",
        "    Options: 'returns', 'log_returns', 'diff', 'none'\n",
        "    \"\"\"\n",
        "    df = df.sort_values('datetime').copy()\n",
        "    if transform == 'returns':\n",
        "        df['value'] = df[price_col].pct_change().fillna(0)\n",
        "    elif transform == 'log_returns':\n",
        "        df['value'] = np.log(df[price_col]).diff().fillna(0)\n",
        "    elif transform == 'diff':\n",
        "        df['value'] = df[price_col].diff().fillna(0)\n",
        "    else:\n",
        "        # no transform\n",
        "        df['value'] = df[price_col]\n",
        "    return df[['datetime','value']]\n",
        "\n",
        "###############################################################################\n",
        "# 3) TIME-BASED RESAMPLING & SHIFT\n",
        "###############################################################################\n",
        "\n",
        "def resample_to_uniform_grid(df, base_time, fill_method='ffill'):\n",
        "    \"\"\"\n",
        "    Convert 'datetime' -> ms from base_time, then create a Series\n",
        "    with 1ms steps from min to max. Forward-fill missing.\n",
        "    Return the (index array, series array).\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    df['time_ms'] = (df['datetime'] - base_time).dt.total_seconds()*1000\n",
        "    t_min = int(df['time_ms'].min())\n",
        "    t_max = int(df['time_ms'].max())\n",
        "    idx = np.arange(t_min, t_max+1, 1, dtype=int)\n",
        "\n",
        "    s = pd.Series(index=idx, dtype=float)\n",
        "    for (tm,val) in zip(df['time_ms'], df['value']):\n",
        "        s.loc[int(tm)] = val\n",
        "\n",
        "    if fill_method:\n",
        "        s = s.fillna(method='ffill').fillna(0)\n",
        "    return idx, s.values  # time index array, data array\n",
        "\n",
        "###############################################################################\n",
        "# 4) CROSS-CORRELATION (TIME-BASED)\n",
        "###############################################################################\n",
        "\n",
        "def find_lead_lag_crosscorr_time(df1, df2, max_lag_ms=2000, step_ms=100):\n",
        "    \"\"\"\n",
        "    For cross-correlation in time. Shift df2 from -max_lag..+max_lag by step_ms,\n",
        "    measure Pearson correlation, pick best. Return (best_lag_ms, best_corr).\n",
        "    \"\"\"\n",
        "    # 1) find a common base_time\n",
        "    base_t = min(df1['datetime'].min(), df2['datetime'].min())\n",
        "    # 2) uniform grid\n",
        "    idx1, arr1 = resample_to_uniform_grid(df1, base_t)\n",
        "    idx2, arr2 = resample_to_uniform_grid(df2, base_t)\n",
        "\n",
        "    best_lag, best_corr = 0, -999\n",
        "\n",
        "    for lag in range(-max_lag_ms, max_lag_ms + 1, step_ms):\n",
        "        # Ensure a1 and a2 have the same length after shifting\n",
        "        len_a1 = len(arr1) - abs(lag)  # Length of a1 after potential truncation\n",
        "        len_a2 = len(arr2) - abs(lag)  # Length of a2 after potential truncation\n",
        "        min_len = min(len_a1, len_a2)  # Take the minimum length\n",
        "\n",
        "        if lag > 0:\n",
        "            a1 = arr1[lag : lag + min_len]\n",
        "            a2 = arr2[:min_len]\n",
        "        elif lag < 0:\n",
        "            a1 = arr1[:min_len]\n",
        "            a2 = arr2[abs(lag) : abs(lag) + min_len]\n",
        "        else:\n",
        "            a1 = arr1[:min_len]\n",
        "            a2 = arr2[:min_len]\n",
        "\n",
        "        if len(a1) < 2 or len(a2) < 2:\n",
        "            continue\n",
        "\n",
        "        c = np.corrcoef(a1, a2)[0, 1]\n",
        "        if abs(c) > abs(best_corr):\n",
        "            best_corr = c\n",
        "            best_lag = lag\n",
        "    return best_lag, best_corr\n",
        "\n",
        "###############################################################################\n",
        "# 5) DTW (TIME-BASED)\n",
        "###############################################################################\n",
        "\n",
        "def find_lead_lag_dtw_time(df1, df2, max_lag_ms=2000, step_ms=100):\n",
        "    \"\"\"\n",
        "    For each shift in [-max_lag_ms..+max_lag_ms], measure DTW distance\n",
        "    after uniform resampling. Return (best_lag_ms, best_dist).\n",
        "    Minimizes DTW distance.\n",
        "    \"\"\"\n",
        "    if not DTW_AVAILABLE:\n",
        "        print(\"DTW library not installed. Returning None.\")\n",
        "        return None, None\n",
        "\n",
        "    base_t = min(df1['datetime'].min(), df2['datetime'].min())\n",
        "    idx1, arr1 = resample_to_uniform_grid(df1, base_t)\n",
        "    idx2, arr2 = resample_to_uniform_grid(df2, base_t)\n",
        "\n",
        "    best_lag, best_dist = 0, 999999999\n",
        "\n",
        "    for lag in range(-max_lag_ms, max_lag_ms+1, step_ms):\n",
        "        if lag > 0:\n",
        "            a1 = arr1[lag:]\n",
        "            a2 = arr2[:len(a1)]\n",
        "        elif lag < 0:\n",
        "            l_ = abs(lag)\n",
        "            a1 = arr1[:len(arr1)-l_]\n",
        "            a2 = arr2[l_:l_+len(a1)]\n",
        "        else:\n",
        "            a1 = arr1\n",
        "            a2 = arr2\n",
        "\n",
        "        if len(a1) < 2 or len(a2) < 2:\n",
        "            continue\n",
        "\n",
        "        dist = dtw.distance(a1, a2)\n",
        "        if dist < best_dist:\n",
        "            best_dist = dist\n",
        "            best_lag = lag\n",
        "    return best_lag, best_dist\n",
        "\n",
        "###############################################################################\n",
        "# 6) HAYASHI–YOSHIDA (TIME-BASED SHIFT)\n",
        "###############################################################################\n",
        "\n",
        "def hayashi_yoshida_covar(arrA, arrB, idxA, idxB):\n",
        "    \"\"\"\n",
        "    Naive HY covariance on two signals (arrA, arrB) with time indexes in ms (idxA, idxB).\n",
        "    We'll interpret each increment in arrA, arrB.\n",
        "    This is simplistic; real HY is typically on tick data.\n",
        "    \"\"\"\n",
        "    # increments for A\n",
        "    A_incr = []\n",
        "    for i in range(1, len(arrA)):\n",
        "        dX = arrA[i] - arrA[i-1]\n",
        "        t0A, t1A = idxA[i-1], idxA[i]\n",
        "        A_incr.append((t0A, t1A, dX))\n",
        "\n",
        "    # increments for B\n",
        "    B_incr = []\n",
        "    for j in range(1, len(arrB)):\n",
        "        dY = arrB[j] - arrB[j-1]\n",
        "        t0B, t1B = idxB[j-1], idxB[j]\n",
        "        B_incr.append((t0B, t1B, dY))\n",
        "\n",
        "    hy_sum = 0.0\n",
        "    for (A0, A1, dX) in A_incr:\n",
        "        for (B0, B1, dY) in B_incr:\n",
        "            if A1 > B0 and B1 > A0:  # intervals overlap\n",
        "                hy_sum += dX * dY\n",
        "    return hy_sum\n",
        "\n",
        "def find_lead_lag_hy_time(df1, df2, max_lag_ms=2000, step_ms=100):\n",
        "    \"\"\"\n",
        "    Shift df2 in [-max_lag_ms..+max_lag_ms], compute HY correlation, find the shift with highest corr.\n",
        "    Return (best_lag_ms, best_corr).\n",
        "    \"\"\"\n",
        "    base_t = min(df1['datetime'].min(), df2['datetime'].min())\n",
        "    idx1, arr1 = resample_to_uniform_grid(df1, base_t)\n",
        "    idx2, arr2 = resample_to_uniform_grid(df2, base_t)\n",
        "\n",
        "    best_lag, best_corr = 0, -999\n",
        "\n",
        "    # We'll define a helper to compute HY correlation\n",
        "    def hy_correlation(aA, iA, aB, iB):\n",
        "        covAB = hayashi_yoshida_covar(aA, aB, iA, iB)\n",
        "        varA  = hayashi_yoshida_covar(aA, aA, iA, iA)\n",
        "        varB  = hayashi_yoshida_covar(aB, aB, iB, iB)\n",
        "        if varA <= 0 or varB <= 0:\n",
        "            return 0\n",
        "        return covAB / np.sqrt(varA*varB)\n",
        "\n",
        "    for lag in range(-max_lag_ms, max_lag_ms+1, step_ms):\n",
        "        if lag > 0:\n",
        "            a1 = arr1[lag:]\n",
        "            i1 = idx1[lag:]\n",
        "            a2 = arr2[:len(a1)]\n",
        "            i2 = idx2[:len(a1)]\n",
        "        elif lag < 0:\n",
        "            l_ = abs(lag)\n",
        "            a1 = arr1[:len(arr1)-l_]\n",
        "            i1 = idx1[:len(arr1)-l_]\n",
        "            a2 = arr2[l_:l_+len(a1)]\n",
        "            i2 = idx2[l_:l_+len(a1)]\n",
        "        else:\n",
        "            a1 = arr1\n",
        "            i1 = idx1\n",
        "            a2 = arr2\n",
        "            i2 = idx2\n",
        "\n",
        "        if len(a1) < 2 or len(a2) < 2:\n",
        "            continue\n",
        "\n",
        "        corr = hy_correlation(a1, i1, a2, i2)\n",
        "        if abs(corr) > abs(best_corr):\n",
        "            best_corr = corr\n",
        "            best_lag = lag\n",
        "\n",
        "    return best_lag, best_corr\n",
        "\n",
        "###############################################################################\n",
        "# 7) WAVELET (TIME-BASED SHIFT)\n",
        "###############################################################################\n",
        "\n",
        "def find_lead_lag_wavelet_time(df1, df2, max_lag_ms=2000, step_ms=100, wavelet='morl', scales=range(1,6)):\n",
        "    \"\"\"\n",
        "    Shift df2, compute wavelet transform of both. Then measure correlation\n",
        "    between summed wavelet coeffs. Return shift that yields highest correlation in absolute value.\n",
        "    \"\"\"\n",
        "    if not PYWT_AVAILABLE:\n",
        "        print(\"PyWavelets not installed. Returning None.\")\n",
        "        return None, None\n",
        "\n",
        "    base_t = min(df1['datetime'].min(), df2['datetime'].min())\n",
        "    idx1, arr1 = resample_to_uniform_grid(df1, base_t)\n",
        "    idx2, arr2 = resample_to_uniform_grid(df2, base_t)\n",
        "\n",
        "    def wavelet_feats(sig):\n",
        "        # compute CWT, sum abs coefficients\n",
        "        coeffs, freqs = pywt.cwt(sig, scales, wavelet)\n",
        "        # shape = (len(scales), len(sig))\n",
        "        return np.sum(np.abs(coeffs), axis=1)  # 1D array of length=len(scales)\n",
        "\n",
        "    best_lag, best_corr = 0, -999\n",
        "\n",
        "    for lag in range(-max_lag_ms, max_lag_ms+1, step_ms):\n",
        "        if lag > 0:\n",
        "            a1 = arr1[lag:]\n",
        "            a2 = arr2[:len(a1)]\n",
        "        elif lag < 0:\n",
        "            l_ = abs(lag)\n",
        "            a1 = arr1[:len(arr1)-l_]\n",
        "            a2 = arr2[l_:l_+len(a1)]\n",
        "        else:\n",
        "            a1 = arr1\n",
        "            a2 = arr2\n",
        "\n",
        "        if len(a1) < 2 or len(a2) < 2:\n",
        "            continue\n",
        "\n",
        "        w1 = wavelet_feats(a1)\n",
        "        w2 = wavelet_feats(a2)\n",
        "\n",
        "        # correlation of wavelet feature vectors ( length=len(scales) )\n",
        "        c = np.corrcoef(w1, w2)[0,1]\n",
        "        if abs(c) > abs(best_corr):\n",
        "            best_corr = c\n",
        "            best_lag = lag\n",
        "    return best_lag, best_corr\n",
        "\n",
        "###############################################################################\n",
        "# 8) SIGNATURE (TIME-BASED SHIFT)\n",
        "###############################################################################\n",
        "\n",
        "def find_lead_lag_signature_time(df1, df2, max_lag_ms=2000, step_ms=100, level=2):\n",
        "    \"\"\"\n",
        "    Shift df2, compute signature for each series, measure correlation of signature vectors.\n",
        "    Return best shift.\n",
        "    Only works if 'esig' is installed.\n",
        "    \"\"\"\n",
        "    if not ESIG_AVAILABLE:\n",
        "        print(\"esig not installed. Returning None.\")\n",
        "        return None, None\n",
        "\n",
        "    base_t = min(df1['datetime'].min(), df2['datetime'].min())\n",
        "    idx1, arr1 = resample_to_uniform_grid(df1, base_t)\n",
        "    idx2, arr2 = resample_to_uniform_grid(df2, base_t)\n",
        "\n",
        "    # Helper to compute signature up to 'level'\n",
        "    def compute_sig(signal):\n",
        "        # shape => (len, 1) for esig\n",
        "        path = signal.reshape(-1,1)\n",
        "        sig = tosig.stream2sig(path, level)\n",
        "        return sig\n",
        "\n",
        "    best_lag, best_corr = 0, -999\n",
        "\n",
        "    for lag in range(-max_lag_ms, max_lag_ms+1, step_ms):\n",
        "        if lag > 0:\n",
        "            a1 = arr1[lag:]\n",
        "            a2 = arr2[:len(a1)]\n",
        "        elif lag < 0:\n",
        "            l_ = abs(lag)\n",
        "            a1 = arr1[:len(arr1)-l_]\n",
        "            a2 = arr2[l_:l_+len(a1)]\n",
        "        else:\n",
        "            a1 = arr1\n",
        "            a2 = arr2\n",
        "\n",
        "        if len(a1) < 2 or len(a2) < 2:\n",
        "            continue\n",
        "\n",
        "        s1 = compute_sig(a1)\n",
        "        s2 = compute_sig(a2)\n",
        "        # measure correlation of signature vectors (use Pearson or cos-sim)\n",
        "        if np.std(s1) < 1e-15 or np.std(s2) < 1e-15:\n",
        "            continue\n",
        "        c = np.corrcoef(s1, s2)[0,1]\n",
        "        if abs(c) > abs(best_corr):\n",
        "            best_corr = c\n",
        "            best_lag = lag\n",
        "\n",
        "    return best_lag, best_corr\n",
        "\n",
        "###############################################################################\n",
        "# 9) DEMO\n",
        "###############################################################################\n",
        "\n",
        "def main():\n",
        "    # A) Create real data\n",
        "    df_real = create_real_data(n=100, seed=42)\n",
        "    # B) Create synthetic referencing real subgroups\n",
        "    df_synth = generate_synthetic_data2(n=80, reference_df=df_real, use_features_from_reference=True, seed=123)\n",
        "\n",
        "    # Choose transformations (e.g. log_returns)\n",
        "    df_real_trans  = transform_series(df_real, transform='log_returns')\n",
        "    df_synth_trans = transform_series(df_synth, transform='log_returns')\n",
        "\n",
        "    # CROSS-CORRELATION\n",
        "    best_lag_cc, best_corr_cc = find_lead_lag_crosscorr_time(df_real_trans, df_synth_trans,\n",
        "                                                             max_lag_ms=2000, step_ms=100)\n",
        "    print(f\"[CrossCorr] best_lag={best_lag_cc} ms, corr={best_corr_cc:.4f}\")\n",
        "\n",
        "    # DTW\n",
        "    if DTW_AVAILABLE:\n",
        "        best_lag_dtw, best_dist_dtw = find_lead_lag_dtw_time(df_real_trans, df_synth_trans,\n",
        "                                                             max_lag_ms=2000, step_ms=100)\n",
        "        print(f\"[DTW] best_lag={best_lag_dtw} ms, distance={best_dist_dtw:.4f}\")\n",
        "\n",
        "    # Hayashi-Yoshida\n",
        "    best_lag_hy, best_corr_hy = find_lead_lag_hy_time(df_real_trans, df_synth_trans,\n",
        "                                                      max_lag_ms=2000, step_ms=100)\n",
        "    print(f\"[HY] best_lag={best_lag_hy} ms, corr={best_corr_hy:.4f}\")\n",
        "\n",
        "    # Wavelet\n",
        "    if PYWT_AVAILABLE:\n",
        "        best_lag_wave, best_corr_wave = find_lead_lag_wavelet_time(df_real_trans, df_synth_trans,\n",
        "                                                                   max_lag_ms=2000, step_ms=100,\n",
        "                                                                   wavelet='morl', scales=range(1,6))\n",
        "        print(f\"[Wavelet] best_lag={best_lag_wave} ms, corr={best_corr_wave:.4f}\")\n",
        "\n",
        "    # Signature\n",
        "    if ESIG_AVAILABLE:\n",
        "        best_lag_sig, best_corr_sig = find_lead_lag_signature_time(df_real_trans, df_synth_trans,\n",
        "                                                                   max_lag_ms=2000, step_ms=100, level=2)\n",
        "        print(f\"[Signature] best_lag={best_lag_sig} ms, corr={best_corr_sig:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5ni6l95RbqF",
        "outputId": "cb94806f-e2e8-477b-a35e-2116fd88fdef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-03fda98646b9>:174: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  s = s.fillna(method='ffill').fillna(0)\n",
            "<ipython-input-18-03fda98646b9>:174: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  s = s.fillna(method='ffill').fillna(0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CrossCorr] best_lag=0 ms, corr=-999.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-03fda98646b9>:174: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  s = s.fillna(method='ffill').fillna(0)\n",
            "<ipython-input-18-03fda98646b9>:174: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  s = s.fillna(method='ffill').fillna(0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_synthetic_data2(n=100,\n",
        "                            reference_df=None,\n",
        "                            use_features_from_reference=True,\n",
        "                            seed=999):\n",
        "    \"\"\"\n",
        "    Create a synthetic dataset with the same columns.\n",
        "    We only group by ['symbol','side','orderType'] so the group key is a 3-tuple.\n",
        "    We do NOT group by data1 or data2, but we DO store them from the chosen reference row.\n",
        "\n",
        "    Avoids the mismatch in shape that causes:\n",
        "        ValueError: must supply a tuple to get_group with multiple grouping keys\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # We'll produce random ascending datetimes for synthetic\n",
        "    start = pd.Timestamp(\"2023-01-02 09:30:00\")\n",
        "    random_gaps = np.random.exponential(scale=25, size=n).cumsum()\n",
        "    synthetic_times = start + pd.to_timedelta(random_gaps, unit='s')\n",
        "\n",
        "    if (reference_df is None) or (not use_features_from_reference):\n",
        "        # Unconditional\n",
        "        symbols = np.random.choice(['AAPL','GOOG','TSLA'], size=n)\n",
        "        price = 100 + np.cumsum(np.random.randn(n))\n",
        "        quantity = np.random.randint(1, 500, size=n)\n",
        "        side = np.random.choice(['buy','sell'], size=n)\n",
        "        order_type = np.random.choice(['market','limit','stop'], size=n, p=[0.5,0.4,0.1])\n",
        "        data1 = np.random.normal(loc=50, scale=10, size=n)\n",
        "        data2 = np.random.normal(loc=100, scale=20, size=n)\n",
        "\n",
        "        df_synth = pd.DataFrame({\n",
        "            'datetime': synthetic_times,\n",
        "            'symbol': symbols,\n",
        "            'price': price,\n",
        "            'quantity': quantity,\n",
        "            'side': side,\n",
        "            'orderType': order_type,\n",
        "            'data1': data1,\n",
        "            'data2': data2\n",
        "        }).sort_values('datetime')\n",
        "        return df_synth\n",
        "\n",
        "    else:\n",
        "        # Group only by these 3 columns => each group_key is a 3-tuple\n",
        "        groups = reference_df.groupby(['symbol','side','orderType'])\n",
        "        group_keys = list(groups.groups.keys())  # Keep group_keys as a list of tuples\n",
        "\n",
        "        group_sizes = [len(groups.get_group(k)) for k in group_keys]\n",
        "        total = sum(group_sizes)\n",
        "        probs = [gs/total for gs in group_sizes]\n",
        "\n",
        "        rows = []\n",
        "        for i in range(n):\n",
        "            dt = synthetic_times[i]\n",
        "            # Sample from the range of group indices\n",
        "            choice_index = np.random.choice(range(len(group_keys)), p=probs)\n",
        "            # Get the actual group key using the sampled index\n",
        "            choice_key = group_keys[choice_index]\n",
        "\n",
        "            sub_df = groups.get_group(choice_key)\n",
        "            # sample 1 row from that subgroup\n",
        "            row_ref = sub_df.sample(n=1).iloc[0]\n",
        "            # copy relevant fields from that row\n",
        "            new_row = {\n",
        "                'datetime': dt,\n",
        "                'symbol': row_ref['symbol'],\n",
        "                'price': row_ref['price'],\n",
        "                'quantity': row_ref['quantity'],\n",
        "                'side': row_ref['side'],\n",
        "                'orderType': row_ref['orderType'],\n",
        "                'data1': row_ref['data1'],  # we store the same data1, or random from subgroup\n",
        "                'data2': row_ref['data2']\n",
        "            }\n",
        "            rows.append(new_row)\n",
        "\n",
        "        df_synth = pd.DataFrame(rows).sort_values('datetime')\n",
        "        return df_synth"
      ],
      "metadata": {
        "id": "yM-8bpNfQZyv"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}