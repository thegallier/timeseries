{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNcQJZVwQUY4bY38zjq0fX3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thegallier/timeseries/blob/main/timeseries2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the Dataset\n",
        "class MaskedOrderDataset(Dataset):\n",
        "    def __init__(self, num_samples, mask_prob=0.15, max_seq_len=50):\n",
        "        self.num_samples = num_samples\n",
        "        self.N_sec_ids = 100  # Number of unique securities\n",
        "        self.N_distance_bins = 10  # Number of distance bins\n",
        "        self.mask_prob = mask_prob\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "        # Generate synthetic data\n",
        "        self.timestamps = torch.randint(0, 1000, (num_samples,))\n",
        "        self.security_ids = torch.randint(0, self.N_sec_ids, (num_samples,))\n",
        "        self.buy_sell = torch.randint(0, 2, (num_samples,))  # 0: buy, 1: sell\n",
        "        self.add_modify_delete = torch.randint(0, 3, (num_samples,))  # 0: add, 1: modify, 2: delete\n",
        "        self.quantity = torch.randint(1, 1000, (num_samples,)).float()\n",
        "        self.price = torch.rand(num_samples) * 100  # Prices between 0 and 100\n",
        "        self.distance = torch.randint(0, self.N_distance_bins, (num_samples,))\n",
        "\n",
        "        # Sort the data by timestamps (ascending)\n",
        "        sorted_indices = torch.argsort(self.timestamps)\n",
        "        self.timestamps = self.timestamps[sorted_indices]\n",
        "        self.security_ids = self.security_ids[sorted_indices]\n",
        "        self.buy_sell = self.buy_sell[sorted_indices]\n",
        "        self.add_modify_delete = self.add_modify_delete[sorted_indices]\n",
        "        self.quantity = self.quantity[sorted_indices]\n",
        "        self.price = self.price[sorted_indices]\n",
        "        self.distance = self.distance[sorted_indices]\n",
        "\n",
        "        # Define mask indices\n",
        "        self.security_id_mask = self.N_sec_ids\n",
        "        self.buy_sell_mask = 2\n",
        "        self.add_modify_delete_mask = 3\n",
        "        self.distance_mask = self.N_distance_bins\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get sequence from idx to end, up to max_seq_len\n",
        "        end_idx = min(idx + self.max_seq_len, self.num_samples)\n",
        "        seq_len = end_idx - idx\n",
        "        sequence = {\n",
        "            'timestamp': self.timestamps[idx:end_idx],\n",
        "            'security_id': self.security_ids[idx:end_idx],\n",
        "            'buy_sell': self.buy_sell[idx:end_idx],\n",
        "            'add_modify_delete': self.add_modify_delete[idx:end_idx],\n",
        "            'quantity': self.quantity[idx:end_idx],\n",
        "            'price': self.price[idx:end_idx],\n",
        "            'distance': self.distance[idx:end_idx],\n",
        "        }\n",
        "\n",
        "        # Apply masking logic to the first element (idx)\n",
        "        sample = {key: sequence[key][0] for key in sequence}\n",
        "        masked_sample = sample.copy()\n",
        "        target = {}\n",
        "\n",
        "        for key in ['security_id', 'buy_sell', 'add_modify_delete', 'quantity', 'price', 'distance']:\n",
        "            if torch.rand(1).item() < self.mask_prob:\n",
        "                # Mask the feature\n",
        "                target[key] = sample[key]\n",
        "                if key == 'security_id':\n",
        "                    masked_sample[key] = self.security_id_mask\n",
        "                elif key == 'buy_sell':\n",
        "                    masked_sample[key] = self.buy_sell_mask\n",
        "                elif key == 'add_modify_delete':\n",
        "                    masked_sample[key] = self.add_modify_delete_mask\n",
        "                elif key == 'distance':\n",
        "                    masked_sample[key] = self.distance_mask\n",
        "                elif key in ['quantity', 'price']:\n",
        "                    masked_sample[key] = 0.0  # For continuous features, use 0.0 as masked value\n",
        "            else:\n",
        "                target[key] = None  # Not masked\n",
        "\n",
        "        # Return masked_sample, target, and the sequence starting from idx\n",
        "        return masked_sample, target, sequence\n",
        "\n",
        "# Define the Model\n",
        "class OrderModel(nn.Module):\n",
        "    def __init__(self, N_sec_ids, N_distance_bins, embedding_dim=32, hidden_dim=64):\n",
        "        super(OrderModel, self).__init__()\n",
        "\n",
        "        # Mask indices\n",
        "        self.security_id_mask = N_sec_ids\n",
        "        self.buy_sell_mask = 2\n",
        "        self.add_modify_delete_mask = 3\n",
        "        self.distance_mask = N_distance_bins\n",
        "\n",
        "        # Embedding layers for categorical features\n",
        "        self.security_id_embedding = nn.Embedding(N_sec_ids + 1, embedding_dim, padding_idx=self.security_id_mask)\n",
        "        self.buy_sell_embedding = nn.Embedding(3, embedding_dim, padding_idx=self.buy_sell_mask)  # 0,1,2(mask)\n",
        "        self.add_modify_delete_embedding = nn.Embedding(4, embedding_dim, padding_idx=self.add_modify_delete_mask)  # 0,1,2,3(mask)\n",
        "        self.distance_embedding = nn.Embedding(N_distance_bins + 1, embedding_dim, padding_idx=self.distance_mask)\n",
        "\n",
        "        # Linear layers for continuous features\n",
        "        self.quantity_linear = nn.Linear(1, embedding_dim)\n",
        "        self.price_linear = nn.Linear(1, embedding_dim)\n",
        "\n",
        "        # For timestamp (we don't mask timestamp)\n",
        "        self.timestamp_linear = nn.Linear(1, embedding_dim)\n",
        "\n",
        "        # LSTM for sequences\n",
        "        self.lstm_input_dim = embedding_dim * 7  # Number of features\n",
        "        self.lstm = nn.LSTM(input_size=self.lstm_input_dim, hidden_size=hidden_dim, batch_first=True)\n",
        "\n",
        "        # Final fully connected layers\n",
        "        self.fc1 = nn.Linear(embedding_dim * 7 + hidden_dim, 128)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Output heads\n",
        "        self.security_id_head = nn.Linear(128, N_sec_ids)\n",
        "        self.buy_sell_head = nn.Linear(128, 2)\n",
        "        self.add_modify_delete_head = nn.Linear(128, 3)\n",
        "        self.distance_head = nn.Linear(128, N_distance_bins)\n",
        "        self.quantity_head = nn.Linear(128, 1)\n",
        "        self.price_head = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x, seq_data, seq_lengths):\n",
        "        # x is a dictionary of features for the masked sample\n",
        "        # seq_data is a dictionary of sequences\n",
        "        # seq_lengths is a list of sequence lengths\n",
        "\n",
        "        # Process the masked sample\n",
        "        timestamp = x['timestamp'].unsqueeze(1).float()\n",
        "        security_id = x['security_id']\n",
        "        buy_sell = x['buy_sell']\n",
        "        add_modify_delete = x['add_modify_delete']\n",
        "        quantity = x['quantity'].unsqueeze(1)\n",
        "        price = x['price'].unsqueeze(1)\n",
        "        distance = x['distance']\n",
        "\n",
        "        # Embeddings\n",
        "        timestamp_emb = self.timestamp_linear(timestamp)\n",
        "        security_id_emb = self.security_id_embedding(security_id)\n",
        "        buy_sell_emb = self.buy_sell_embedding(buy_sell)\n",
        "        add_modify_delete_emb = self.add_modify_delete_embedding(add_modify_delete)\n",
        "        quantity_emb = self.quantity_linear(quantity)\n",
        "        price_emb = self.price_linear(price)\n",
        "        distance_emb = self.distance_embedding(distance)\n",
        "\n",
        "        # Concatenate embeddings\n",
        "        sample_emb = torch.cat([timestamp_emb,\n",
        "                                security_id_emb,\n",
        "                                buy_sell_emb,\n",
        "                                add_modify_delete_emb,\n",
        "                                quantity_emb,\n",
        "                                price_emb,\n",
        "                                distance_emb], dim=1)  # Shape: (batch_size, embedding_dim * 7)\n",
        "\n",
        "        # Process the sequence data\n",
        "        # For each feature in seq_data, get embeddings\n",
        "        batch_size = timestamp.shape[0]\n",
        "\n",
        "        seq_timestamp = seq_data['timestamp'].float()\n",
        "        seq_security_id = seq_data['security_id']\n",
        "        seq_buy_sell = seq_data['buy_sell']\n",
        "        seq_add_modify_delete = seq_data['add_modify_delete']\n",
        "        seq_quantity = seq_data['quantity']\n",
        "        seq_price = seq_data['price']\n",
        "        seq_distance = seq_data['distance']\n",
        "\n",
        "        # Embeddings for sequence data\n",
        "        seq_timestamp_emb = self.timestamp_linear(seq_timestamp.unsqueeze(-1))  # (batch_size, seq_len, embedding_dim)\n",
        "        seq_security_id_emb = self.security_id_embedding(seq_security_id)\n",
        "        seq_buy_sell_emb = self.buy_sell_embedding(seq_buy_sell)\n",
        "        seq_add_modify_delete_emb = self.add_modify_delete_embedding(seq_add_modify_delete)\n",
        "        seq_quantity_emb = self.quantity_linear(seq_quantity.unsqueeze(-1))\n",
        "        seq_price_emb = self.price_linear(seq_price.unsqueeze(-1))\n",
        "        seq_distance_emb = self.distance_embedding(seq_distance)\n",
        "\n",
        "        # Concatenate sequence embeddings\n",
        "        seq_emb = torch.cat([seq_timestamp_emb,\n",
        "                             seq_security_id_emb,\n",
        "                             seq_buy_sell_emb,\n",
        "                             seq_add_modify_delete_emb,\n",
        "                             seq_quantity_emb,\n",
        "                             seq_price_emb,\n",
        "                             seq_distance_emb], dim=2)  # Shape: (batch_size, seq_len, embedding_dim * 7)\n",
        "\n",
        "        # Pack the sequences\n",
        "        packed_seq_emb = nn.utils.rnn.pack_padded_sequence(seq_emb, seq_lengths, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        # Pass through LSTM\n",
        "        packed_output, (h_n, c_n) = self.lstm(packed_seq_emb)\n",
        "\n",
        "        # Get the last hidden state for each sequence\n",
        "        seq_context = h_n.squeeze(0)  # Shape: (batch_size, hidden_dim)\n",
        "\n",
        "        # Combine the sample embedding with the sequence context\n",
        "        combined = torch.cat([sample_emb, seq_context], dim=1)  # Shape: (batch_size, embedding_dim * 7 + hidden_dim)\n",
        "\n",
        "        # Forward pass\n",
        "        x = self.fc1(combined)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        # Outputs\n",
        "        outputs = {\n",
        "            'security_id': self.security_id_head(x),\n",
        "            'buy_sell': self.buy_sell_head(x),\n",
        "            'add_modify_delete': self.add_modify_delete_head(x),\n",
        "            'distance': self.distance_head(x),\n",
        "            'quantity': self.quantity_head(x),\n",
        "            'price': self.price_head(x)\n",
        "        }\n",
        "\n",
        "        return outputs\n",
        "\n",
        "# Collate function for DataLoader\n",
        "def collate_fn(batch):\n",
        "    # batch is a list of (masked_sample, target, sequence)\n",
        "    batch_size = len(batch)\n",
        "    masked_samples = [item[0] for item in batch]\n",
        "    targets = [item[1] for item in batch]\n",
        "    sequences = [item[2] for item in batch]\n",
        "\n",
        "    # Convert masked_samples to tensors\n",
        "    batch_data = {}\n",
        "    batch_target = {}\n",
        "    seq_data = {}\n",
        "    seq_lengths = []\n",
        "\n",
        "    # Process data\n",
        "    for key in ['timestamp', 'security_id', 'buy_sell', 'add_modify_delete', 'quantity', 'price', 'distance']:\n",
        "        batch_data[key] = torch.tensor([sample[key] for sample in masked_samples])\n",
        "\n",
        "    # Process sequences\n",
        "    # Sequences are variable-length\n",
        "    # For each key, we have a list of sequences\n",
        "    for key in ['timestamp', 'security_id', 'buy_sell', 'add_modify_delete', 'quantity', 'price', 'distance']:\n",
        "        seq_list = [torch.tensor(seq[key]) for seq in sequences]\n",
        "        seq_padded = nn.utils.rnn.pad_sequence(seq_list, batch_first=True, padding_value=0)\n",
        "        seq_data[key] = seq_padded\n",
        "    # Record lengths\n",
        "    seq_lengths = [len(seq['timestamp']) for seq in sequences]\n",
        "\n",
        "    # Convert seq_lengths to tensor\n",
        "    seq_lengths = torch.tensor(seq_lengths, dtype=torch.long)\n",
        "\n",
        "    # Process target (exclude 'timestamp' as it's not in target)\n",
        "    for key in ['security_id', 'buy_sell', 'add_modify_delete', 'quantity', 'price', 'distance']:\n",
        "        batch_target[key] = []\n",
        "        for target in targets:\n",
        "            if target[key] is not None:\n",
        "                batch_target[key].append(target[key])\n",
        "            else:\n",
        "                batch_target[key].append(-100 if key not in ['quantity', 'price'] else 0.0)\n",
        "\n",
        "        # Convert to tensor and create mask\n",
        "        if key in ['quantity', 'price']:\n",
        "            # For continuous features\n",
        "            batch_target[key + '_mask'] = torch.tensor([t != 0.0 for t in batch_target[key]], dtype=torch.bool)\n",
        "            batch_target[key] = torch.tensor(batch_target[key], dtype=torch.float)\n",
        "        else:\n",
        "            # For categorical features\n",
        "            batch_target[key + '_mask'] = torch.tensor([t != -100 for t in batch_target[key]], dtype=torch.bool)\n",
        "            batch_target[key] = torch.tensor(batch_target[key], dtype=torch.long)\n",
        "\n",
        "    return batch_data, batch_target, seq_data, seq_lengths\n",
        "\n",
        "# Training and Testing Loop\n",
        "def train_model():\n",
        "    # Create Dataset and DataLoader\n",
        "    dataset = MaskedOrderDataset(num_samples=10000)\n",
        "    dataloader = DataLoader(dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "    # Create model\n",
        "    model = OrderModel(N_sec_ids=dataset.N_sec_ids, N_distance_bins=dataset.N_distance_bins)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Loss functions\n",
        "    criterion_ce = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "    criterion_l1 = nn.L1Loss()\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(5):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        for batch_data, batch_target, seq_data, seq_lengths in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(batch_data, seq_data, seq_lengths)\n",
        "\n",
        "            loss = 0.0\n",
        "\n",
        "            # Compute loss for security_id\n",
        "            target = batch_target['security_id']\n",
        "            mask = batch_target['security_id_mask']\n",
        "            if mask.any():\n",
        "                loss_sec_id = criterion_ce(outputs['security_id'], target)\n",
        "                loss += loss_sec_id\n",
        "\n",
        "            # Compute loss for buy_sell\n",
        "            target = batch_target['buy_sell']\n",
        "            mask = batch_target['buy_sell_mask']\n",
        "            if mask.any():\n",
        "                loss_buy_sell = criterion_ce(outputs['buy_sell'], target)\n",
        "                loss += loss_buy_sell\n",
        "\n",
        "            # Compute loss for add_modify_delete\n",
        "            target = batch_target['add_modify_delete']\n",
        "            mask = batch_target['add_modify_delete_mask']\n",
        "            if mask.any():\n",
        "                loss_amd = criterion_ce(outputs['add_modify_delete'], target)\n",
        "                loss += loss_amd\n",
        "\n",
        "            # Compute loss for distance\n",
        "            target = batch_target['distance']\n",
        "            mask = batch_target['distance_mask']\n",
        "            if mask.any():\n",
        "                loss_distance = criterion_ce(outputs['distance'], target)\n",
        "                loss += loss_distance\n",
        "\n",
        "            # Compute loss for quantity\n",
        "            target = batch_target['quantity']\n",
        "            mask = batch_target['quantity_mask']\n",
        "            if mask.any():\n",
        "                output = outputs['quantity'].squeeze()\n",
        "                loss_quantity = criterion_l1(output[mask], target[mask])\n",
        "                loss += loss_quantity\n",
        "\n",
        "            # Compute loss for price\n",
        "            target = batch_target['price']\n",
        "            mask = batch_target['price_mask']\n",
        "            if mask.any():\n",
        "                output = outputs['price'].squeeze()\n",
        "                loss_price = criterion_l1(output[mask], target[mask])\n",
        "                loss += loss_price\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Loss: {total_loss/len(dataloader)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "ZmEZwyPGCTJh",
        "outputId": "ac4dece1-211b-4364-c572-c2520739e963"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-d57e3d80251b>:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  seq_list = [torch.tensor(seq[key]) for seq in sequences]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-d57e3d80251b>\u001b[0m in \u001b[0;36m<cell line: 344>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-d57e3d80251b>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m    335\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_price\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sequences(model, past_data, security_id, num_timesteps, beam_width, num_samples, device='cpu'):\n",
        "    \"\"\"\n",
        "    Generate sequences using beam search.\n",
        "\n",
        "    Args:\n",
        "        model: Trained model.\n",
        "        past_data: Dictionary containing past sequences.\n",
        "        security_id: The security ID to generate sequences for.\n",
        "        num_timesteps: Number of timesteps to generate.\n",
        "        beam_width: Beam width.\n",
        "        num_samples: Number of samples to generate.\n",
        "        device: Device to run the computations on.\n",
        "\n",
        "    Returns:\n",
        "        generated_sequences: A list of generated sequences.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Initialize the beam with the past_data\n",
        "        # Each beam entry is a tuple (sequence, cumulative_log_prob)\n",
        "        beam = [({'timestamp': past_data['timestamp'],\n",
        "                  'position': past_data['position'],\n",
        "                  'security_id': past_data['security_id'],\n",
        "                  'buy_sell': past_data['buy_sell'],\n",
        "                  'add_modify_delete': past_data['add_modify_delete'],\n",
        "                  'quantity': past_data['quantity'],\n",
        "                  'price': past_data['price'],\n",
        "                  'distance': past_data['distance']},\n",
        "                 0.0)]  # Start with log probability 0\n",
        "\n",
        "        for t in range(num_timesteps):\n",
        "            new_beam = []\n",
        "            for seq, cum_log_prob in beam:\n",
        "                # Prepare input data\n",
        "                # Use the last max_seq_len elements\n",
        "                seq_len = len(seq['timestamp'])\n",
        "                start_idx = max(0, seq_len - model.lstm_input_dim) if hasattr(model, 'lstm_input_dim') else 0\n",
        "                seq_data = {key: torch.tensor(seq[key][start_idx:]).unsqueeze(0).to(device) for key in seq}\n",
        "                seq_lengths = torch.tensor([seq_data['timestamp'].shape[1]]).to(device)\n",
        "\n",
        "                # Create a sample with masked features (except security_id and timestamp)\n",
        "                sample = {\n",
        "                    'timestamp': torch.tensor([seq['timestamp'][-1] + 1]).to(device),\n",
        "                    'position': torch.tensor([seq['position'][-1] + 1]).to(device),\n",
        "                    'security_id': torch.tensor([security_id]).to(device),\n",
        "                    'buy_sell': torch.tensor([model.buy_sell_mask if hasattr(model, 'buy_sell_mask') else 2]).to(device),\n",
        "                    'add_modify_delete': torch.tensor([model.add_modify_delete_mask if hasattr(model, 'add_modify_delete_mask') else 3]).to(device),\n",
        "                    'quantity': torch.tensor([0.0]).to(device),\n",
        "                    'price': torch.tensor([0.0]).to(device),\n",
        "                    'distance': torch.tensor([model.distance_mask if hasattr(model, 'distance_mask') else 10]).to(device)\n",
        "                }\n",
        "\n",
        "                # Predict missing features\n",
        "                outputs = model(sample, seq_data, seq_lengths)\n",
        "\n",
        "                # For each categorical feature, get top N predictions\n",
        "                candidates = []\n",
        "\n",
        "                # Adjust topk for buy_sell\n",
        "                if 'buy_sell' in outputs:\n",
        "                    buy_sell_probs = F.softmax(outputs['buy_sell'], dim=-1)\n",
        "                    num_buy_sell_classes = buy_sell_probs.size(-1)\n",
        "                    buy_sell_topk_k = min(beam_width, num_buy_sell_classes)\n",
        "                    buy_sell_topk = torch.topk(buy_sell_probs, buy_sell_topk_k)\n",
        "                else:\n",
        "                    buy_sell_topk_k = 1\n",
        "                    buy_sell_topk = None\n",
        "\n",
        "                # Adjust topk for add_modify_delete\n",
        "                if 'add_modify_delete' in outputs:\n",
        "                    add_modify_delete_probs = F.softmax(outputs['add_modify_delete'], dim=-1)\n",
        "                    num_amd_classes = add_modify_delete_probs.size(-1)\n",
        "                    amd_topk_k = min(beam_width, num_amd_classes)\n",
        "                    amd_topk = torch.topk(add_modify_delete_probs, amd_topk_k)\n",
        "                else:\n",
        "                    amd_topk_k = 1\n",
        "                    amd_topk = None\n",
        "\n",
        "                # Adjust topk for distance\n",
        "                if 'distance' in outputs:\n",
        "                    distance_probs = F.softmax(outputs['distance'], dim=-1)\n",
        "                    num_distance_classes = distance_probs.size(-1)\n",
        "                    distance_topk_k = min(beam_width, num_distance_classes)\n",
        "                    distance_topk = torch.topk(distance_probs, distance_topk_k)\n",
        "                else:\n",
        "                    distance_topk_k = 1\n",
        "                    distance_topk = None\n",
        "\n",
        "                # For continuous features, we can use the predicted value directly\n",
        "                quantity_pred = outputs['quantity'].item() if 'quantity' in outputs else 0.0\n",
        "                price_pred = outputs['price'].item() if 'price' in outputs else 0.0\n",
        "\n",
        "                # Generate combinations of top predictions\n",
        "                for i in range(buy_sell_topk_k):\n",
        "                    for j in range(amd_topk_k):\n",
        "                        for k in range(distance_topk_k):\n",
        "                            new_seq = {key: seq[key] + [sample[key].item()] for key in seq}\n",
        "                            if buy_sell_topk is not None:\n",
        "                                new_seq['buy_sell'][-1] = buy_sell_topk.indices[0][i].item()\n",
        "                            else:\n",
        "                                new_seq['buy_sell'][-1] = seq['buy_sell'][-1]  # Use previous value\n",
        "\n",
        "                            if amd_topk is not None:\n",
        "                                new_seq['add_modify_delete'][-1] = amd_topk.indices[0][j].item()\n",
        "                            else:\n",
        "                                new_seq['add_modify_delete'][-1] = seq['add_modify_delete'][-1]  # Use previous value\n",
        "\n",
        "                            if distance_topk is not None:\n",
        "                                new_seq['distance'][-1] = distance_topk.indices[0][k].item()\n",
        "                            else:\n",
        "                                new_seq['distance'][-1] = seq['distance'][-1]  # Use previous value\n",
        "\n",
        "                            new_seq['quantity'][-1] = quantity_pred\n",
        "                            new_seq['price'][-1] = price_pred\n",
        "\n",
        "                            # Compute new cumulative log probability\n",
        "                            log_prob = cum_log_prob\n",
        "                            if buy_sell_topk is not None:\n",
        "                                log_prob += torch.log(buy_sell_topk.values[0][i] + 1e-9).item()\n",
        "                            if amd_topk is not None:\n",
        "                                log_prob += torch.log(amd_topk.values[0][j] + 1e-9).item()\n",
        "                            if distance_topk is not None:\n",
        "                                log_prob += torch.log(distance_topk.values[0][k] + 1e-9).item()\n",
        "\n",
        "                            candidates.append((new_seq, log_prob))\n",
        "\n",
        "                # Keep top beam_width candidates\n",
        "                candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "                new_beam.extend(candidates[:beam_width])\n",
        "\n",
        "            # Keep top beam_width sequences\n",
        "            new_beam.sort(key=lambda x: x[1], reverse=True)\n",
        "            beam = new_beam[:beam_width]\n",
        "\n",
        "        # After generation, sample from the beam to get the required number of samples\n",
        "        generated_sequences = [seq for seq, _ in beam[:num_samples]]\n",
        "        return generated_sequences\n"
      ],
      "metadata": {
        "id": "C0rbaPkQwrYD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import random\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Define the Dataset\n",
        "class MaskedOrderDataset(Dataset):\n",
        "    def __init__(self, num_samples, mask_prob=0.15, max_seq_len=50):\n",
        "        self.num_samples = num_samples\n",
        "        self.N_sec_ids = 100  # Number of unique securities\n",
        "        self.N_distance_bins = 10  # Number of distance bins\n",
        "        self.mask_prob = mask_prob\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "        # Generate synthetic data\n",
        "        self.timestamps = torch.randint(0, 1000, (num_samples,))\n",
        "        self.security_ids = torch.randint(0, self.N_sec_ids, (num_samples,))\n",
        "        self.buy_sell = torch.randint(0, 2, (num_samples,))  # 0: buy, 1: sell\n",
        "        self.add_modify_delete = torch.randint(0, 3, (num_samples,))  # 0: add, 1: modify, 2: delete\n",
        "        self.quantity = torch.randint(1, 1000, (num_samples,)).float()\n",
        "        self.price = torch.rand(num_samples) * 100  # Prices between 0 and 100\n",
        "        self.distance = torch.randint(0, self.N_distance_bins, (num_samples,))\n",
        "\n",
        "        # Sort the data by timestamps (ascending)\n",
        "        sorted_indices = torch.argsort(self.timestamps)\n",
        "        self.timestamps = self.timestamps[sorted_indices]\n",
        "        self.security_ids = self.security_ids[sorted_indices]\n",
        "        self.buy_sell = self.buy_sell[sorted_indices]\n",
        "        self.add_modify_delete = self.add_modify_delete[sorted_indices]\n",
        "        self.quantity = self.quantity[sorted_indices]\n",
        "        self.price = self.price[sorted_indices]\n",
        "        self.distance = self.distance[sorted_indices]\n",
        "\n",
        "        # Define mask indices\n",
        "        self.security_id_mask = self.N_sec_ids\n",
        "        self.buy_sell_mask = 2\n",
        "        self.add_modify_delete_mask = 3\n",
        "        self.distance_mask = self.N_distance_bins\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get sequence from idx to end, up to max_seq_len\n",
        "        end_idx = min(idx + self.max_seq_len, self.num_samples)\n",
        "        seq_len = end_idx - idx\n",
        "        sequence = {\n",
        "            'timestamp': self.timestamps[idx:end_idx],\n",
        "            'security_id': self.security_ids[idx:end_idx],\n",
        "            'buy_sell': self.buy_sell[idx:end_idx],\n",
        "            'add_modify_delete': self.add_modify_delete[idx:end_idx],\n",
        "            'quantity': self.quantity[idx:end_idx],\n",
        "            'price': self.price[idx:end_idx],\n",
        "            'distance': self.distance[idx:end_idx],\n",
        "            'position': torch.arange(seq_len)  # Row number\n",
        "        }\n",
        "\n",
        "        # Apply masking logic to the first element (idx)\n",
        "        sample = {key: sequence[key][0] for key in sequence}\n",
        "        masked_sample = sample.copy()\n",
        "        target = {}\n",
        "\n",
        "        # List of features to potentially mask\n",
        "        maskable_keys = ['security_id', 'buy_sell', 'add_modify_delete', 'quantity', 'price', 'distance']\n",
        "\n",
        "        # Decide how many features to mask (at least one)\n",
        "        num_features_to_mask = random.randint(1, len(maskable_keys))\n",
        "\n",
        "        # Randomly select features to mask\n",
        "        features_to_mask = random.sample(maskable_keys, num_features_to_mask)\n",
        "\n",
        "        for key in maskable_keys:\n",
        "            if key in features_to_mask:\n",
        "                # Mask the feature\n",
        "                target[key] = sample[key]\n",
        "                if key == 'security_id':\n",
        "                    masked_sample[key] = self.security_id_mask\n",
        "                elif key == 'buy_sell':\n",
        "                    masked_sample[key] = self.buy_sell_mask\n",
        "                elif key == 'add_modify_delete':\n",
        "                    masked_sample[key] = self.add_modify_delete_mask\n",
        "                elif key == 'distance':\n",
        "                    masked_sample[key] = self.distance_mask\n",
        "                elif key in ['quantity', 'price']:\n",
        "                    masked_sample[key] = 0.0  # For continuous features, use 0.0 as masked value\n",
        "            else:\n",
        "                target[key] = None  # Not masked\n",
        "\n",
        "        # Return masked_sample, target, and the sequence starting from idx\n",
        "        return masked_sample, target, sequence\n",
        "\n",
        "# Learnable Positional Encoder\n",
        "class PositionalEncoder(nn.Module):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(PositionalEncoder, self).__init__()\n",
        "        self.timestamp_encoder = nn.Linear(1, embedding_dim)\n",
        "        self.position_encoder = nn.Embedding(1000, embedding_dim)  # Assume max 1000 positions\n",
        "\n",
        "    def forward(self, timestamps, positions):\n",
        "        timestamp_emb = self.timestamp_encoder(timestamps.unsqueeze(-1).float())\n",
        "        position_emb = self.position_encoder(positions)\n",
        "        return timestamp_emb + position_emb\n",
        "\n",
        "# Base Model Class\n",
        "class BaseModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BaseModel, self).__init__()\n",
        "\n",
        "    def forward(self, *args, **kwargs):\n",
        "        raise NotImplementedError\n",
        "\n",
        "# Order Model with Positional Encoder\n",
        "class OrderModel(BaseModel):\n",
        "    def __init__(self, N_sec_ids, N_distance_bins, embedding_dim=32, hidden_dim=64):\n",
        "        super(OrderModel, self).__init__()\n",
        "\n",
        "        # Mask indices\n",
        "        self.security_id_mask = N_sec_ids\n",
        "        self.buy_sell_mask = 2\n",
        "        self.add_modify_delete_mask = 3\n",
        "        self.distance_mask = N_distance_bins\n",
        "\n",
        "        # Embedding layers for categorical features\n",
        "        self.security_id_embedding = nn.Embedding(N_sec_ids + 1, embedding_dim, padding_idx=self.security_id_mask)\n",
        "        self.buy_sell_embedding = nn.Embedding(3, embedding_dim, padding_idx=self.buy_sell_mask)  # 0,1,2(mask)\n",
        "        self.add_modify_delete_embedding = nn.Embedding(4, embedding_dim, padding_idx=self.add_modify_delete_mask)  # 0,1,2,3(mask)\n",
        "        self.distance_embedding = nn.Embedding(N_distance_bins + 1, embedding_dim, padding_idx=self.distance_mask)\n",
        "\n",
        "        # Linear layers for continuous features\n",
        "        self.quantity_linear = nn.Linear(1, embedding_dim)\n",
        "        self.price_linear = nn.Linear(1, embedding_dim)\n",
        "\n",
        "        # Positional Encoder\n",
        "        self.positional_encoder = PositionalEncoder(embedding_dim)\n",
        "\n",
        "        # LSTM for sequences\n",
        "        self.lstm_input_dim = embedding_dim * 7  # Number of features\n",
        "        self.lstm = nn.LSTM(input_size=self.lstm_input_dim, hidden_size=hidden_dim, batch_first=True)\n",
        "\n",
        "        # Final fully connected layers\n",
        "        self.fc1 = nn.Linear(embedding_dim * 7 + hidden_dim, 128)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Output heads\n",
        "        self.security_id_head = nn.Linear(128, N_sec_ids)\n",
        "        self.buy_sell_head = nn.Linear(128, 2)\n",
        "        self.add_modify_delete_head = nn.Linear(128, 3)\n",
        "        self.distance_head = nn.Linear(128, N_distance_bins)\n",
        "        self.quantity_head = nn.Linear(128, 1)\n",
        "        self.price_head = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x, seq_data, seq_lengths):\n",
        "        # x is a dictionary of features for the masked sample\n",
        "        # seq_data is a dictionary of sequences\n",
        "        # seq_lengths is a list of sequence lengths\n",
        "\n",
        "        batch_size = x['timestamp'].shape[0]\n",
        "\n",
        "        # Process the masked sample\n",
        "        timestamp = x['timestamp']\n",
        "        position = x['position']\n",
        "        security_id = x['security_id']\n",
        "        buy_sell = x['buy_sell']\n",
        "        add_modify_delete = x['add_modify_delete']\n",
        "        quantity = x['quantity'].unsqueeze(1)\n",
        "        price = x['price'].unsqueeze(1)\n",
        "        distance = x['distance']\n",
        "\n",
        "        # Embeddings\n",
        "        positional_emb = self.positional_encoder(timestamp, position)\n",
        "        security_id_emb = self.security_id_embedding(security_id)\n",
        "        buy_sell_emb = self.buy_sell_embedding(buy_sell)\n",
        "        add_modify_delete_emb = self.add_modify_delete_embedding(add_modify_delete)\n",
        "        quantity_emb = self.quantity_linear(quantity)\n",
        "        price_emb = self.price_linear(price)\n",
        "        distance_emb = self.distance_embedding(distance)\n",
        "\n",
        "        # Concatenate embeddings\n",
        "        sample_emb = torch.cat([positional_emb,\n",
        "                                security_id_emb,\n",
        "                                buy_sell_emb,\n",
        "                                add_modify_delete_emb,\n",
        "                                quantity_emb,\n",
        "                                price_emb,\n",
        "                                distance_emb], dim=1)  # Shape: (batch_size, embedding_dim * 7)\n",
        "\n",
        "        # Process the sequence data\n",
        "        # For each feature in seq_data, get embeddings\n",
        "\n",
        "        seq_timestamp = seq_data['timestamp']\n",
        "        seq_position = seq_data['position']\n",
        "        seq_security_id = seq_data['security_id']\n",
        "        seq_buy_sell = seq_data['buy_sell']\n",
        "        seq_add_modify_delete = seq_data['add_modify_delete']\n",
        "        seq_quantity = seq_data['quantity']\n",
        "        seq_price = seq_data['price']\n",
        "        seq_distance = seq_data['distance']\n",
        "\n",
        "        # Embeddings for sequence data\n",
        "        seq_positional_emb = self.positional_encoder(seq_timestamp, seq_position)\n",
        "        seq_security_id_emb = self.security_id_embedding(seq_security_id)\n",
        "        seq_buy_sell_emb = self.buy_sell_embedding(seq_buy_sell)\n",
        "        seq_add_modify_delete_emb = self.add_modify_delete_embedding(seq_add_modify_delete)\n",
        "        seq_quantity_emb = self.quantity_linear(seq_quantity.unsqueeze(-1))\n",
        "        seq_price_emb = self.price_linear(seq_price.unsqueeze(-1))\n",
        "        seq_distance_emb = self.distance_embedding(seq_distance)\n",
        "\n",
        "        # Concatenate sequence embeddings\n",
        "        seq_emb = torch.cat([seq_positional_emb,\n",
        "                             seq_security_id_emb,\n",
        "                             seq_buy_sell_emb,\n",
        "                             seq_add_modify_delete_emb,\n",
        "                             seq_quantity_emb,\n",
        "                             seq_price_emb,\n",
        "                             seq_distance_emb], dim=2)  # Shape: (batch_size, seq_len, embedding_dim * 7)\n",
        "\n",
        "        # Pack the sequences\n",
        "        packed_seq_emb = pack_padded_sequence(seq_emb, seq_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        # Pass through LSTM\n",
        "        packed_output, (h_n, c_n) = self.lstm(packed_seq_emb)\n",
        "\n",
        "        # Get the last hidden state for each sequence\n",
        "        seq_context = h_n[-1]  # Shape: (batch_size, hidden_dim)\n",
        "\n",
        "        # Combine the sample embedding with the sequence context\n",
        "        combined = torch.cat([sample_emb, seq_context], dim=1)  # Shape: (batch_size, embedding_dim * 7 + hidden_dim)\n",
        "\n",
        "        # Forward pass\n",
        "        x = self.fc1(combined)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        # Outputs\n",
        "        outputs = {\n",
        "            'security_id': self.security_id_head(x),\n",
        "            'buy_sell': self.buy_sell_head(x),\n",
        "            'add_modify_delete': self.add_modify_delete_head(x),\n",
        "            'distance': self.distance_head(x),\n",
        "            'quantity': self.quantity_head(x),\n",
        "            'price': self.price_head(x)\n",
        "        }\n",
        "\n",
        "        return outputs\n",
        "\n",
        "# Additional Models (Assuming models are unchanged, not included here for brevity)\n",
        "\n",
        "# [Include other models here if needed]\n",
        "\n",
        "# Collate function for DataLoader\n",
        "def collate_fn(batch):\n",
        "    # batch is a list of (masked_sample, target, sequence)\n",
        "    batch_size = len(batch)\n",
        "    masked_samples = [item[0] for item in batch]\n",
        "    targets = [item[1] for item in batch]\n",
        "    sequences = [item[2] for item in batch]\n",
        "\n",
        "    # Convert masked_samples to tensors\n",
        "    batch_data = {}\n",
        "    batch_target = {}\n",
        "    seq_data = {}\n",
        "    seq_lengths = []\n",
        "\n",
        "    # Process data\n",
        "    for key in ['timestamp', 'position', 'security_id', 'buy_sell', 'add_modify_delete', 'quantity', 'price', 'distance']:\n",
        "        batch_data[key] = torch.tensor([sample[key] for sample in masked_samples])\n",
        "\n",
        "    # Process sequences\n",
        "    # Sequences are variable-length\n",
        "    # For each key, we have a list of sequences\n",
        "    for key in ['timestamp', 'position', 'security_id', 'buy_sell', 'add_modify_delete', 'quantity', 'price', 'distance']:\n",
        "        seq_list = [torch.tensor(seq[key]) for seq in sequences]\n",
        "        seq_padded = nn.utils.rnn.pad_sequence(seq_list, batch_first=True, padding_value=0)\n",
        "        seq_data[key] = seq_padded\n",
        "    # Record lengths\n",
        "    seq_lengths = [len(seq['timestamp']) for seq in sequences]\n",
        "\n",
        "    # Convert seq_lengths to tensor\n",
        "    seq_lengths = torch.tensor(seq_lengths, dtype=torch.long)\n",
        "\n",
        "    # Process target (exclude 'timestamp' and 'position' as they're not in target)\n",
        "    for key in ['security_id', 'buy_sell', 'add_modify_delete', 'quantity', 'price', 'distance']:\n",
        "        batch_target[key] = []\n",
        "        for target in targets:\n",
        "            if target[key] is not None:\n",
        "                batch_target[key].append(target[key])\n",
        "            else:\n",
        "                batch_target[key].append(-100 if key not in ['quantity', 'price'] else 0.0)\n",
        "\n",
        "        # Convert to tensor and create mask\n",
        "        if key in ['quantity', 'price']:\n",
        "            # For continuous features\n",
        "            batch_target[key + '_mask'] = torch.tensor([t != 0.0 for t in batch_target[key]], dtype=torch.bool)\n",
        "            batch_target[key] = torch.tensor(batch_target[key], dtype=torch.float)\n",
        "        else:\n",
        "            # For categorical features\n",
        "            batch_target[key + '_mask'] = torch.tensor([t != -100 for t in batch_target[key]], dtype=torch.bool)\n",
        "            batch_target[key] = torch.tensor(batch_target[key], dtype=torch.long)\n",
        "\n",
        "    return batch_data, batch_target, seq_data, seq_lengths\n",
        "\n",
        "# Training and Testing Loop with Checkpointing and TensorBoard\n",
        "def train_model(model_class, model_params, dataset_params, training_params, save_path=None, load_path=None):\n",
        "    # Create Dataset and DataLoader\n",
        "    dataset = MaskedOrderDataset(**dataset_params)\n",
        "    dataloader = DataLoader(dataset, batch_size=training_params['batch_size'], shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "    # Create model\n",
        "    model = model_class(**model_params).to(training_params['device'])\n",
        "    optimizer = optim.Adam(model.parameters(), lr=training_params['learning_rate'])\n",
        "\n",
        "    # Load checkpoint if provided\n",
        "    start_epoch = 0\n",
        "    if load_path and os.path.exists(load_path):\n",
        "        checkpoint = torch.load(load_path)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        start_epoch = checkpoint['epoch'] + 1\n",
        "        print(f\"Loaded checkpoint from '{load_path}' (epoch {start_epoch})\")\n",
        "\n",
        "    # TensorBoard SummaryWriter\n",
        "    writer = SummaryWriter()\n",
        "\n",
        "    # Loss functions\n",
        "    criterion_ce = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "    criterion_l1 = nn.L1Loss()\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(start_epoch, training_params['num_epochs']):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        for batch_idx, (batch_data, batch_target, seq_data, seq_lengths) in enumerate(dataloader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Move data to device\n",
        "            device = training_params.get('device', 'cpu')\n",
        "            batch_data = {k: v.to(device) for k, v in batch_data.items()}\n",
        "            batch_target = {k: v.to(device) for k, v in batch_target.items()}\n",
        "            seq_data = {k: v.to(device) for k, v in seq_data.items()}\n",
        "            seq_lengths = seq_lengths.to(device)\n",
        "\n",
        "            outputs = model(batch_data, seq_data, seq_lengths)\n",
        "\n",
        "            loss = 0.0\n",
        "\n",
        "            # Compute loss for each feature\n",
        "            for key in ['security_id', 'buy_sell', 'add_modify_delete', 'distance']:\n",
        "                if key in outputs:\n",
        "                    target = batch_target[key]\n",
        "                    mask = batch_target[key + '_mask']\n",
        "                    if mask.any():\n",
        "                        output = outputs[key]\n",
        "                        loss_feature = criterion_ce(output, target)\n",
        "                        loss += loss_feature\n",
        "\n",
        "            for key in ['quantity', 'price']:\n",
        "                if key in outputs:\n",
        "                    target = batch_target[key]\n",
        "                    mask = batch_target[key + '_mask']\n",
        "                    if mask.any():\n",
        "                        output = outputs[key].squeeze()\n",
        "                        loss_feature = criterion_l1(output[mask], target[mask])\n",
        "                        loss += loss_feature\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Log training loss to TensorBoard\n",
        "            writer.add_scalar('Loss/train_batch', loss.item(), epoch * len(dataloader) + batch_idx)\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f\"Epoch {epoch+1}, Loss: {avg_loss}\")\n",
        "\n",
        "        # Log average loss per epoch to TensorBoard\n",
        "        writer.add_scalar('Loss/train_epoch', avg_loss, epoch+1)\n",
        "\n",
        "        # Save checkpoint\n",
        "        if save_path:\n",
        "            checkpoint = {\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "            }\n",
        "            torch.save(checkpoint, save_path)\n",
        "            print(f\"Checkpoint saved at '{save_path}'\")\n",
        "\n",
        "    writer.close()\n",
        "    return model\n",
        "\n",
        "# Function to load a saved model\n",
        "def load_model(model_class, model_params, checkpoint_path, device='cpu'):\n",
        "    model = model_class(**model_params).to(device)\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(f\"Model loaded from '{checkpoint_path}'\")\n",
        "    return model\n",
        "\n",
        "# Beam Search Generation Function (Assuming unchanged)\n",
        "\n",
        "# Hyperparameter Optimization Function (Assuming unchanged)\n",
        "\n",
        "# Example Usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Training parameters\n",
        "    training_params = {\n",
        "        'batch_size': 32,\n",
        "        'learning_rate': 0.001,\n",
        "        'num_epochs': 5,\n",
        "        'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    }\n",
        "\n",
        "    # Dataset parameters\n",
        "    dataset_params = {\n",
        "        'num_samples': 5000,\n",
        "        'mask_prob': 0.15,\n",
        "        'max_seq_len': 50\n",
        "    }\n",
        "\n",
        "    # Common parameters\n",
        "    output_keys = ['security_id', 'buy_sell', 'add_modify_delete', 'distance', 'quantity', 'price']\n",
        "    num_classes_dict = {\n",
        "        'security_id': 100,\n",
        "        'buy_sell': 2,\n",
        "        'add_modify_delete': 3,\n",
        "        'distance': 10\n",
        "    }\n",
        "\n",
        "    # Example: Training OrderModel with checkpointing and TensorBoard\n",
        "    print(\"Training OrderModel with checkpointing and TensorBoard...\")\n",
        "    model_params_order = {\n",
        "        'N_sec_ids': 100,\n",
        "        'N_distance_bins': 10,\n",
        "        'embedding_dim': 32,\n",
        "        'hidden_dim': 64\n",
        "    }\n",
        "\n",
        "    # Paths for saving and loading checkpoints\n",
        "    checkpoint_path = 'order_model_checkpoint.pth'\n",
        "\n",
        "    model_order = train_model(\n",
        "        model_class=OrderModel,\n",
        "        model_params=model_params_order,\n",
        "        dataset_params=dataset_params,\n",
        "        training_params=training_params,\n",
        "        save_path=checkpoint_path,\n",
        "        load_path=None  # Set to checkpoint_path if you want to load from a checkpoint\n",
        "    )\n",
        "\n",
        "    # Example: Loading the trained model\n",
        "    loaded_model_order = load_model(\n",
        "        model_class=OrderModel,\n",
        "        model_params=model_params_order,\n",
        "        checkpoint_path=checkpoint_path,\n",
        "        device=training_params['device']\n",
        "    )\n",
        "\n",
        "    # Generate sequences using beam search with loaded model\n",
        "    print(\"\\nGenerating sequences using beam search with loaded OrderModel...\")\n",
        "    # Prepare past_data\n",
        "    past_data = {\n",
        "        'timestamp': [1000, 1001, 1002],\n",
        "        'position': [0, 1, 2],\n",
        "        'security_id': [5, 5, 5],\n",
        "        'buy_sell': [0, 1, 0],\n",
        "        'add_modify_delete': [0, 1, 2],\n",
        "        'quantity': [500.0, 600.0, 700.0],\n",
        "        'price': [50.0, 51.0, 52.0],\n",
        "        'distance': [3, 2, 1]\n",
        "    }\n",
        "\n",
        "    # Parameters for generation\n",
        "    security_id = 5\n",
        "    num_timesteps = 5\n",
        "    beam_width = 3\n",
        "    num_samples = 3\n",
        "\n",
        "    generated_sequences = generate_sequences(\n",
        "        model=loaded_model_order,\n",
        "        past_data=past_data,\n",
        "        security_id=security_id,\n",
        "        num_timesteps=num_timesteps,\n",
        "        beam_width=beam_width,\n",
        "        num_samples=num_samples,\n",
        "        device=training_params['device']\n",
        "    )\n",
        "\n",
        "    # Compute distribution of buys vs sells\n",
        "    total_buy = 0\n",
        "    total_sell = 0\n",
        "    total_actions = 0\n",
        "\n",
        "    for seq in generated_sequences:\n",
        "        buy_sell_seq = seq['buy_sell'][-num_timesteps:]  # Only consider generated timesteps\n",
        "        total_buy += buy_sell_seq.count(0)\n",
        "        total_sell += buy_sell_seq.count(1)\n",
        "        total_actions += len(buy_sell_seq)\n",
        "\n",
        "    buy_percentage = (total_buy / total_actions) * 100 if total_actions > 0 else 0\n",
        "    sell_percentage = (total_sell / total_actions) * 100 if total_actions > 0 else 0\n",
        "\n",
        "    print(f\"\\nGenerated {num_samples} sequences of {num_timesteps} timesteps each.\")\n",
        "    print(f\"Buy actions: {buy_percentage:.2f}%\")\n",
        "    print(f\"Sell actions: {sell_percentage:.2f}%\")\n",
        "\n",
        "    # To visualize TensorBoard logs, run the following command in your terminal:\n",
        "    # tensorboard --logdir runs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODy_moR2ulVe",
        "outputId": "841af501-84f7-45b0-91ad-98f51a72a9f4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training OrderModel with checkpointing and TensorBoard...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-c1912c2e3b47>:282: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  seq_list = [torch.tensor(seq[key]) for seq in sequences]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 377.2614810238978\n",
            "Checkpoint saved at 'order_model_checkpoint.pth'\n",
            "Epoch 2, Loss: 349.6525611634467\n",
            "Checkpoint saved at 'order_model_checkpoint.pth'\n",
            "Epoch 3, Loss: 335.3304006005548\n",
            "Checkpoint saved at 'order_model_checkpoint.pth'\n",
            "Epoch 4, Loss: 316.08162004021324\n",
            "Checkpoint saved at 'order_model_checkpoint.pth'\n",
            "Epoch 5, Loss: 296.4181347500746\n",
            "Checkpoint saved at 'order_model_checkpoint.pth'\n",
            "Model loaded from 'order_model_checkpoint.pth'\n",
            "\n",
            "Generating sequences using beam search with loaded OrderModel...\n",
            "\n",
            "Generated 3 sequences of 5 timesteps each.\n",
            "Buy actions: 100.00%\n",
            "Sell actions: 0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-c1912c2e3b47>:405: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path, map_location=device)\n"
          ]
        }
      ]
    }
  ]
}