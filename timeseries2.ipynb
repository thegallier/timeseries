{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9JbNOGf6jt963SOG0A6td",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thegallier/timeseries/blob/main/timeseries2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the Dataset\n",
        "class MaskedOrderDataset(Dataset):\n",
        "    def __init__(self, num_samples, mask_prob=0.15, max_seq_len=50):\n",
        "        self.num_samples = num_samples\n",
        "        self.N_sec_ids = 100  # Number of unique securities\n",
        "        self.N_distance_bins = 10  # Number of distance bins\n",
        "        self.mask_prob = mask_prob\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "        # Generate synthetic data\n",
        "        self.timestamps = torch.randint(0, 1000, (num_samples,))\n",
        "        self.security_ids = torch.randint(0, self.N_sec_ids, (num_samples,))\n",
        "        self.buy_sell = torch.randint(0, 2, (num_samples,))  # 0: buy, 1: sell\n",
        "        self.add_modify_delete = torch.randint(0, 3, (num_samples,))  # 0: add, 1: modify, 2: delete\n",
        "        self.quantity = torch.randint(1, 1000, (num_samples,)).float()\n",
        "        self.price = torch.rand(num_samples) * 100  # Prices between 0 and 100\n",
        "        self.distance = torch.randint(0, self.N_distance_bins, (num_samples,))\n",
        "\n",
        "        # Sort the data by timestamps (ascending)\n",
        "        sorted_indices = torch.argsort(self.timestamps)\n",
        "        self.timestamps = self.timestamps[sorted_indices]\n",
        "        self.security_ids = self.security_ids[sorted_indices]\n",
        "        self.buy_sell = self.buy_sell[sorted_indices]\n",
        "        self.add_modify_delete = self.add_modify_delete[sorted_indices]\n",
        "        self.quantity = self.quantity[sorted_indices]\n",
        "        self.price = self.price[sorted_indices]\n",
        "        self.distance = self.distance[sorted_indices]\n",
        "\n",
        "        # Define mask indices\n",
        "        self.security_id_mask = self.N_sec_ids\n",
        "        self.buy_sell_mask = 2\n",
        "        self.add_modify_delete_mask = 3\n",
        "        self.distance_mask = self.N_distance_bins\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get sequence from idx to end, up to max_seq_len\n",
        "        end_idx = min(idx + self.max_seq_len, self.num_samples)\n",
        "        seq_len = end_idx - idx\n",
        "        sequence = {\n",
        "            'timestamp': self.timestamps[idx:end_idx],\n",
        "            'security_id': self.security_ids[idx:end_idx],\n",
        "            'buy_sell': self.buy_sell[idx:end_idx],\n",
        "            'add_modify_delete': self.add_modify_delete[idx:end_idx],\n",
        "            'quantity': self.quantity[idx:end_idx],\n",
        "            'price': self.price[idx:end_idx],\n",
        "            'distance': self.distance[idx:end_idx],\n",
        "        }\n",
        "\n",
        "        # Apply masking logic to the first element (idx)\n",
        "        sample = {key: sequence[key][0] for key in sequence}\n",
        "        masked_sample = sample.copy()\n",
        "        target = {}\n",
        "\n",
        "        for key in ['security_id', 'buy_sell', 'add_modify_delete', 'quantity', 'price', 'distance']:\n",
        "            if torch.rand(1).item() < self.mask_prob:\n",
        "                # Mask the feature\n",
        "                target[key] = sample[key]\n",
        "                if key == 'security_id':\n",
        "                    masked_sample[key] = self.security_id_mask\n",
        "                elif key == 'buy_sell':\n",
        "                    masked_sample[key] = self.buy_sell_mask\n",
        "                elif key == 'add_modify_delete':\n",
        "                    masked_sample[key] = self.add_modify_delete_mask\n",
        "                elif key == 'distance':\n",
        "                    masked_sample[key] = self.distance_mask\n",
        "                elif key in ['quantity', 'price']:\n",
        "                    masked_sample[key] = 0.0  # For continuous features, use 0.0 as masked value\n",
        "            else:\n",
        "                target[key] = None  # Not masked\n",
        "\n",
        "        # Return masked_sample, target, and the sequence starting from idx\n",
        "        return masked_sample, target, sequence\n",
        "\n",
        "# Define the Model\n",
        "class OrderModel(nn.Module):\n",
        "    def __init__(self, N_sec_ids, N_distance_bins, embedding_dim=32, hidden_dim=64):\n",
        "        super(OrderModel, self).__init__()\n",
        "\n",
        "        # Mask indices\n",
        "        self.security_id_mask = N_sec_ids\n",
        "        self.buy_sell_mask = 2\n",
        "        self.add_modify_delete_mask = 3\n",
        "        self.distance_mask = N_distance_bins\n",
        "\n",
        "        # Embedding layers for categorical features\n",
        "        self.security_id_embedding = nn.Embedding(N_sec_ids + 1, embedding_dim, padding_idx=self.security_id_mask)\n",
        "        self.buy_sell_embedding = nn.Embedding(3, embedding_dim, padding_idx=self.buy_sell_mask)  # 0,1,2(mask)\n",
        "        self.add_modify_delete_embedding = nn.Embedding(4, embedding_dim, padding_idx=self.add_modify_delete_mask)  # 0,1,2,3(mask)\n",
        "        self.distance_embedding = nn.Embedding(N_distance_bins + 1, embedding_dim, padding_idx=self.distance_mask)\n",
        "\n",
        "        # Linear layers for continuous features\n",
        "        self.quantity_linear = nn.Linear(1, embedding_dim)\n",
        "        self.price_linear = nn.Linear(1, embedding_dim)\n",
        "\n",
        "        # For timestamp (we don't mask timestamp)\n",
        "        self.timestamp_linear = nn.Linear(1, embedding_dim)\n",
        "\n",
        "        # LSTM for sequences\n",
        "        self.lstm_input_dim = embedding_dim * 7  # Number of features\n",
        "        self.lstm = nn.LSTM(input_size=self.lstm_input_dim, hidden_size=hidden_dim, batch_first=True)\n",
        "\n",
        "        # Final fully connected layers\n",
        "        self.fc1 = nn.Linear(embedding_dim * 7 + hidden_dim, 128)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Output heads\n",
        "        self.security_id_head = nn.Linear(128, N_sec_ids)\n",
        "        self.buy_sell_head = nn.Linear(128, 2)\n",
        "        self.add_modify_delete_head = nn.Linear(128, 3)\n",
        "        self.distance_head = nn.Linear(128, N_distance_bins)\n",
        "        self.quantity_head = nn.Linear(128, 1)\n",
        "        self.price_head = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x, seq_data, seq_lengths):\n",
        "        # x is a dictionary of features for the masked sample\n",
        "        # seq_data is a dictionary of sequences\n",
        "        # seq_lengths is a list of sequence lengths\n",
        "\n",
        "        # Process the masked sample\n",
        "        timestamp = x['timestamp'].unsqueeze(1).float()\n",
        "        security_id = x['security_id']\n",
        "        buy_sell = x['buy_sell']\n",
        "        add_modify_delete = x['add_modify_delete']\n",
        "        quantity = x['quantity'].unsqueeze(1)\n",
        "        price = x['price'].unsqueeze(1)\n",
        "        distance = x['distance']\n",
        "\n",
        "        # Embeddings\n",
        "        timestamp_emb = self.timestamp_linear(timestamp)\n",
        "        security_id_emb = self.security_id_embedding(security_id)\n",
        "        buy_sell_emb = self.buy_sell_embedding(buy_sell)\n",
        "        add_modify_delete_emb = self.add_modify_delete_embedding(add_modify_delete)\n",
        "        quantity_emb = self.quantity_linear(quantity)\n",
        "        price_emb = self.price_linear(price)\n",
        "        distance_emb = self.distance_embedding(distance)\n",
        "\n",
        "        # Concatenate embeddings\n",
        "        sample_emb = torch.cat([timestamp_emb,\n",
        "                                security_id_emb,\n",
        "                                buy_sell_emb,\n",
        "                                add_modify_delete_emb,\n",
        "                                quantity_emb,\n",
        "                                price_emb,\n",
        "                                distance_emb], dim=1)  # Shape: (batch_size, embedding_dim * 7)\n",
        "\n",
        "        # Process the sequence data\n",
        "        # For each feature in seq_data, get embeddings\n",
        "        batch_size = timestamp.shape[0]\n",
        "\n",
        "        seq_timestamp = seq_data['timestamp'].float()\n",
        "        seq_security_id = seq_data['security_id']\n",
        "        seq_buy_sell = seq_data['buy_sell']\n",
        "        seq_add_modify_delete = seq_data['add_modify_delete']\n",
        "        seq_quantity = seq_data['quantity']\n",
        "        seq_price = seq_data['price']\n",
        "        seq_distance = seq_data['distance']\n",
        "\n",
        "        # Embeddings for sequence data\n",
        "        seq_timestamp_emb = self.timestamp_linear(seq_timestamp.unsqueeze(-1))  # (batch_size, seq_len, embedding_dim)\n",
        "        seq_security_id_emb = self.security_id_embedding(seq_security_id)\n",
        "        seq_buy_sell_emb = self.buy_sell_embedding(seq_buy_sell)\n",
        "        seq_add_modify_delete_emb = self.add_modify_delete_embedding(seq_add_modify_delete)\n",
        "        seq_quantity_emb = self.quantity_linear(seq_quantity.unsqueeze(-1))\n",
        "        seq_price_emb = self.price_linear(seq_price.unsqueeze(-1))\n",
        "        seq_distance_emb = self.distance_embedding(seq_distance)\n",
        "\n",
        "        # Concatenate sequence embeddings\n",
        "        seq_emb = torch.cat([seq_timestamp_emb,\n",
        "                             seq_security_id_emb,\n",
        "                             seq_buy_sell_emb,\n",
        "                             seq_add_modify_delete_emb,\n",
        "                             seq_quantity_emb,\n",
        "                             seq_price_emb,\n",
        "                             seq_distance_emb], dim=2)  # Shape: (batch_size, seq_len, embedding_dim * 7)\n",
        "\n",
        "        # Pack the sequences\n",
        "        packed_seq_emb = nn.utils.rnn.pack_padded_sequence(seq_emb, seq_lengths, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        # Pass through LSTM\n",
        "        packed_output, (h_n, c_n) = self.lstm(packed_seq_emb)\n",
        "\n",
        "        # Get the last hidden state for each sequence\n",
        "        seq_context = h_n.squeeze(0)  # Shape: (batch_size, hidden_dim)\n",
        "\n",
        "        # Combine the sample embedding with the sequence context\n",
        "        combined = torch.cat([sample_emb, seq_context], dim=1)  # Shape: (batch_size, embedding_dim * 7 + hidden_dim)\n",
        "\n",
        "        # Forward pass\n",
        "        x = self.fc1(combined)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        # Outputs\n",
        "        outputs = {\n",
        "            'security_id': self.security_id_head(x),\n",
        "            'buy_sell': self.buy_sell_head(x),\n",
        "            'add_modify_delete': self.add_modify_delete_head(x),\n",
        "            'distance': self.distance_head(x),\n",
        "            'quantity': self.quantity_head(x),\n",
        "            'price': self.price_head(x)\n",
        "        }\n",
        "\n",
        "        return outputs\n",
        "\n",
        "# Collate function for DataLoader\n",
        "def collate_fn(batch):\n",
        "    # batch is a list of (masked_sample, target, sequence)\n",
        "    batch_size = len(batch)\n",
        "    masked_samples = [item[0] for item in batch]\n",
        "    targets = [item[1] for item in batch]\n",
        "    sequences = [item[2] for item in batch]\n",
        "\n",
        "    # Convert masked_samples to tensors\n",
        "    batch_data = {}\n",
        "    batch_target = {}\n",
        "    seq_data = {}\n",
        "    seq_lengths = []\n",
        "\n",
        "    # Process data\n",
        "    for key in ['timestamp', 'security_id', 'buy_sell', 'add_modify_delete', 'quantity', 'price', 'distance']:\n",
        "        batch_data[key] = torch.tensor([sample[key] for sample in masked_samples])\n",
        "\n",
        "    # Process sequences\n",
        "    # Sequences are variable-length\n",
        "    # For each key, we have a list of sequences\n",
        "    for key in ['timestamp', 'security_id', 'buy_sell', 'add_modify_delete', 'quantity', 'price', 'distance']:\n",
        "        seq_list = [torch.tensor(seq[key]) for seq in sequences]\n",
        "        seq_padded = nn.utils.rnn.pad_sequence(seq_list, batch_first=True, padding_value=0)\n",
        "        seq_data[key] = seq_padded\n",
        "    # Record lengths\n",
        "    seq_lengths = [len(seq['timestamp']) for seq in sequences]\n",
        "\n",
        "    # Convert seq_lengths to tensor\n",
        "    seq_lengths = torch.tensor(seq_lengths, dtype=torch.long)\n",
        "\n",
        "    # Process target (exclude 'timestamp' as it's not in target)\n",
        "    for key in ['security_id', 'buy_sell', 'add_modify_delete', 'quantity', 'price', 'distance']:\n",
        "        batch_target[key] = []\n",
        "        for target in targets:\n",
        "            if target[key] is not None:\n",
        "                batch_target[key].append(target[key])\n",
        "            else:\n",
        "                batch_target[key].append(-100 if key not in ['quantity', 'price'] else 0.0)\n",
        "\n",
        "        # Convert to tensor and create mask\n",
        "        if key in ['quantity', 'price']:\n",
        "            # For continuous features\n",
        "            batch_target[key + '_mask'] = torch.tensor([t != 0.0 for t in batch_target[key]], dtype=torch.bool)\n",
        "            batch_target[key] = torch.tensor(batch_target[key], dtype=torch.float)\n",
        "        else:\n",
        "            # For categorical features\n",
        "            batch_target[key + '_mask'] = torch.tensor([t != -100 for t in batch_target[key]], dtype=torch.bool)\n",
        "            batch_target[key] = torch.tensor(batch_target[key], dtype=torch.long)\n",
        "\n",
        "    return batch_data, batch_target, seq_data, seq_lengths\n",
        "\n",
        "# Training and Testing Loop\n",
        "def train_model():\n",
        "    # Create Dataset and DataLoader\n",
        "    dataset = MaskedOrderDataset(num_samples=10000)\n",
        "    dataloader = DataLoader(dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "    # Create model\n",
        "    model = OrderModel(N_sec_ids=dataset.N_sec_ids, N_distance_bins=dataset.N_distance_bins)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Loss functions\n",
        "    criterion_ce = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "    criterion_l1 = nn.L1Loss()\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(5):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        for batch_data, batch_target, seq_data, seq_lengths in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(batch_data, seq_data, seq_lengths)\n",
        "\n",
        "            loss = 0.0\n",
        "\n",
        "            # Compute loss for security_id\n",
        "            target = batch_target['security_id']\n",
        "            mask = batch_target['security_id_mask']\n",
        "            if mask.any():\n",
        "                loss_sec_id = criterion_ce(outputs['security_id'], target)\n",
        "                loss += loss_sec_id\n",
        "\n",
        "            # Compute loss for buy_sell\n",
        "            target = batch_target['buy_sell']\n",
        "            mask = batch_target['buy_sell_mask']\n",
        "            if mask.any():\n",
        "                loss_buy_sell = criterion_ce(outputs['buy_sell'], target)\n",
        "                loss += loss_buy_sell\n",
        "\n",
        "            # Compute loss for add_modify_delete\n",
        "            target = batch_target['add_modify_delete']\n",
        "            mask = batch_target['add_modify_delete_mask']\n",
        "            if mask.any():\n",
        "                loss_amd = criterion_ce(outputs['add_modify_delete'], target)\n",
        "                loss += loss_amd\n",
        "\n",
        "            # Compute loss for distance\n",
        "            target = batch_target['distance']\n",
        "            mask = batch_target['distance_mask']\n",
        "            if mask.any():\n",
        "                loss_distance = criterion_ce(outputs['distance'], target)\n",
        "                loss += loss_distance\n",
        "\n",
        "            # Compute loss for quantity\n",
        "            target = batch_target['quantity']\n",
        "            mask = batch_target['quantity_mask']\n",
        "            if mask.any():\n",
        "                output = outputs['quantity'].squeeze()\n",
        "                loss_quantity = criterion_l1(output[mask], target[mask])\n",
        "                loss += loss_quantity\n",
        "\n",
        "            # Compute loss for price\n",
        "            target = batch_target['price']\n",
        "            mask = batch_target['price_mask']\n",
        "            if mask.any():\n",
        "                output = outputs['price'].squeeze()\n",
        "                loss_price = criterion_l1(output[mask], target[mask])\n",
        "                loss += loss_price\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Loss: {total_loss/len(dataloader)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmEZwyPGCTJh",
        "outputId": "cd18dad6-1b2a-4178-d8c1-7e0410bc5d9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-d57e3d80251b>:238: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  seq_list = [torch.tensor(seq[key]) for seq in sequences]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 365.27560881596463\n",
            "Epoch 2, Loss: 327.6048114557934\n",
            "Epoch 3, Loss: 325.116089061567\n",
            "Epoch 4, Loss: 312.7274585893959\n",
            "Epoch 5, Loss: 312.8242979596375\n"
          ]
        }
      ]
    }
  ]
}